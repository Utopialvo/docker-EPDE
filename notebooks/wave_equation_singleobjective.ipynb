{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47016ec-1c27-4f59-8f1b-878b36da2729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7f22693dba00>\n",
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7f22693dbfa0>\n",
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7f22693dbfa0>\n",
      "trig_token_params: VALUES = (0, 1)\n",
      "Deriv orders after definition [[None], [0], [0, 0], [1], [1, 1]]\n",
      "Surface training t=0, loss=3.3719687461853027\n",
      "Surface training t=1, loss=3.2930716276168823\n",
      "Surface training t=2, loss=3.221894145011902\n",
      "Surface training t=3, loss=3.1512012481689453\n",
      "Surface training t=4, loss=3.0987859964370728\n",
      "Surface training t=5, loss=3.021836757659912\n",
      "Surface training t=6, loss=2.945072054862976\n",
      "Surface training t=7, loss=2.852617859840393\n",
      "Surface training t=8, loss=2.799723982810974\n",
      "Surface training t=9, loss=2.7262412309646606\n",
      "Surface training t=10, loss=2.643246293067932\n",
      "Surface training t=11, loss=2.5522162914276123\n",
      "Surface training t=12, loss=2.499799966812134\n",
      "Surface training t=13, loss=2.388542652130127\n",
      "Surface training t=14, loss=2.336835503578186\n",
      "Surface training t=15, loss=2.2478188276290894\n",
      "Surface training t=16, loss=2.1556018590927124\n",
      "Surface training t=17, loss=2.1181195974349976\n",
      "Surface training t=18, loss=1.9978343844413757\n",
      "Surface training t=19, loss=1.9418727159500122\n",
      "Surface training t=20, loss=1.8789194822311401\n",
      "Surface training t=21, loss=1.8163605332374573\n",
      "Surface training t=22, loss=1.7846174240112305\n",
      "Surface training t=23, loss=1.7411844730377197\n",
      "Surface training t=24, loss=1.720180630683899\n",
      "Surface training t=25, loss=1.7118332386016846\n",
      "Surface training t=26, loss=1.6795334815979004\n",
      "Surface training t=27, loss=1.6724600195884705\n",
      "Surface training t=28, loss=1.673660933971405\n",
      "Surface training t=29, loss=1.696303367614746\n",
      "Surface training t=30, loss=1.6901479363441467\n",
      "Surface training t=31, loss=1.6785178780555725\n",
      "Surface training t=32, loss=1.6799874305725098\n",
      "Surface training t=33, loss=1.693247139453888\n",
      "Surface training t=34, loss=1.6828305125236511\n",
      "Surface training t=35, loss=1.6810736656188965\n",
      "Surface training t=36, loss=1.680359423160553\n",
      "Surface training t=37, loss=1.6874414682388306\n",
      "Surface training t=38, loss=1.6732261776924133\n",
      "Surface training t=39, loss=1.6779508590698242\n",
      "Surface training t=40, loss=1.6782833337783813\n",
      "Surface training t=41, loss=1.6759486198425293\n",
      "Surface training t=42, loss=1.6795008778572083\n",
      "Surface training t=43, loss=1.6804953217506409\n",
      "Surface training t=44, loss=1.671924889087677\n",
      "Surface training t=45, loss=1.672536015510559\n",
      "Surface training t=46, loss=1.6720964908599854\n",
      "Surface training t=47, loss=1.6955629587173462\n",
      "Surface training t=48, loss=1.6759980916976929\n",
      "Surface training t=49, loss=1.6885849237442017\n",
      "Surface training t=50, loss=1.6803736090660095\n",
      "Surface training t=51, loss=1.6856998205184937\n",
      "Surface training t=52, loss=1.6787577867507935\n",
      "Surface training t=53, loss=1.6656003594398499\n",
      "Surface training t=54, loss=1.6816333532333374\n",
      "Surface training t=55, loss=1.6667845249176025\n",
      "Surface training t=56, loss=1.672728717327118\n",
      "Surface training t=57, loss=1.6619136929512024\n",
      "Surface training t=58, loss=1.6640600562095642\n",
      "Surface training t=59, loss=1.68263578414917\n",
      "Surface training t=60, loss=1.6879683136940002\n",
      "Surface training t=61, loss=1.6794650554656982\n",
      "Surface training t=62, loss=1.6394876837730408\n",
      "Surface training t=63, loss=1.6778545379638672\n",
      "Surface training t=64, loss=1.6750682592391968\n",
      "Surface training t=65, loss=1.6645681262016296\n",
      "Surface training t=66, loss=1.6994252800941467\n",
      "Surface training t=67, loss=1.6777636408805847\n",
      "Surface training t=68, loss=1.6729348301887512\n",
      "Surface training t=69, loss=1.6611668467521667\n",
      "Surface training t=70, loss=1.6739786267280579\n",
      "Surface training t=71, loss=1.6847979426383972\n",
      "Surface training t=72, loss=1.6699487566947937\n",
      "Surface training t=73, loss=1.6704102158546448\n",
      "Surface training t=74, loss=1.6593121886253357\n",
      "Surface training t=75, loss=1.6873198747634888\n",
      "Surface training t=76, loss=1.668367087841034\n",
      "Surface training t=77, loss=1.6657686233520508\n",
      "Surface training t=78, loss=1.6711005568504333\n",
      "Surface training t=79, loss=1.664385437965393\n",
      "Surface training t=80, loss=1.6562250256538391\n",
      "Surface training t=81, loss=1.6702821254730225\n",
      "Surface training t=82, loss=1.6609703302383423\n",
      "Surface training t=83, loss=1.6518086791038513\n",
      "Surface training t=84, loss=1.6690617203712463\n",
      "Surface training t=85, loss=1.671828269958496\n",
      "Surface training t=86, loss=1.6456353664398193\n",
      "Surface training t=87, loss=1.6506649851799011\n",
      "Surface training t=88, loss=1.6569713950157166\n",
      "Surface training t=89, loss=1.6532870531082153\n",
      "Surface training t=90, loss=1.6638048887252808\n",
      "Surface training t=91, loss=1.6568512916564941\n",
      "Surface training t=92, loss=1.666578471660614\n",
      "Surface training t=93, loss=1.643481969833374\n",
      "Surface training t=94, loss=1.6486281752586365\n",
      "Surface training t=95, loss=1.640661358833313\n",
      "Surface training t=96, loss=1.6450256705284119\n",
      "Surface training t=97, loss=1.6399793028831482\n",
      "Surface training t=98, loss=1.6566702127456665\n",
      "Surface training t=99, loss=1.6440761089324951\n",
      "Surface training t=100, loss=1.634640395641327\n",
      "Surface training t=101, loss=1.6458914279937744\n",
      "Surface training t=102, loss=1.628104567527771\n",
      "Surface training t=103, loss=1.6347413063049316\n",
      "Surface training t=104, loss=1.6381905674934387\n",
      "Surface training t=105, loss=1.626085340976715\n",
      "Surface training t=106, loss=1.6279906034469604\n",
      "Surface training t=107, loss=1.6356032490730286\n",
      "Surface training t=108, loss=1.6174219250679016\n",
      "Surface training t=109, loss=1.6125864386558533\n",
      "Surface training t=110, loss=1.6100329756736755\n",
      "Surface training t=111, loss=1.610211730003357\n",
      "Surface training t=112, loss=1.6169289946556091\n",
      "Surface training t=113, loss=1.5905901193618774\n",
      "Surface training t=114, loss=1.6150757670402527\n",
      "Surface training t=115, loss=1.6047547459602356\n",
      "Surface training t=116, loss=1.5977557301521301\n",
      "Surface training t=117, loss=1.6007582545280457\n",
      "Surface training t=118, loss=1.5880195498466492\n",
      "Surface training t=119, loss=1.6000242829322815\n",
      "Surface training t=120, loss=1.5754494071006775\n",
      "Surface training t=121, loss=1.5645055770874023\n",
      "Surface training t=122, loss=1.5617169737815857\n",
      "Surface training t=123, loss=1.549326479434967\n",
      "Surface training t=124, loss=1.5446044206619263\n",
      "Surface training t=125, loss=1.5493899583816528\n",
      "Surface training t=126, loss=1.5275810956954956\n",
      "Surface training t=127, loss=1.5267733931541443\n",
      "Surface training t=128, loss=1.5239403247833252\n",
      "Surface training t=129, loss=1.4996774196624756\n",
      "Surface training t=130, loss=1.5097699761390686\n",
      "Surface training t=131, loss=1.489371418952942\n",
      "Surface training t=132, loss=1.482493281364441\n",
      "Surface training t=133, loss=1.4810820817947388\n",
      "Surface training t=134, loss=1.448357105255127\n",
      "Surface training t=135, loss=1.4340433478355408\n",
      "Surface training t=136, loss=1.4310171008110046\n",
      "Surface training t=137, loss=1.4226340055465698\n",
      "Surface training t=138, loss=1.4113711714744568\n",
      "Surface training t=139, loss=1.3797380924224854\n",
      "Surface training t=140, loss=1.3720110058784485\n",
      "Surface training t=141, loss=1.342043399810791\n",
      "Surface training t=142, loss=1.3344433307647705\n",
      "Surface training t=143, loss=1.296791434288025\n",
      "Surface training t=144, loss=1.2879718542099\n",
      "Surface training t=145, loss=1.2727687358856201\n",
      "Surface training t=146, loss=1.238555669784546\n",
      "Surface training t=147, loss=1.2094085216522217\n",
      "Surface training t=148, loss=1.1871617436408997\n",
      "Surface training t=149, loss=1.1526533961296082\n",
      "Surface training t=150, loss=1.1263538002967834\n",
      "Surface training t=151, loss=1.093054711818695\n",
      "Surface training t=152, loss=1.0504108667373657\n",
      "Surface training t=153, loss=1.0062696933746338\n",
      "Surface training t=154, loss=0.9743848145008087\n",
      "Surface training t=155, loss=0.9625846743583679\n",
      "Surface training t=156, loss=0.901293933391571\n",
      "Surface training t=157, loss=0.8590620756149292\n",
      "Surface training t=158, loss=0.8157493770122528\n",
      "Surface training t=159, loss=0.7888242900371552\n",
      "Surface training t=160, loss=0.7407502830028534\n",
      "Surface training t=161, loss=0.6956741213798523\n",
      "Surface training t=162, loss=0.6509747207164764\n",
      "Surface training t=163, loss=0.6068711578845978\n",
      "Surface training t=164, loss=0.5756982862949371\n",
      "Surface training t=165, loss=0.5359323620796204\n",
      "Surface training t=166, loss=0.48794686794281006\n",
      "Surface training t=167, loss=0.44904449582099915\n",
      "Surface training t=168, loss=0.41758470237255096\n",
      "Surface training t=169, loss=0.38357897102832794\n",
      "Surface training t=170, loss=0.358129158616066\n",
      "Surface training t=171, loss=0.3469187766313553\n",
      "Surface training t=172, loss=0.3314693421125412\n",
      "Surface training t=173, loss=0.3284514248371124\n",
      "Surface training t=174, loss=0.31856483221054077\n",
      "Surface training t=175, loss=0.3127410113811493\n",
      "Surface training t=176, loss=0.30672067403793335\n",
      "Surface training t=177, loss=0.30225466191768646\n",
      "Surface training t=178, loss=0.2974886894226074\n",
      "Surface training t=179, loss=0.2956547439098358\n",
      "Surface training t=180, loss=0.2928933650255203\n",
      "Surface training t=181, loss=0.28239724040031433\n",
      "Surface training t=182, loss=0.2763868719339371\n",
      "Surface training t=183, loss=0.26709070801734924\n",
      "Surface training t=184, loss=0.26276345551013947\n",
      "Surface training t=185, loss=0.2598152458667755\n",
      "Surface training t=186, loss=0.2527570351958275\n",
      "Surface training t=187, loss=0.2514844536781311\n",
      "Surface training t=188, loss=0.24228141456842422\n",
      "Surface training t=189, loss=0.24411015212535858\n",
      "Surface training t=190, loss=0.23812342435121536\n",
      "Surface training t=191, loss=0.23509451746940613\n",
      "Surface training t=192, loss=0.22842960804700851\n",
      "Surface training t=193, loss=0.2256298065185547\n",
      "Surface training t=194, loss=0.21876409649848938\n",
      "Surface training t=195, loss=0.21444863080978394\n",
      "Surface training t=196, loss=0.2072349190711975\n",
      "Surface training t=197, loss=0.20269156992435455\n",
      "Surface training t=198, loss=0.19822517782449722\n",
      "Surface training t=199, loss=0.1902994066476822\n",
      "Surface training t=200, loss=0.18561996519565582\n",
      "Surface training t=201, loss=0.1796640157699585\n",
      "Surface training t=202, loss=0.1731375977396965\n",
      "Surface training t=203, loss=0.16984977573156357\n",
      "Surface training t=204, loss=0.16409438848495483\n",
      "Surface training t=205, loss=0.15809088200330734\n",
      "Surface training t=206, loss=0.15506024658679962\n",
      "Surface training t=207, loss=0.14706485718488693\n",
      "Surface training t=208, loss=0.141447015106678\n",
      "Surface training t=209, loss=0.13583403825759888\n",
      "Surface training t=210, loss=0.1299373209476471\n",
      "Surface training t=211, loss=0.12383943051099777\n",
      "Surface training t=212, loss=0.11725304648280144\n",
      "Surface training t=213, loss=0.11462806165218353\n",
      "Surface training t=214, loss=0.10959900543093681\n",
      "Surface training t=215, loss=0.10523912310600281\n",
      "Surface training t=216, loss=0.1023104302585125\n",
      "Surface training t=217, loss=0.09750061854720116\n",
      "Surface training t=218, loss=0.09493885189294815\n",
      "Surface training t=219, loss=0.09080473333597183\n",
      "Surface training t=220, loss=0.08884655311703682\n",
      "Surface training t=221, loss=0.0866401419043541\n",
      "Surface training t=222, loss=0.08302011340856552\n",
      "Surface training t=223, loss=0.08364871889352798\n",
      "Surface training t=224, loss=0.0820619948208332\n",
      "Surface training t=225, loss=0.08164642378687859\n",
      "Surface training t=226, loss=0.08009813353419304\n",
      "Surface training t=227, loss=0.07794786244630814\n",
      "Surface training t=228, loss=0.07823275402188301\n",
      "Surface training t=229, loss=0.07854853942990303\n",
      "Surface training t=230, loss=0.07928936183452606\n",
      "Surface training t=231, loss=0.07856706902384758\n",
      "Surface training t=232, loss=0.0779430903494358\n",
      "Surface training t=233, loss=0.07727085053920746\n",
      "Surface training t=234, loss=0.07702558860182762\n",
      "Surface training t=235, loss=0.07736903801560402\n",
      "Surface training t=236, loss=0.07702751085162163\n",
      "Surface training t=237, loss=0.07652629911899567\n",
      "Surface training t=238, loss=0.07621747255325317\n",
      "Surface training t=239, loss=0.07538644224405289\n",
      "Surface training t=240, loss=0.0757317915558815\n",
      "Surface training t=241, loss=0.0766100287437439\n",
      "Surface training t=242, loss=0.07570184767246246\n",
      "Surface training t=243, loss=0.07434152811765671\n",
      "Surface training t=244, loss=0.07549059018492699\n",
      "Surface training t=245, loss=0.07526629790663719\n",
      "Surface training t=246, loss=0.07540443539619446\n",
      "Surface training t=247, loss=0.07458262890577316\n",
      "Surface training t=248, loss=0.07443148642778397\n",
      "Surface training t=249, loss=0.07288281619548798\n",
      "Surface training t=250, loss=0.07362492009997368\n",
      "Surface training t=251, loss=0.07487045973539352\n",
      "Surface training t=252, loss=0.07562966272234917\n",
      "Surface training t=253, loss=0.07351727783679962\n",
      "Surface training t=254, loss=0.07310275360941887\n",
      "Surface training t=255, loss=0.07230181992053986\n",
      "Surface training t=256, loss=0.07206271216273308\n",
      "Surface training t=257, loss=0.07131268829107285\n",
      "Surface training t=258, loss=0.07146333530545235\n",
      "Surface training t=259, loss=0.07051724940538406\n",
      "Surface training t=260, loss=0.0727713443338871\n",
      "Surface training t=261, loss=0.07223612070083618\n",
      "Surface training t=262, loss=0.07227787747979164\n",
      "Surface training t=263, loss=0.07348637282848358\n",
      "Surface training t=264, loss=0.07018186897039413\n",
      "Surface training t=265, loss=0.0716562382876873\n",
      "Surface training t=266, loss=0.07187043130397797\n",
      "Surface training t=267, loss=0.06982172653079033\n",
      "Surface training t=268, loss=0.06931693479418755\n",
      "Surface training t=269, loss=0.07118652388453484\n",
      "Surface training t=270, loss=0.06862660869956017\n",
      "Surface training t=271, loss=0.06879981234669685\n",
      "Surface training t=272, loss=0.06987721100449562\n",
      "Surface training t=273, loss=0.067518450319767\n",
      "Surface training t=274, loss=0.06862230971455574\n",
      "Surface training t=275, loss=0.06793221458792686\n",
      "Surface training t=276, loss=0.068843062967062\n",
      "Surface training t=277, loss=0.06729799881577492\n",
      "Surface training t=278, loss=0.06779200583696365\n",
      "Surface training t=279, loss=0.06769512221217155\n",
      "Surface training t=280, loss=0.06884753704071045\n",
      "Surface training t=281, loss=0.06828709691762924\n",
      "Surface training t=282, loss=0.06786220893263817\n",
      "Surface training t=283, loss=0.06869102641940117\n",
      "Surface training t=284, loss=0.06763073801994324\n",
      "Surface training t=285, loss=0.06727784126996994\n",
      "Surface training t=286, loss=0.06831136718392372\n",
      "Surface training t=287, loss=0.06574112176895142\n",
      "Surface training t=288, loss=0.06786146759986877\n",
      "Surface training t=289, loss=0.06662731617689133\n",
      "Surface training t=290, loss=0.06496177613735199\n",
      "Surface training t=291, loss=0.06591444090008736\n",
      "Surface training t=292, loss=0.0658966451883316\n",
      "Surface training t=293, loss=0.0646456852555275\n",
      "Surface training t=294, loss=0.06567389890551567\n",
      "Surface training t=295, loss=0.06258215755224228\n",
      "Surface training t=296, loss=0.06701287999749184\n",
      "Surface training t=297, loss=0.06433280929923058\n",
      "Surface training t=298, loss=0.06567734107375145\n",
      "Surface training t=299, loss=0.06583502143621445\n",
      "Surface training t=300, loss=0.06464778259396553\n",
      "Surface training t=301, loss=0.06480205431580544\n",
      "Surface training t=302, loss=0.06431742385029793\n",
      "Surface training t=303, loss=0.0640665739774704\n",
      "Surface training t=304, loss=0.06462417170405388\n",
      "Surface training t=305, loss=0.065235685557127\n",
      "Surface training t=306, loss=0.06392014399170876\n",
      "Surface training t=307, loss=0.06371777318418026\n",
      "Surface training t=308, loss=0.06434853374958038\n",
      "Surface training t=309, loss=0.06313682906329632\n",
      "Surface training t=310, loss=0.06358829140663147\n",
      "Surface training t=311, loss=0.06522325053811073\n",
      "Surface training t=312, loss=0.06252765655517578\n",
      "Surface training t=313, loss=0.06281782686710358\n",
      "Surface training t=314, loss=0.06299640983343124\n",
      "Surface training t=315, loss=0.0630708709359169\n",
      "Surface training t=316, loss=0.06396730616688728\n",
      "Surface training t=317, loss=0.06227582320570946\n",
      "Surface training t=318, loss=0.06304747983813286\n",
      "Surface training t=319, loss=0.06253272481262684\n",
      "Surface training t=320, loss=0.061627913266420364\n",
      "Surface training t=321, loss=0.060919446870684624\n",
      "Surface training t=322, loss=0.06356523372232914\n",
      "Surface training t=323, loss=0.05981415696442127\n",
      "Surface training t=324, loss=0.06168581172823906\n",
      "Surface training t=325, loss=0.06158442050218582\n",
      "Surface training t=326, loss=0.06286167539656162\n",
      "Surface training t=327, loss=0.0617190208286047\n",
      "Surface training t=328, loss=0.06047622300684452\n",
      "Surface training t=329, loss=0.06099238246679306\n",
      "Surface training t=330, loss=0.06093126721680164\n",
      "Surface training t=331, loss=0.05872771516442299\n",
      "Surface training t=332, loss=0.06075725704431534\n",
      "Surface training t=333, loss=0.05995599366724491\n",
      "Surface training t=334, loss=0.06036652624607086\n",
      "Surface training t=335, loss=0.0606699138879776\n",
      "Surface training t=336, loss=0.05894790031015873\n",
      "Surface training t=337, loss=0.06096154823899269\n",
      "Surface training t=338, loss=0.05910736881196499\n",
      "Surface training t=339, loss=0.06096693314611912\n",
      "Surface training t=340, loss=0.0579038355499506\n",
      "Surface training t=341, loss=0.060330238193273544\n",
      "Surface training t=342, loss=0.05975227802991867\n",
      "Surface training t=343, loss=0.057804547250270844\n",
      "Surface training t=344, loss=0.05864191986620426\n",
      "Surface training t=345, loss=0.06012113578617573\n",
      "Surface training t=346, loss=0.06023039668798447\n",
      "Surface training t=347, loss=0.05946820601820946\n",
      "Surface training t=348, loss=0.0591527558863163\n",
      "Surface training t=349, loss=0.059459540992975235\n",
      "Surface training t=350, loss=0.0594477616250515\n",
      "Surface training t=351, loss=0.058862870559096336\n",
      "Surface training t=352, loss=0.05613928101956844\n",
      "Surface training t=353, loss=0.057722436264157295\n",
      "Surface training t=354, loss=0.0586845763027668\n",
      "Surface training t=355, loss=0.05811488255858421\n",
      "Surface training t=356, loss=0.06047369912266731\n",
      "Surface training t=357, loss=0.06081913970410824\n",
      "Surface training t=358, loss=0.05636649206280708\n",
      "Surface training t=359, loss=0.05744301714003086\n",
      "Surface training t=360, loss=0.058148493990302086\n",
      "Surface training t=361, loss=0.05856912024319172\n",
      "Surface training t=362, loss=0.057233791798353195\n",
      "Surface training t=363, loss=0.05814799480140209\n",
      "Surface training t=364, loss=0.056592706590890884\n",
      "Surface training t=365, loss=0.05947398208081722\n",
      "Surface training t=366, loss=0.05733352713286877\n",
      "Surface training t=367, loss=0.05756286904215813\n",
      "Surface training t=368, loss=0.05834898166358471\n",
      "Surface training t=369, loss=0.05834035761654377\n",
      "Surface training t=370, loss=0.05842923931777477\n",
      "Surface training t=371, loss=0.056486062705516815\n",
      "Surface training t=372, loss=0.056240472942590714\n",
      "Surface training t=373, loss=0.05516873300075531\n",
      "Surface training t=374, loss=0.05640796944499016\n",
      "Surface training t=375, loss=0.05834744870662689\n",
      "Surface training t=376, loss=0.060033466666936874\n",
      "Surface training t=377, loss=0.06532508507370949\n",
      "Surface training t=378, loss=0.07002176344394684\n",
      "Surface training t=379, loss=0.07352366857230663\n",
      "Surface training t=380, loss=0.0720159076154232\n",
      "Surface training t=381, loss=0.06654701009392738\n",
      "Surface training t=382, loss=0.0652850903570652\n",
      "Surface training t=383, loss=0.06399628147482872\n",
      "Surface training t=384, loss=0.06488643027842045\n",
      "Surface training t=385, loss=0.06747306138277054\n",
      "Surface training t=386, loss=0.06948452070355415\n",
      "Surface training t=387, loss=0.06973901763558388\n",
      "Surface training t=388, loss=0.0643163938075304\n",
      "Surface training t=389, loss=0.06266922503709793\n",
      "Surface training t=390, loss=0.058905648067593575\n",
      "Surface training t=391, loss=0.05794668383896351\n",
      "Surface training t=392, loss=0.06161000207066536\n",
      "Surface training t=393, loss=0.05818490497767925\n",
      "Surface training t=394, loss=0.06147752329707146\n",
      "Surface training t=395, loss=0.062160883098840714\n",
      "Surface training t=396, loss=0.06284143403172493\n",
      "Surface training t=397, loss=0.06400040909647942\n",
      "Surface training t=398, loss=0.06437165662646294\n",
      "Surface training t=399, loss=0.06228157319128513\n",
      "Surface training t=400, loss=0.05963555909693241\n",
      "Surface training t=401, loss=0.059248726814985275\n",
      "Surface training t=402, loss=0.059821901842951775\n",
      "Surface training t=403, loss=0.05865735560655594\n",
      "Surface training t=404, loss=0.05948438495397568\n",
      "Surface training t=405, loss=0.05851946398615837\n",
      "Surface training t=406, loss=0.06032722443342209\n",
      "Surface training t=407, loss=0.06017536297440529\n",
      "Surface training t=408, loss=0.062176793813705444\n",
      "Surface training t=409, loss=0.060210373252630234\n",
      "Surface training t=410, loss=0.06058130040764809\n",
      "Surface training t=411, loss=0.057492997497320175\n",
      "Surface training t=412, loss=0.05815655551850796\n",
      "Surface training t=413, loss=0.05934455990791321\n",
      "Surface training t=414, loss=0.058463577181100845\n",
      "Surface training t=415, loss=0.05951496958732605\n",
      "Surface training t=416, loss=0.0608090665191412\n",
      "Surface training t=417, loss=0.061656566336750984\n",
      "Surface training t=418, loss=0.06035902351140976\n",
      "Surface training t=419, loss=0.06019602529704571\n",
      "Surface training t=420, loss=0.05761633813381195\n",
      "Surface training t=421, loss=0.06207253225147724\n",
      "Surface training t=422, loss=0.05802152678370476\n",
      "Surface training t=423, loss=0.05513003468513489\n",
      "Surface training t=424, loss=0.05544360727071762\n",
      "Surface training t=425, loss=0.05709519609808922\n",
      "Surface training t=426, loss=0.05857210420072079\n",
      "Surface training t=427, loss=0.058386776596307755\n",
      "Surface training t=428, loss=0.05869453772902489\n",
      "Surface training t=429, loss=0.059149669483304024\n",
      "Surface training t=430, loss=0.05994800664484501\n",
      "Surface training t=431, loss=0.060051050037145615\n",
      "Surface training t=432, loss=0.058541249483823776\n",
      "Surface training t=433, loss=0.05805794335901737\n",
      "Surface training t=434, loss=0.058885686099529266\n",
      "Surface training t=435, loss=0.058907024562358856\n",
      "Surface training t=436, loss=0.05779954791069031\n",
      "Surface training t=437, loss=0.05648915097117424\n",
      "Surface training t=438, loss=0.05763421952724457\n",
      "Surface training t=439, loss=0.05742509663105011\n",
      "Surface training t=440, loss=0.05918979458510876\n",
      "Surface training t=441, loss=0.05867388844490051\n",
      "Surface training t=442, loss=0.06019376032054424\n",
      "Surface training t=443, loss=0.058803435415029526\n",
      "Surface training t=444, loss=0.056746046990156174\n",
      "Surface training t=445, loss=0.05734194628894329\n",
      "Surface training t=446, loss=0.05778418853878975\n",
      "Surface training t=447, loss=0.05747309699654579\n",
      "Surface training t=448, loss=0.05694860965013504\n",
      "Surface training t=449, loss=0.05806255713105202\n",
      "Surface training t=450, loss=0.05972482077777386\n",
      "Surface training t=451, loss=0.058863675221800804\n",
      "Surface training t=452, loss=0.058890076354146004\n",
      "Surface training t=453, loss=0.056855546310544014\n",
      "Surface training t=454, loss=0.05866286717355251\n",
      "Surface training t=455, loss=0.05563469044864178\n",
      "Surface training t=456, loss=0.05776261352002621\n",
      "Surface training t=457, loss=0.05556149408221245\n",
      "Surface training t=458, loss=0.05812026374042034\n",
      "Surface training t=459, loss=0.05722688138484955\n",
      "Surface training t=460, loss=0.05424552597105503\n",
      "Surface training t=461, loss=0.05616072751581669\n",
      "Surface training t=462, loss=0.05597073771059513\n",
      "Surface training t=463, loss=0.05791743844747543\n",
      "Surface training t=464, loss=0.056213222444057465\n",
      "Surface training t=465, loss=0.05849597975611687\n",
      "Surface training t=466, loss=0.0583161897957325\n",
      "Surface training t=467, loss=0.059160348027944565\n",
      "Surface training t=468, loss=0.05662187188863754\n",
      "Surface training t=469, loss=0.05499156005680561\n",
      "Surface training t=470, loss=0.05438974127173424\n",
      "Surface training t=471, loss=0.05639899708330631\n",
      "Surface training t=472, loss=0.05565278232097626\n",
      "Surface training t=473, loss=0.0567976962774992\n",
      "Surface training t=474, loss=0.056530263274908066\n",
      "Surface training t=475, loss=0.05705326236784458\n",
      "Surface training t=476, loss=0.05735359899699688\n",
      "Surface training t=477, loss=0.06004723533987999\n",
      "Surface training t=478, loss=0.05846066772937775\n",
      "Surface training t=479, loss=0.05500399321317673\n",
      "Surface training t=480, loss=0.05691605247557163\n",
      "Surface training t=481, loss=0.055047424510121346\n",
      "Surface training t=482, loss=0.05504515767097473\n",
      "Surface training t=483, loss=0.053837474435567856\n",
      "Surface training t=484, loss=0.055469151586294174\n",
      "Surface training t=485, loss=0.05277877114713192\n",
      "Surface training t=486, loss=0.05412111431360245\n",
      "Surface training t=487, loss=0.05367550253868103\n",
      "Surface training t=488, loss=0.054279521107673645\n",
      "Surface training t=489, loss=0.054014090448617935\n",
      "Surface training t=490, loss=0.05345886945724487\n",
      "Surface training t=491, loss=0.05457826890051365\n",
      "Surface training t=492, loss=0.05659475736320019\n",
      "Surface training t=493, loss=0.0568982008844614\n",
      "Surface training t=494, loss=0.05428112857043743\n",
      "Surface training t=495, loss=0.055464720353484154\n",
      "Surface training t=496, loss=0.053711581975221634\n",
      "Surface training t=497, loss=0.05631636269390583\n",
      "Surface training t=498, loss=0.05433339439332485\n",
      "Surface training t=499, loss=0.053805528208613396\n",
      "Surface training t=500, loss=0.05439428426325321\n",
      "Surface training t=501, loss=0.05754079483449459\n",
      "Surface training t=502, loss=0.05462383292615414\n",
      "Surface training t=503, loss=0.05466200225055218\n",
      "Surface training t=504, loss=0.05536760576069355\n",
      "Surface training t=505, loss=0.05419541709125042\n",
      "Surface training t=506, loss=0.05453952215611935\n",
      "Surface training t=507, loss=0.05585575848817825\n",
      "Surface training t=508, loss=0.055010393261909485\n",
      "Surface training t=509, loss=0.05548021383583546\n",
      "Surface training t=510, loss=0.05385444313287735\n",
      "Surface training t=511, loss=0.05493730865418911\n",
      "Surface training t=512, loss=0.055579837411642075\n",
      "Surface training t=513, loss=0.05647287704050541\n",
      "Surface training t=514, loss=0.05362645536661148\n",
      "Surface training t=515, loss=0.05549856647849083\n",
      "Surface training t=516, loss=0.0538164209574461\n",
      "Surface training t=517, loss=0.053371502086520195\n",
      "Surface training t=518, loss=0.05428128316998482\n",
      "Surface training t=519, loss=0.05562124028801918\n",
      "Surface training t=520, loss=0.052895668894052505\n",
      "Surface training t=521, loss=0.05336255766451359\n",
      "Surface training t=522, loss=0.057099342346191406\n",
      "Surface training t=523, loss=0.05436565913259983\n",
      "Surface training t=524, loss=0.05606270767748356\n",
      "Surface training t=525, loss=0.05608851835131645\n",
      "Surface training t=526, loss=0.05495677888393402\n",
      "Surface training t=527, loss=0.05397511459887028\n",
      "Surface training t=528, loss=0.05355028249323368\n",
      "Surface training t=529, loss=0.05435653403401375\n",
      "Surface training t=530, loss=0.0558601263910532\n",
      "Surface training t=531, loss=0.054668325930833817\n",
      "Surface training t=532, loss=0.054755598306655884\n",
      "Surface training t=533, loss=0.05700308829545975\n",
      "Surface training t=534, loss=0.05411662720143795\n",
      "Surface training t=535, loss=0.05579305998980999\n",
      "Surface training t=536, loss=0.05601530149579048\n",
      "Surface training t=537, loss=0.05593501776456833\n",
      "Surface training t=538, loss=0.05738029256463051\n",
      "Surface training t=539, loss=0.058564258739352226\n",
      "Surface training t=540, loss=0.0588208232074976\n",
      "Surface training t=541, loss=0.05704244412481785\n",
      "Surface training t=542, loss=0.055831924080848694\n",
      "Surface training t=543, loss=0.056892139837145805\n",
      "Surface training t=544, loss=0.05694134347140789\n",
      "Surface training t=545, loss=0.05515000410377979\n",
      "Surface training t=546, loss=0.05672292783856392\n",
      "Surface training t=547, loss=0.05677010118961334\n",
      "Surface training t=548, loss=0.05435121990740299\n",
      "Surface training t=549, loss=0.056081390008330345\n",
      "Surface training t=550, loss=0.05427251197397709\n",
      "Surface training t=551, loss=0.0541198942810297\n",
      "Surface training t=552, loss=0.05611681938171387\n",
      "Surface training t=553, loss=0.054435497149825096\n",
      "Surface training t=554, loss=0.05664031021296978\n",
      "Surface training t=555, loss=0.055971695110201836\n",
      "Surface training t=556, loss=0.0574411042034626\n",
      "Surface training t=557, loss=0.056343356147408485\n",
      "Surface training t=558, loss=0.056469814851880074\n",
      "Surface training t=559, loss=0.056045981124043465\n",
      "Surface training t=560, loss=0.053523046895861626\n",
      "Surface training t=561, loss=0.05383151210844517\n",
      "Surface training t=562, loss=0.05424439162015915\n",
      "Surface training t=563, loss=0.05715933069586754\n",
      "Surface training t=564, loss=0.05585077777504921\n",
      "Surface training t=565, loss=0.05466962791979313\n",
      "Surface training t=566, loss=0.05426205322146416\n",
      "Surface training t=567, loss=0.054809628054499626\n",
      "Surface training t=568, loss=0.054561011493206024\n",
      "Surface training t=569, loss=0.05361446551978588\n",
      "Surface training t=570, loss=0.05564386956393719\n",
      "Surface training t=571, loss=0.05464964546263218\n",
      "Surface training t=572, loss=0.05471167527139187\n",
      "Surface training t=573, loss=0.05370554327964783\n",
      "Surface training t=574, loss=0.05626417137682438\n",
      "Surface training t=575, loss=0.05485934019088745\n",
      "Surface training t=576, loss=0.0569179393351078\n",
      "Surface training t=577, loss=0.0564822219312191\n",
      "Surface training t=578, loss=0.05689382366836071\n",
      "Surface training t=579, loss=0.05544608272612095\n",
      "Surface training t=580, loss=0.05343970097601414\n",
      "Surface training t=581, loss=0.05608632043004036\n",
      "Surface training t=582, loss=0.05494289472699165\n",
      "Surface training t=583, loss=0.05446576699614525\n",
      "Surface training t=584, loss=0.053943224251270294\n",
      "Surface training t=585, loss=0.05777604132890701\n",
      "Surface training t=586, loss=0.0555159617215395\n",
      "Surface training t=587, loss=0.056490458548069\n",
      "Surface training t=588, loss=0.05561132729053497\n",
      "Surface training t=589, loss=0.05406937375664711\n",
      "Surface training t=590, loss=0.055123936384916306\n",
      "Surface training t=591, loss=0.053599290549755096\n",
      "Surface training t=592, loss=0.05207766778767109\n",
      "Surface training t=593, loss=0.05233466438949108\n",
      "Surface training t=594, loss=0.05213480815291405\n",
      "Surface training t=595, loss=0.0502453725785017\n",
      "Surface training t=596, loss=0.054025281220674515\n",
      "Surface training t=597, loss=0.0540796909481287\n",
      "Surface training t=598, loss=0.05600487440824509\n",
      "Surface training t=599, loss=0.060373567044734955\n",
      "Surface training t=600, loss=0.05775219015777111\n",
      "Surface training t=601, loss=0.056199850514531136\n",
      "Surface training t=602, loss=0.05493711307644844\n",
      "Surface training t=603, loss=0.053134847432374954\n",
      "Surface training t=604, loss=0.05720238760113716\n",
      "Surface training t=605, loss=0.05415712483227253\n",
      "Surface training t=606, loss=0.05504587106406689\n",
      "Surface training t=607, loss=0.0574403814971447\n",
      "Surface training t=608, loss=0.05515501648187637\n",
      "Surface training t=609, loss=0.05797423794865608\n",
      "Surface training t=610, loss=0.05580666661262512\n",
      "Surface training t=611, loss=0.05475303903222084\n",
      "Surface training t=612, loss=0.053933894261717796\n",
      "Surface training t=613, loss=0.05396348237991333\n",
      "Surface training t=614, loss=0.05379666015505791\n",
      "Surface training t=615, loss=0.05477219820022583\n",
      "Surface training t=616, loss=0.05474521778523922\n",
      "Surface training t=617, loss=0.05510600470006466\n",
      "Surface training t=618, loss=0.05579322390258312\n",
      "Surface training t=619, loss=0.05679371394217014\n",
      "Surface training t=620, loss=0.054022666066884995\n",
      "Surface training t=621, loss=0.05386005714535713\n",
      "Surface training t=622, loss=0.05473719723522663\n",
      "Surface training t=623, loss=0.05242111161351204\n",
      "Surface training t=624, loss=0.05438590049743652\n",
      "Surface training t=625, loss=0.057103414088487625\n",
      "Surface training t=626, loss=0.058302419260144234\n",
      "Surface training t=627, loss=0.05307595245540142\n",
      "Surface training t=628, loss=0.0528623778373003\n",
      "Surface training t=629, loss=0.05195446498692036\n",
      "Surface training t=630, loss=0.05386924743652344\n",
      "Surface training t=631, loss=0.052716679871082306\n",
      "Surface training t=632, loss=0.053231995552778244\n",
      "Surface training t=633, loss=0.05738183110952377\n",
      "Surface training t=634, loss=0.05863015167415142\n",
      "Surface training t=635, loss=0.0556202307343483\n",
      "Surface training t=636, loss=0.055536504834890366\n",
      "Surface training t=637, loss=0.051526864990592\n",
      "Surface training t=638, loss=0.05291217193007469\n",
      "Surface training t=639, loss=0.05570852942764759\n",
      "Surface training t=640, loss=0.05694017373025417\n",
      "Surface training t=641, loss=0.05504926107823849\n",
      "Surface training t=642, loss=0.056160612031817436\n",
      "Surface training t=643, loss=0.05525374971330166\n",
      "Surface training t=644, loss=0.05230500176548958\n",
      "Surface training t=645, loss=0.05397772789001465\n",
      "Surface training t=646, loss=0.05239563807845116\n",
      "Surface training t=647, loss=0.054203638806939125\n",
      "Surface training t=648, loss=0.05721753463149071\n",
      "Surface training t=649, loss=0.054268935695290565\n",
      "Surface training t=650, loss=0.05381050892174244\n",
      "Surface training t=651, loss=0.05456734821200371\n",
      "Surface training t=652, loss=0.05356057360768318\n",
      "Surface training t=653, loss=0.05333870276808739\n",
      "Surface training t=654, loss=0.05482024326920509\n",
      "Surface training t=655, loss=0.056871695443987846\n",
      "Surface training t=656, loss=0.0540488064289093\n",
      "Surface training t=657, loss=0.05623142793774605\n",
      "Surface training t=658, loss=0.054597873240709305\n",
      "Surface training t=659, loss=0.05222874693572521\n",
      "Surface training t=660, loss=0.05269354581832886\n",
      "Surface training t=661, loss=0.05174165591597557\n",
      "Surface training t=662, loss=0.05332542583346367\n",
      "Surface training t=663, loss=0.05477781407535076\n",
      "Surface training t=664, loss=0.056619176641106606\n",
      "Surface training t=665, loss=0.05499620735645294\n",
      "Surface training t=666, loss=0.0561065711081028\n",
      "Surface training t=667, loss=0.054197633638978004\n",
      "Surface training t=668, loss=0.05341123417019844\n",
      "Surface training t=669, loss=0.05421414412558079\n",
      "Surface training t=670, loss=0.05321156606078148\n",
      "Surface training t=671, loss=0.051913242787122726\n",
      "Surface training t=672, loss=0.05110776610672474\n",
      "Surface training t=673, loss=0.05407210998237133\n",
      "Surface training t=674, loss=0.05187114700675011\n",
      "Surface training t=675, loss=0.05420398712158203\n",
      "Surface training t=676, loss=0.055925026535987854\n",
      "Surface training t=677, loss=0.056654393672943115\n",
      "Surface training t=678, loss=0.05634826235473156\n",
      "Surface training t=679, loss=0.054938904941082\n",
      "Surface training t=680, loss=0.05162115581333637\n",
      "Surface training t=681, loss=0.0536147840321064\n",
      "Surface training t=682, loss=0.053326912224292755\n",
      "Surface training t=683, loss=0.05411061644554138\n",
      "Surface training t=684, loss=0.056552184745669365\n",
      "Surface training t=685, loss=0.05376238934695721\n",
      "Surface training t=686, loss=0.054745014756917953\n",
      "Surface training t=687, loss=0.05368814058601856\n",
      "Surface training t=688, loss=0.053380921483039856\n",
      "Surface training t=689, loss=0.05261029489338398\n",
      "Surface training t=690, loss=0.05386188440024853\n",
      "Surface training t=691, loss=0.05568012036383152\n",
      "Surface training t=692, loss=0.054672056809067726\n",
      "Surface training t=693, loss=0.05432867258787155\n",
      "Surface training t=694, loss=0.05393754132091999\n",
      "Surface training t=695, loss=0.051623933017253876\n",
      "Surface training t=696, loss=0.054153937846422195\n",
      "Surface training t=697, loss=0.054368434473872185\n",
      "Surface training t=698, loss=0.05515247583389282\n",
      "Surface training t=699, loss=0.054596517235040665\n",
      "Surface training t=700, loss=0.05403275042772293\n",
      "Surface training t=701, loss=0.05281502194702625\n",
      "Surface training t=702, loss=0.05053202249109745\n",
      "Surface training t=703, loss=0.05379204824566841\n",
      "Surface training t=704, loss=0.05203180015087128\n",
      "Surface training t=705, loss=0.05344012752175331\n",
      "Surface training t=706, loss=0.056404322385787964\n",
      "Surface training t=707, loss=0.0542831402271986\n",
      "Surface training t=708, loss=0.05327872186899185\n",
      "Surface training t=709, loss=0.05303681269288063\n",
      "Surface training t=710, loss=0.05165761336684227\n",
      "Surface training t=711, loss=0.05252007767558098\n",
      "Surface training t=712, loss=0.05297472141683102\n",
      "Surface training t=713, loss=0.05401984043419361\n",
      "Surface training t=714, loss=0.05440876446664333\n",
      "Surface training t=715, loss=0.05343971401453018\n",
      "Surface training t=716, loss=0.05655955336987972\n",
      "Surface training t=717, loss=0.05506157875061035\n",
      "Surface training t=718, loss=0.05433257296681404\n",
      "Surface training t=719, loss=0.05319621041417122\n",
      "Surface training t=720, loss=0.05360984988510609\n",
      "Surface training t=721, loss=0.05103380233049393\n",
      "Surface training t=722, loss=0.05250241048634052\n",
      "Surface training t=723, loss=0.052532000467181206\n",
      "Surface training t=724, loss=0.053251082077622414\n",
      "Surface training t=725, loss=0.05589086934924126\n",
      "Surface training t=726, loss=0.05761157162487507\n",
      "Surface training t=727, loss=0.0533317681401968\n",
      "Surface training t=728, loss=0.055654922500252724\n",
      "Surface training t=729, loss=0.05351681634783745\n",
      "Surface training t=730, loss=0.0507305134087801\n",
      "Surface training t=731, loss=0.0517132505774498\n",
      "Surface training t=732, loss=0.052400389686226845\n",
      "Surface training t=733, loss=0.05585962161421776\n",
      "Surface training t=734, loss=0.055429793894290924\n",
      "Surface training t=735, loss=0.05469323880970478\n",
      "Surface training t=736, loss=0.0521231722086668\n",
      "Surface training t=737, loss=0.0533701591193676\n",
      "Surface training t=738, loss=0.05327325314283371\n",
      "Surface training t=739, loss=0.05444253981113434\n",
      "Surface training t=740, loss=0.05429812893271446\n",
      "Surface training t=741, loss=0.052081478759646416\n",
      "Surface training t=742, loss=0.052943864837288857\n",
      "Surface training t=743, loss=0.0544899869710207\n",
      "Surface training t=744, loss=0.051723748445510864\n",
      "Surface training t=745, loss=0.05470498651266098\n",
      "Surface training t=746, loss=0.052828771993517876\n",
      "Surface training t=747, loss=0.05275898054242134\n",
      "Surface training t=748, loss=0.05162673257291317\n",
      "Surface training t=749, loss=0.053730929270386696\n",
      "Surface training t=750, loss=0.053167544305324554\n",
      "Surface training t=751, loss=0.05595006234943867\n",
      "Surface training t=752, loss=0.05454598180949688\n",
      "Surface training t=753, loss=0.055140260607004166\n",
      "Surface training t=754, loss=0.05403583683073521\n",
      "Surface training t=755, loss=0.05290951766073704\n",
      "Surface training t=756, loss=0.050639521330595016\n",
      "Surface training t=757, loss=0.051462482661008835\n",
      "Surface training t=758, loss=0.05175923742353916\n",
      "Surface training t=759, loss=0.05050615407526493\n",
      "Surface training t=760, loss=0.05138233117759228\n",
      "Surface training t=761, loss=0.049607014283537865\n",
      "Surface training t=762, loss=0.04863809607923031\n",
      "Surface training t=763, loss=0.050186268985271454\n",
      "Surface training t=764, loss=0.05019591934978962\n",
      "Surface training t=765, loss=0.05155005306005478\n",
      "Surface training t=766, loss=0.05462350323796272\n",
      "Surface training t=767, loss=0.05248907767236233\n",
      "Surface training t=768, loss=0.05524093843996525\n",
      "Surface training t=769, loss=0.057831183075904846\n",
      "Surface training t=770, loss=0.05603083036839962\n",
      "Surface training t=771, loss=0.05451519973576069\n",
      "Surface training t=772, loss=0.05295871943235397\n",
      "Surface training t=773, loss=0.05356982909142971\n",
      "Surface training t=774, loss=0.053307728841900826\n",
      "Surface training t=775, loss=0.05361679568886757\n",
      "Surface training t=776, loss=0.053488707169890404\n",
      "Surface training t=777, loss=0.05239644646644592\n",
      "Surface training t=778, loss=0.05223650299012661\n",
      "Surface training t=779, loss=0.05244656652212143\n",
      "Surface training t=780, loss=0.051320044323801994\n",
      "Surface training t=781, loss=0.05209230072796345\n",
      "Surface training t=782, loss=0.052927933633327484\n",
      "Surface training t=783, loss=0.053978096693754196\n",
      "Surface training t=784, loss=0.05259132757782936\n",
      "Surface training t=785, loss=0.05157807283103466\n",
      "Surface training t=786, loss=0.05096384696662426\n",
      "Surface training t=787, loss=0.05162976123392582\n",
      "Surface training t=788, loss=0.05342250131070614\n",
      "Surface training t=789, loss=0.05546118877828121\n",
      "Surface training t=790, loss=0.05383583344519138\n",
      "Surface training t=791, loss=0.05211431160569191\n",
      "Surface training t=792, loss=0.04968240670859814\n",
      "Surface training t=793, loss=0.052333615720272064\n",
      "Surface training t=794, loss=0.05255487747490406\n",
      "Surface training t=795, loss=0.05038551799952984\n",
      "Surface training t=796, loss=0.04985469952225685\n",
      "Surface training t=797, loss=0.05164737440645695\n",
      "Surface training t=798, loss=0.05164021626114845\n",
      "Surface training t=799, loss=0.05156549997627735\n",
      "Surface training t=800, loss=0.05219534412026405\n",
      "Surface training t=801, loss=0.05098727159202099\n",
      "Surface training t=802, loss=0.05057574808597565\n",
      "Surface training t=803, loss=0.04959486424922943\n",
      "Surface training t=804, loss=0.05076245032250881\n",
      "Surface training t=805, loss=0.0502559132874012\n",
      "Surface training t=806, loss=0.0504140630364418\n",
      "Surface training t=807, loss=0.05154886841773987\n",
      "Surface training t=808, loss=0.04776980169117451\n",
      "Surface training t=809, loss=0.051053497940301895\n",
      "Surface training t=810, loss=0.04988241754472256\n",
      "Surface training t=811, loss=0.052873704582452774\n",
      "Surface training t=812, loss=0.05070675164461136\n",
      "Surface training t=813, loss=0.04977813549339771\n",
      "Surface training t=814, loss=0.049958791583776474\n",
      "Surface training t=815, loss=0.0513741672039032\n",
      "Surface training t=816, loss=0.04941847175359726\n",
      "Surface training t=817, loss=0.05051346868276596\n",
      "Surface training t=818, loss=0.05122422054409981\n",
      "Surface training t=819, loss=0.04990386217832565\n",
      "Surface training t=820, loss=0.05139327608048916\n",
      "Surface training t=821, loss=0.05028087645769119\n",
      "Surface training t=822, loss=0.04964674264192581\n",
      "Surface training t=823, loss=0.05118977464735508\n",
      "Surface training t=824, loss=0.0509908702224493\n",
      "Surface training t=825, loss=0.05001229606568813\n",
      "Surface training t=826, loss=0.05125563032925129\n",
      "Surface training t=827, loss=0.050291743129491806\n",
      "Surface training t=828, loss=0.05029059946537018\n",
      "Surface training t=829, loss=0.04953101836144924\n",
      "Surface training t=830, loss=0.04725000075995922\n",
      "Surface training t=831, loss=0.048244258388876915\n",
      "Surface training t=832, loss=0.049699146300554276\n",
      "Surface training t=833, loss=0.05032677762210369\n",
      "Surface training t=834, loss=0.05005488358438015\n",
      "Surface training t=835, loss=0.051419831812381744\n",
      "Surface training t=836, loss=0.050306422635912895\n",
      "Surface training t=837, loss=0.047689566388726234\n",
      "Surface training t=838, loss=0.050201207399368286\n",
      "Surface training t=839, loss=0.04991249367594719\n",
      "Surface training t=840, loss=0.05054047331213951\n",
      "Surface training t=841, loss=0.04965237155556679\n",
      "Surface training t=842, loss=0.050547003746032715\n",
      "Surface training t=843, loss=0.049127623438835144\n",
      "Surface training t=844, loss=0.04825477488338947\n",
      "Surface training t=845, loss=0.049885112792253494\n",
      "Surface training t=846, loss=0.05043577402830124\n",
      "Surface training t=847, loss=0.049208514392375946\n",
      "Surface training t=848, loss=0.050097331404685974\n",
      "Surface training t=849, loss=0.049365777522325516\n",
      "Surface training t=850, loss=0.05072516202926636\n",
      "Surface training t=851, loss=0.048680197447538376\n",
      "Surface training t=852, loss=0.04975111782550812\n",
      "Surface training t=853, loss=0.04870249889791012\n",
      "Surface training t=854, loss=0.04953213222324848\n",
      "Surface training t=855, loss=0.048777515068650246\n",
      "Surface training t=856, loss=0.05002708174288273\n",
      "Surface training t=857, loss=0.05343293398618698\n",
      "Surface training t=858, loss=0.05050988122820854\n",
      "Surface training t=859, loss=0.05007852055132389\n",
      "Surface training t=860, loss=0.05046279914677143\n",
      "Surface training t=861, loss=0.04796637035906315\n",
      "Surface training t=862, loss=0.050142792984843254\n",
      "Surface training t=863, loss=0.04924744740128517\n",
      "Surface training t=864, loss=0.05042294226586819\n",
      "Surface training t=865, loss=0.04974952153861523\n",
      "Surface training t=866, loss=0.05003999173641205\n",
      "Surface training t=867, loss=0.05154771916568279\n",
      "Surface training t=868, loss=0.049916770309209824\n",
      "Surface training t=869, loss=0.05045436508953571\n",
      "Surface training t=870, loss=0.0496149156242609\n",
      "Surface training t=871, loss=0.04942798428237438\n",
      "Surface training t=872, loss=0.05135917104780674\n",
      "Surface training t=873, loss=0.04937286674976349\n",
      "Surface training t=874, loss=0.05099731124937534\n",
      "Surface training t=875, loss=0.04959087260067463\n",
      "Surface training t=876, loss=0.049666404724121094\n",
      "Surface training t=877, loss=0.05016070231795311\n",
      "Surface training t=878, loss=0.04953356459736824\n",
      "Surface training t=879, loss=0.052569679915905\n",
      "Surface training t=880, loss=0.04961225390434265\n",
      "Surface training t=881, loss=0.04978644661605358\n",
      "Surface training t=882, loss=0.050348732620477676\n",
      "Surface training t=883, loss=0.04873194359242916\n",
      "Surface training t=884, loss=0.04999218322336674\n",
      "Surface training t=885, loss=0.05051736533641815\n",
      "Surface training t=886, loss=0.04837953299283981\n",
      "Surface training t=887, loss=0.05133636109530926\n",
      "Surface training t=888, loss=0.051586735993623734\n",
      "Surface training t=889, loss=0.05196672864258289\n",
      "Surface training t=890, loss=0.05106125399470329\n",
      "Surface training t=891, loss=0.05179252661764622\n",
      "Surface training t=892, loss=0.049911290407180786\n",
      "Surface training t=893, loss=0.04950847290456295\n",
      "Surface training t=894, loss=0.051885828375816345\n",
      "Surface training t=895, loss=0.05120197683572769\n",
      "Surface training t=896, loss=0.05297167971730232\n",
      "Surface training t=897, loss=0.05330502986907959\n",
      "Surface training t=898, loss=0.050526540726423264\n",
      "Surface training t=899, loss=0.050387006253004074\n",
      "Surface training t=900, loss=0.04941241070628166\n",
      "Surface training t=901, loss=0.04889263957738876\n",
      "Surface training t=902, loss=0.05131771042943001\n",
      "Surface training t=903, loss=0.04924502223730087\n",
      "Surface training t=904, loss=0.05144546367228031\n",
      "Surface training t=905, loss=0.04894248768687248\n",
      "Surface training t=906, loss=0.0503875557333231\n",
      "Surface training t=907, loss=0.049566034227609634\n",
      "Surface training t=908, loss=0.0493181012570858\n",
      "Surface training t=909, loss=0.04946901090443134\n",
      "Surface training t=910, loss=0.04918288625776768\n",
      "Surface training t=911, loss=0.05119425617158413\n",
      "Surface training t=912, loss=0.05065992660820484\n",
      "Surface training t=913, loss=0.05088433623313904\n",
      "Surface training t=914, loss=0.04950928874313831\n",
      "Surface training t=915, loss=0.0493962075561285\n",
      "Surface training t=916, loss=0.04750676639378071\n",
      "Surface training t=917, loss=0.049182791262865067\n",
      "Surface training t=918, loss=0.05170295573771\n",
      "Surface training t=919, loss=0.049951205030083656\n",
      "Surface training t=920, loss=0.04932753928005695\n",
      "Surface training t=921, loss=0.050946351140737534\n",
      "Surface training t=922, loss=0.049600204452872276\n",
      "Surface training t=923, loss=0.0492396242916584\n",
      "Surface training t=924, loss=0.050037264823913574\n",
      "Surface training t=925, loss=0.04942202754318714\n",
      "Surface training t=926, loss=0.0499648991972208\n",
      "Surface training t=927, loss=0.051477277651429176\n",
      "Surface training t=928, loss=0.05273710563778877\n",
      "Surface training t=929, loss=0.05114930868148804\n",
      "Surface training t=930, loss=0.04871034622192383\n",
      "Surface training t=931, loss=0.04971790127456188\n",
      "Surface training t=932, loss=0.050451887771487236\n",
      "Surface training t=933, loss=0.04986274801194668\n",
      "Surface training t=934, loss=0.05067205801606178\n",
      "Surface training t=935, loss=0.04969767481088638\n",
      "Surface training t=936, loss=0.04938601143658161\n",
      "Surface training t=937, loss=0.04996133781969547\n",
      "Surface training t=938, loss=0.04865022003650665\n",
      "Surface training t=939, loss=0.05034913681447506\n",
      "Surface training t=940, loss=0.0484690610319376\n",
      "Surface training t=941, loss=0.05283566936850548\n",
      "Surface training t=942, loss=0.050222812220454216\n",
      "Surface training t=943, loss=0.05305732227861881\n",
      "Surface training t=944, loss=0.051491398364305496\n",
      "Surface training t=945, loss=0.04937298968434334\n",
      "Surface training t=946, loss=0.05156821571290493\n",
      "Surface training t=947, loss=0.050657350569963455\n",
      "Surface training t=948, loss=0.05192948319017887\n",
      "Surface training t=949, loss=0.05243309400975704\n",
      "Surface training t=950, loss=0.048693425953388214\n",
      "Surface training t=951, loss=0.0495159886777401\n",
      "Surface training t=952, loss=0.04973912797868252\n",
      "Surface training t=953, loss=0.051834046840667725\n",
      "Surface training t=954, loss=0.049515971913933754\n",
      "Surface training t=955, loss=0.05092911049723625\n",
      "Surface training t=956, loss=0.049275463446974754\n",
      "Surface training t=957, loss=0.05079515464603901\n",
      "Surface training t=958, loss=0.05182027071714401\n",
      "Surface training t=959, loss=0.049613479524850845\n",
      "Surface training t=960, loss=0.05002790503203869\n",
      "Surface training t=961, loss=0.05179350823163986\n",
      "Surface training t=962, loss=0.05061513930559158\n",
      "Surface training t=963, loss=0.05042674578726292\n",
      "Surface training t=964, loss=0.04880421794950962\n",
      "Surface training t=965, loss=0.04989161528646946\n",
      "Surface training t=966, loss=0.05215503089129925\n",
      "Surface training t=967, loss=0.04899395816028118\n",
      "Surface training t=968, loss=0.049074359238147736\n",
      "Surface training t=969, loss=0.05012325011193752\n",
      "Surface training t=970, loss=0.0489206500351429\n",
      "Surface training t=971, loss=0.0476676058024168\n",
      "Surface training t=972, loss=0.050226759165525436\n",
      "Surface training t=973, loss=0.050549209117889404\n",
      "Surface training t=974, loss=0.04979642666876316\n",
      "Surface training t=975, loss=0.04895346611738205\n",
      "Surface training t=976, loss=0.050120849162340164\n",
      "Surface training t=977, loss=0.050425754860043526\n",
      "Surface training t=978, loss=0.0485609695315361\n",
      "Surface training t=979, loss=0.05015522241592407\n",
      "Surface training t=980, loss=0.05166907608509064\n",
      "Surface training t=981, loss=0.05022084526717663\n",
      "Surface training t=982, loss=0.04914339631795883\n",
      "Surface training t=983, loss=0.050273286178708076\n",
      "Surface training t=984, loss=0.048498183488845825\n",
      "Surface training t=985, loss=0.04890751652419567\n",
      "Surface training t=986, loss=0.049613965675234795\n",
      "Surface training t=987, loss=0.05099792592227459\n",
      "Surface training t=988, loss=0.04933011159300804\n",
      "Surface training t=989, loss=0.049604250118136406\n",
      "Surface training t=990, loss=0.04980883188545704\n",
      "Surface training t=991, loss=0.050012582913041115\n",
      "Surface training t=992, loss=0.050570450723171234\n",
      "Surface training t=993, loss=0.05002971552312374\n",
      "Surface training t=994, loss=0.049863722175359726\n",
      "Surface training t=995, loss=0.049558039754629135\n",
      "Surface training t=996, loss=0.049506569281220436\n",
      "Surface training t=997, loss=0.05200081504881382\n",
      "Surface training t=998, loss=0.05348815582692623\n",
      "Surface training t=999, loss=0.05698723345994949\n",
      "Surface training t=1000, loss=0.056544193997979164\n",
      "Surface training t=1001, loss=0.05211427062749863\n",
      "Surface training t=1002, loss=0.05288875102996826\n",
      "Surface training t=1003, loss=0.05225767195224762\n",
      "Surface training t=1004, loss=0.052768636494874954\n",
      "Surface training t=1005, loss=0.05476087145507336\n",
      "Surface training t=1006, loss=0.053697556257247925\n",
      "Surface training t=1007, loss=0.05483417958021164\n",
      "Surface training t=1008, loss=0.051649728789925575\n",
      "Surface training t=1009, loss=0.05032133869826794\n",
      "Surface training t=1010, loss=0.053890535607934\n",
      "Surface training t=1011, loss=0.050382884219288826\n",
      "Surface training t=1012, loss=0.0506593082100153\n",
      "Surface training t=1013, loss=0.05141295678913593\n",
      "Surface training t=1014, loss=0.0503919143229723\n",
      "Surface training t=1015, loss=0.05066474340856075\n",
      "Surface training t=1016, loss=0.049575578421354294\n",
      "Surface training t=1017, loss=0.04919406957924366\n",
      "Surface training t=1018, loss=0.050000712275505066\n",
      "Surface training t=1019, loss=0.048386212438344955\n",
      "Surface training t=1020, loss=0.04851103201508522\n",
      "Surface training t=1021, loss=0.04880940169095993\n",
      "Surface training t=1022, loss=0.04889800772070885\n",
      "Surface training t=1023, loss=0.04796789586544037\n",
      "Surface training t=1024, loss=0.04959091730415821\n",
      "Surface training t=1025, loss=0.04859178885817528\n",
      "Surface training t=1026, loss=0.04921436123549938\n",
      "Surface training t=1027, loss=0.04964111000299454\n",
      "Surface training t=1028, loss=0.0497148223221302\n",
      "Surface training t=1029, loss=0.05093764513731003\n",
      "Surface training t=1030, loss=0.049921274185180664\n",
      "Surface training t=1031, loss=0.04934081621468067\n",
      "Surface training t=1032, loss=0.050458312034606934\n",
      "Surface training t=1033, loss=0.04814984276890755\n",
      "Surface training t=1034, loss=0.04850667528808117\n",
      "Surface training t=1035, loss=0.05046221613883972\n",
      "Surface training t=1036, loss=0.04893546365201473\n",
      "Surface training t=1037, loss=0.05201190710067749\n",
      "Surface training t=1038, loss=0.050676172599196434\n",
      "Surface training t=1039, loss=0.050085654482245445\n",
      "Surface training t=1040, loss=0.05136723443865776\n",
      "Surface training t=1041, loss=0.051651731133461\n",
      "Surface training t=1042, loss=0.052253233268857\n",
      "Surface training t=1043, loss=0.05074085295200348\n",
      "Surface training t=1044, loss=0.050498662516474724\n",
      "Surface training t=1045, loss=0.050233615562319756\n",
      "Surface training t=1046, loss=0.05042938329279423\n",
      "Surface training t=1047, loss=0.05140148103237152\n",
      "Surface training t=1048, loss=0.0493432842195034\n",
      "Surface training t=1049, loss=0.049180906265974045\n",
      "Surface training t=1050, loss=0.050812674686312675\n",
      "Surface training t=1051, loss=0.050405558198690414\n",
      "Surface training t=1052, loss=0.050188222900033\n",
      "Surface training t=1053, loss=0.04808061011135578\n",
      "Surface training t=1054, loss=0.04970850609242916\n",
      "Surface training t=1055, loss=0.05051454156637192\n",
      "Surface training t=1056, loss=0.0498997513204813\n",
      "Surface training t=1057, loss=0.05060567520558834\n",
      "Surface training t=1058, loss=0.04939432255923748\n",
      "Surface training t=1059, loss=0.048715947195887566\n",
      "Surface training t=1060, loss=0.05094427987933159\n",
      "Surface training t=1061, loss=0.05202978663146496\n",
      "Surface training t=1062, loss=0.05247960053384304\n",
      "Surface training t=1063, loss=0.05016043595969677\n",
      "Surface training t=1064, loss=0.04972534999251366\n",
      "Surface training t=1065, loss=0.049773307517170906\n",
      "Surface training t=1066, loss=0.04851348698139191\n",
      "Surface training t=1067, loss=0.04970816522836685\n",
      "Surface training t=1068, loss=0.05167650617659092\n",
      "Surface training t=1069, loss=0.050354497507214546\n",
      "Surface training t=1070, loss=0.05034419149160385\n",
      "Surface training t=1071, loss=0.04847235046327114\n",
      "Surface training t=1072, loss=0.053010184317827225\n",
      "Surface training t=1073, loss=0.05074167624115944\n",
      "Surface training t=1074, loss=0.04905792698264122\n",
      "Surface training t=1075, loss=0.0508752167224884\n",
      "Surface training t=1076, loss=0.04951410926878452\n",
      "Surface training t=1077, loss=0.04898344725370407\n",
      "Surface training t=1078, loss=0.05144796893000603\n",
      "Surface training t=1079, loss=0.05033749155700207\n",
      "Surface training t=1080, loss=0.05334624648094177\n",
      "Surface training t=1081, loss=0.0501115657389164\n",
      "Surface training t=1082, loss=0.04910765215754509\n",
      "Surface training t=1083, loss=0.05079243890941143\n",
      "Surface training t=1084, loss=0.049385542050004005\n",
      "Surface training t=1085, loss=0.04914206080138683\n",
      "Surface training t=1086, loss=0.048886047676205635\n",
      "Surface training t=1087, loss=0.04921995662152767\n",
      "Surface training t=1088, loss=0.049300285056233406\n",
      "Surface training t=1089, loss=0.04845239408314228\n",
      "Surface training t=1090, loss=0.048931218683719635\n",
      "Surface training t=1091, loss=0.0506218746304512\n",
      "Surface training t=1092, loss=0.04948837123811245\n",
      "Surface training t=1093, loss=0.04900122061371803\n",
      "Surface training t=1094, loss=0.04952290095388889\n",
      "Surface training t=1095, loss=0.04758261889219284\n",
      "Surface training t=1096, loss=0.04937005788087845\n",
      "Surface training t=1097, loss=0.05009314604103565\n",
      "Surface training t=1098, loss=0.05208226479589939\n",
      "Surface training t=1099, loss=0.05098168924450874\n",
      "Surface training t=1100, loss=0.049868013709783554\n",
      "Surface training t=1101, loss=0.049395984038710594\n",
      "Surface training t=1102, loss=0.05142359435558319\n",
      "Surface training t=1103, loss=0.04958983324468136\n",
      "Surface training t=1104, loss=0.04929063282907009\n",
      "Surface training t=1105, loss=0.05221020616590977\n",
      "Surface training t=1106, loss=0.05099847540259361\n",
      "Surface training t=1107, loss=0.049034830182790756\n",
      "Surface training t=1108, loss=0.0505038034170866\n",
      "Surface training t=1109, loss=0.050994182005524635\n",
      "Surface training t=1110, loss=0.04857273027300835\n",
      "Surface training t=1111, loss=0.04863577522337437\n",
      "Surface training t=1112, loss=0.05081845261156559\n",
      "Surface training t=1113, loss=0.04783770814538002\n",
      "Surface training t=1114, loss=0.050696929916739464\n",
      "Surface training t=1115, loss=0.04735096916556358\n",
      "Surface training t=1116, loss=0.048495883122086525\n",
      "Surface training t=1117, loss=0.05054704658687115\n",
      "Surface training t=1118, loss=0.047649774700403214\n",
      "Surface training t=1119, loss=0.04870826564729214\n",
      "Surface training t=1120, loss=0.049643946811556816\n",
      "Surface training t=1121, loss=0.04801340401172638\n",
      "Surface training t=1122, loss=0.04883906617760658\n",
      "Surface training t=1123, loss=0.048557667061686516\n",
      "Surface training t=1124, loss=0.049788111820816994\n",
      "Surface training t=1125, loss=0.04891343414783478\n",
      "Surface training t=1126, loss=0.051191363483667374\n",
      "Surface training t=1127, loss=0.049344053491950035\n",
      "Surface training t=1128, loss=0.04713192768394947\n",
      "Surface training t=1129, loss=0.04835972189903259\n",
      "Surface training t=1130, loss=0.048859572038054466\n",
      "Surface training t=1131, loss=0.05042174458503723\n",
      "Surface training t=1132, loss=0.04875393584370613\n",
      "Surface training t=1133, loss=0.04908733628690243\n",
      "Surface training t=1134, loss=0.04809706099331379\n",
      "Surface training t=1135, loss=0.05158346705138683\n",
      "Surface training t=1136, loss=0.04842082969844341\n",
      "Surface training t=1137, loss=0.04936070926487446\n",
      "Surface training t=1138, loss=0.04748661629855633\n",
      "Surface training t=1139, loss=0.050835613161325455\n",
      "Surface training t=1140, loss=0.05186270922422409\n",
      "Surface training t=1141, loss=0.049668656662106514\n",
      "Surface training t=1142, loss=0.05100950039923191\n",
      "Surface training t=1143, loss=0.04959179274737835\n",
      "Surface training t=1144, loss=0.049582917243242264\n",
      "Surface training t=1145, loss=0.05011883191764355\n",
      "Surface training t=1146, loss=0.050364239141345024\n",
      "Surface training t=1147, loss=0.0496198870241642\n",
      "Surface training t=1148, loss=0.050692103803157806\n",
      "Surface training t=1149, loss=0.0502618532627821\n",
      "Surface training t=1150, loss=0.050252728164196014\n",
      "Surface training t=1151, loss=0.048844436183571815\n",
      "Surface training t=1152, loss=0.0501180961728096\n",
      "Surface training t=1153, loss=0.05222059786319733\n",
      "Surface training t=1154, loss=0.05016578920185566\n",
      "Surface training t=1155, loss=0.05115457996726036\n",
      "Surface training t=1156, loss=0.04988164082169533\n",
      "Surface training t=1157, loss=0.05085618607699871\n",
      "Surface training t=1158, loss=0.048802325502038\n",
      "Surface training t=1159, loss=0.049746397882699966\n",
      "Surface training t=1160, loss=0.049929266795516014\n",
      "Surface training t=1161, loss=0.04839841090142727\n",
      "Surface training t=1162, loss=0.04863476939499378\n",
      "Surface training t=1163, loss=0.05017408728599548\n",
      "Surface training t=1164, loss=0.04974699020385742\n",
      "Surface training t=1165, loss=0.04879356175661087\n",
      "Surface training t=1166, loss=0.048728542402386665\n",
      "Surface training t=1167, loss=0.04884856380522251\n",
      "Surface training t=1168, loss=0.046975620090961456\n",
      "Surface training t=1169, loss=0.04884856566786766\n",
      "Surface training t=1170, loss=0.049459340050816536\n",
      "Surface training t=1171, loss=0.048625096678733826\n",
      "Surface training t=1172, loss=0.04995676130056381\n",
      "Surface training t=1173, loss=0.049143753945827484\n",
      "Surface training t=1174, loss=0.04899919591844082\n",
      "Surface training t=1175, loss=0.048206305131316185\n",
      "Surface training t=1176, loss=0.04922458156943321\n",
      "Surface training t=1177, loss=0.04872042126953602\n",
      "Surface training t=1178, loss=0.05006462521851063\n",
      "Surface training t=1179, loss=0.04846303351223469\n",
      "Surface training t=1180, loss=0.050860920920968056\n",
      "Surface training t=1181, loss=0.0491670798510313\n",
      "Surface training t=1182, loss=0.05064483731985092\n",
      "Surface training t=1183, loss=0.05067455396056175\n",
      "Surface training t=1184, loss=0.047884443774819374\n",
      "Surface training t=1185, loss=0.04728640243411064\n",
      "Surface training t=1186, loss=0.04941443353891373\n",
      "Surface training t=1187, loss=0.04852437600493431\n",
      "Surface training t=1188, loss=0.047074154019355774\n",
      "Surface training t=1189, loss=0.048834072425961494\n",
      "Surface training t=1190, loss=0.04852316714823246\n",
      "Surface training t=1191, loss=0.049560047686100006\n",
      "Surface training t=1192, loss=0.04746262915432453\n",
      "Surface training t=1193, loss=0.04944019578397274\n",
      "Surface training t=1194, loss=0.048884786665439606\n",
      "Surface training t=1195, loss=0.04759541153907776\n",
      "Surface training t=1196, loss=0.04784092865884304\n",
      "Surface training t=1197, loss=0.04965197667479515\n",
      "Surface training t=1198, loss=0.0496197696775198\n",
      "Surface training t=1199, loss=0.04867091216146946\n",
      "Surface training t=1200, loss=0.04928799904882908\n",
      "Surface training t=1201, loss=0.050049083307385445\n",
      "Surface training t=1202, loss=0.04705849289894104\n",
      "Surface training t=1203, loss=0.04941300489008427\n",
      "Surface training t=1204, loss=0.0491021703928709\n",
      "Surface training t=1205, loss=0.048175614327192307\n",
      "Surface training t=1206, loss=0.05020896904170513\n",
      "Surface training t=1207, loss=0.049231626093387604\n",
      "Surface training t=1208, loss=0.04958804324269295\n",
      "Surface training t=1209, loss=0.05048003979027271\n",
      "Surface training t=1210, loss=0.04938765987753868\n",
      "Surface training t=1211, loss=0.04982776939868927\n",
      "Surface training t=1212, loss=0.04960118420422077\n",
      "Surface training t=1213, loss=0.04803414270281792\n",
      "Surface training t=1214, loss=0.048150330781936646\n",
      "Surface training t=1215, loss=0.050606587901711464\n",
      "Surface training t=1216, loss=0.049363721162080765\n",
      "Surface training t=1217, loss=0.049553319811820984\n",
      "Surface training t=1218, loss=0.05031839571893215\n",
      "Surface training t=1219, loss=0.048632264137268066\n",
      "Surface training t=1220, loss=0.047152696177363396\n",
      "Surface training t=1221, loss=0.04783250205218792\n",
      "Surface training t=1222, loss=0.04888510704040527\n",
      "Surface training t=1223, loss=0.04840758256614208\n",
      "Surface training t=1224, loss=0.04623079299926758\n",
      "Surface training t=1225, loss=0.05034242384135723\n",
      "Surface training t=1226, loss=0.04987485706806183\n",
      "Surface training t=1227, loss=0.050277551636099815\n",
      "Surface training t=1228, loss=0.04796616919338703\n",
      "Surface training t=1229, loss=0.050006864592432976\n",
      "Surface training t=1230, loss=0.05068174935877323\n",
      "Surface training t=1231, loss=0.047115229070186615\n",
      "Surface training t=1232, loss=0.04935438185930252\n",
      "Surface training t=1233, loss=0.047482892870903015\n",
      "Surface training t=1234, loss=0.048510827124118805\n",
      "Surface training t=1235, loss=0.049025099724531174\n",
      "Surface training t=1236, loss=0.047395406290888786\n",
      "Surface training t=1237, loss=0.04795573092997074\n",
      "Surface training t=1238, loss=0.0473643708974123\n",
      "Surface training t=1239, loss=0.04922725446522236\n",
      "Surface training t=1240, loss=0.04659502021968365\n",
      "Surface training t=1241, loss=0.04951882176101208\n",
      "Surface training t=1242, loss=0.048367684707045555\n",
      "Surface training t=1243, loss=0.04958098754286766\n",
      "Surface training t=1244, loss=0.04872728884220123\n",
      "Surface training t=1245, loss=0.050603266805410385\n",
      "Surface training t=1246, loss=0.049340857192873955\n",
      "Surface training t=1247, loss=0.04986112751066685\n",
      "Surface training t=1248, loss=0.0511208102107048\n",
      "Surface training t=1249, loss=0.04863038472831249\n",
      "Surface training t=1250, loss=0.04898886755108833\n",
      "Surface training t=1251, loss=0.05067175813019276\n",
      "Surface training t=1252, loss=0.04943962022662163\n",
      "Surface training t=1253, loss=0.049725065007805824\n",
      "Surface training t=1254, loss=0.05076422914862633\n",
      "Surface training t=1255, loss=0.05098423361778259\n",
      "Surface training t=1256, loss=0.04981205612421036\n",
      "Surface training t=1257, loss=0.05041826330125332\n",
      "Surface training t=1258, loss=0.04820922575891018\n",
      "Surface training t=1259, loss=0.04968559369444847\n",
      "Surface training t=1260, loss=0.05075729079544544\n",
      "Surface training t=1261, loss=0.04847677983343601\n",
      "Surface training t=1262, loss=0.049702705815434456\n",
      "Surface training t=1263, loss=0.04947515204548836\n",
      "Surface training t=1264, loss=0.0497884564101696\n",
      "Surface training t=1265, loss=0.050674717873334885\n",
      "Surface training t=1266, loss=0.050243811681866646\n",
      "Surface training t=1267, loss=0.050066445022821426\n",
      "Surface training t=1268, loss=0.050800926983356476\n",
      "Surface training t=1269, loss=0.050843337550759315\n",
      "Surface training t=1270, loss=0.04836145415902138\n",
      "Surface training t=1271, loss=0.04871210642158985\n",
      "Surface training t=1272, loss=0.048774514347314835\n",
      "Surface training t=1273, loss=0.049726469442248344\n",
      "Surface training t=1274, loss=0.05197601579129696\n",
      "Surface training t=1275, loss=0.04909386299550533\n",
      "Surface training t=1276, loss=0.048238618299365044\n",
      "Surface training t=1277, loss=0.049582986161112785\n",
      "Surface training t=1278, loss=0.048496752977371216\n",
      "Surface training t=1279, loss=0.05035366117954254\n",
      "Surface training t=1280, loss=0.05066067911684513\n",
      "Surface training t=1281, loss=0.04795275628566742\n",
      "Surface training t=1282, loss=0.04942557029426098\n",
      "Surface training t=1283, loss=0.05050200782716274\n",
      "Surface training t=1284, loss=0.051664551720023155\n",
      "Surface training t=1285, loss=0.05127367004752159\n",
      "Surface training t=1286, loss=0.04974075220525265\n",
      "Surface training t=1287, loss=0.05130030773580074\n",
      "Surface training t=1288, loss=0.05127936042845249\n",
      "Surface training t=1289, loss=0.052326902747154236\n",
      "Surface training t=1290, loss=0.05062933266162872\n",
      "Surface training t=1291, loss=0.05194532684981823\n",
      "Surface training t=1292, loss=0.051940254867076874\n",
      "Surface training t=1293, loss=0.053583672270178795\n",
      "Surface training t=1294, loss=0.054528554901480675\n",
      "Surface training t=1295, loss=0.055678173899650574\n",
      "Surface training t=1296, loss=0.05398308485746384\n",
      "Surface training t=1297, loss=0.05319473147392273\n",
      "Surface training t=1298, loss=0.052229734137654305\n",
      "Surface training t=1299, loss=0.05106283165514469\n",
      "Surface training t=1300, loss=0.050296587869524956\n",
      "Surface training t=1301, loss=0.05054572597146034\n",
      "Surface training t=1302, loss=0.05040456913411617\n",
      "Surface training t=1303, loss=0.0504326056689024\n",
      "Surface training t=1304, loss=0.05023629404604435\n",
      "Surface training t=1305, loss=0.05240698531270027\n",
      "Surface training t=1306, loss=0.05153069458901882\n",
      "Surface training t=1307, loss=0.05077827349305153\n",
      "Surface training t=1308, loss=0.052980536594986916\n",
      "Surface training t=1309, loss=0.04991532675921917\n",
      "Surface training t=1310, loss=0.04973830655217171\n",
      "Surface training t=1311, loss=0.04900739528238773\n",
      "Surface training t=1312, loss=0.049718866124749184\n",
      "Surface training t=1313, loss=0.04775581881403923\n",
      "Surface training t=1314, loss=0.047998810186982155\n",
      "Surface training t=1315, loss=0.05015013925731182\n",
      "Surface training t=1316, loss=0.04949383810162544\n",
      "Surface training t=1317, loss=0.04977213218808174\n",
      "Surface training t=1318, loss=0.04762968420982361\n",
      "Surface training t=1319, loss=0.0506795234978199\n",
      "Surface training t=1320, loss=0.04986228048801422\n",
      "Surface training t=1321, loss=0.04884144477546215\n",
      "Surface training t=1322, loss=0.050095416605472565\n",
      "Surface training t=1323, loss=0.050797829404473305\n",
      "Surface training t=1324, loss=0.05038855038583279\n",
      "Surface training t=1325, loss=0.048537254333496094\n",
      "Surface training t=1326, loss=0.049824416637420654\n",
      "Surface training t=1327, loss=0.05037609674036503\n",
      "Surface training t=1328, loss=0.04982775077223778\n",
      "Surface training t=1329, loss=0.04999641329050064\n",
      "Surface training t=1330, loss=0.04860137030482292\n",
      "Surface training t=1331, loss=0.04993819445371628\n",
      "Surface training t=1332, loss=0.04880107194185257\n",
      "Surface training t=1333, loss=0.047101859003305435\n",
      "Surface training t=1334, loss=0.04731664061546326\n",
      "Surface training t=1335, loss=0.04983719810843468\n",
      "Surface training t=1336, loss=0.04831920750439167\n",
      "Surface training t=1337, loss=0.0489827785640955\n",
      "Surface training t=1338, loss=0.04924453981220722\n",
      "Surface training t=1339, loss=0.04885006695985794\n",
      "Surface training t=1340, loss=0.04786176420748234\n",
      "Surface training t=1341, loss=0.04732002876698971\n",
      "Surface training t=1342, loss=0.047771066427230835\n",
      "Surface training t=1343, loss=0.04804749973118305\n",
      "Surface training t=1344, loss=0.04787745326757431\n",
      "Surface training t=1345, loss=0.04857771657407284\n",
      "Surface training t=1346, loss=0.05045062303543091\n",
      "Surface training t=1347, loss=0.048438820987939835\n",
      "Surface training t=1348, loss=0.04865769110620022\n",
      "Surface training t=1349, loss=0.05126273073256016\n",
      "Surface training t=1350, loss=0.04905827157199383\n",
      "Surface training t=1351, loss=0.05046366527676582\n",
      "Surface training t=1352, loss=0.049291716888546944\n",
      "Surface training t=1353, loss=0.04971264488995075\n",
      "Surface training t=1354, loss=0.04880339279770851\n",
      "Surface training t=1355, loss=0.04976809397339821\n",
      "Surface training t=1356, loss=0.05124652199447155\n",
      "Surface training t=1357, loss=0.04932013154029846\n",
      "Surface training t=1358, loss=0.0483320988714695\n",
      "Surface training t=1359, loss=0.04905636049807072\n",
      "Surface training t=1360, loss=0.048220595344901085\n",
      "Surface training t=1361, loss=0.05170163884758949\n",
      "Surface training t=1362, loss=0.04777900502085686\n",
      "Surface training t=1363, loss=0.04889309220016003\n",
      "Surface training t=1364, loss=0.05105286464095116\n",
      "Surface training t=1365, loss=0.04808999411761761\n",
      "Surface training t=1366, loss=0.049683885648846626\n",
      "Surface training t=1367, loss=0.049636851996183395\n",
      "Surface training t=1368, loss=0.047325827181339264\n",
      "Surface training t=1369, loss=0.050014881417155266\n",
      "Surface training t=1370, loss=0.05004745349287987\n",
      "Surface training t=1371, loss=0.04849458858370781\n",
      "Surface training t=1372, loss=0.04872880317270756\n",
      "Surface training t=1373, loss=0.04765111394226551\n",
      "Surface training t=1374, loss=0.049969788640737534\n",
      "Surface training t=1375, loss=0.04806439392268658\n",
      "Surface training t=1376, loss=0.048420488834381104\n",
      "Surface training t=1377, loss=0.05051332712173462\n",
      "Surface training t=1378, loss=0.04878677800297737\n",
      "Surface training t=1379, loss=0.04905159771442413\n",
      "Surface training t=1380, loss=0.048855215311050415\n",
      "Surface training t=1381, loss=0.04979968070983887\n",
      "Surface training t=1382, loss=0.04887951724231243\n",
      "Surface training t=1383, loss=0.04660949110984802\n",
      "Surface training t=1384, loss=0.04902329854667187\n",
      "Surface training t=1385, loss=0.04854882135987282\n",
      "Surface training t=1386, loss=0.04973468743264675\n",
      "Surface training t=1387, loss=0.04892119579017162\n",
      "Surface training t=1388, loss=0.04774307273328304\n",
      "Surface training t=1389, loss=0.047790173441171646\n",
      "Surface training t=1390, loss=0.0480485949665308\n",
      "Surface training t=1391, loss=0.04797263629734516\n",
      "Surface training t=1392, loss=0.048649098724126816\n",
      "Surface training t=1393, loss=0.04760614037513733\n",
      "Surface training t=1394, loss=0.04886403866112232\n",
      "Surface training t=1395, loss=0.04476407915353775\n",
      "Surface training t=1396, loss=0.04723811149597168\n",
      "Surface training t=1397, loss=0.04824009910225868\n",
      "Surface training t=1398, loss=0.0478934608399868\n",
      "Surface training t=1399, loss=0.04897405207157135\n",
      "Surface training t=1400, loss=0.049192680045962334\n",
      "Surface training t=1401, loss=0.04751580394804478\n",
      "Surface training t=1402, loss=0.04752999171614647\n",
      "Surface training t=1403, loss=0.04884253814816475\n",
      "Surface training t=1404, loss=0.04852990061044693\n",
      "Surface training t=1405, loss=0.048573778942227364\n",
      "Surface training t=1406, loss=0.04883456602692604\n",
      "Surface training t=1407, loss=0.047275034710764885\n",
      "Surface training t=1408, loss=0.04851207137107849\n",
      "Surface training t=1409, loss=0.0488545298576355\n",
      "Surface training t=1410, loss=0.04724935069680214\n",
      "Surface training t=1411, loss=0.049336763098835945\n",
      "Surface training t=1412, loss=0.04704039357602596\n",
      "Surface training t=1413, loss=0.04853598214685917\n",
      "Surface training t=1414, loss=0.04821256175637245\n",
      "Surface training t=1415, loss=0.05021212063729763\n",
      "Surface training t=1416, loss=0.04910292103886604\n",
      "Surface training t=1417, loss=0.04623139090836048\n",
      "Surface training t=1418, loss=0.04879383370280266\n",
      "Surface training t=1419, loss=0.04876524396240711\n",
      "Surface training t=1420, loss=0.04708428494632244\n",
      "Surface training t=1421, loss=0.04909244552254677\n",
      "Surface training t=1422, loss=0.0490913912653923\n",
      "Surface training t=1423, loss=0.047929150983691216\n",
      "Surface training t=1424, loss=0.04949050210416317\n",
      "Surface training t=1425, loss=0.048170847818255424\n",
      "Surface training t=1426, loss=0.04778202436864376\n",
      "Surface training t=1427, loss=0.04832570068538189\n",
      "Surface training t=1428, loss=0.04865377955138683\n",
      "Surface training t=1429, loss=0.048209862783551216\n",
      "Surface training t=1430, loss=0.04595412313938141\n",
      "Surface training t=1431, loss=0.048386240378022194\n",
      "Surface training t=1432, loss=0.047564661130309105\n",
      "Surface training t=1433, loss=0.04882052540779114\n",
      "Surface training t=1434, loss=0.04839589260518551\n",
      "Surface training t=1435, loss=0.04891449213027954\n",
      "Surface training t=1436, loss=0.05158657021820545\n",
      "Surface training t=1437, loss=0.04928109236061573\n",
      "Surface training t=1438, loss=0.04839102737605572\n",
      "Surface training t=1439, loss=0.048712847754359245\n",
      "Surface training t=1440, loss=0.048186132684350014\n",
      "Surface training t=1441, loss=0.04821489937603474\n",
      "Surface training t=1442, loss=0.05030884966254234\n",
      "Surface training t=1443, loss=0.05048150569200516\n",
      "Surface training t=1444, loss=0.04832203686237335\n",
      "Surface training t=1445, loss=0.04759140685200691\n",
      "Surface training t=1446, loss=0.05008970759809017\n",
      "Surface training t=1447, loss=0.04894702136516571\n",
      "Surface training t=1448, loss=0.052365781739354134\n",
      "Surface training t=1449, loss=0.051204340532422066\n",
      "Surface training t=1450, loss=0.05116348713636398\n",
      "Surface training t=1451, loss=0.0521231833845377\n",
      "Surface training t=1452, loss=0.05168258771300316\n",
      "Surface training t=1453, loss=0.05521118640899658\n",
      "Surface training t=1454, loss=0.05397812835872173\n",
      "Surface training t=1455, loss=0.053585756570100784\n",
      "Surface training t=1456, loss=0.0516048651188612\n",
      "Surface training t=1457, loss=0.051955943927168846\n",
      "Surface training t=1458, loss=0.05143620818853378\n",
      "Surface training t=1459, loss=0.05104279890656471\n",
      "Surface training t=1460, loss=0.05209892801940441\n",
      "Surface training t=1461, loss=0.05530896224081516\n",
      "Surface training t=1462, loss=0.05384002439677715\n",
      "Surface training t=1463, loss=0.052539922297000885\n",
      "Surface training t=1464, loss=0.053758127614855766\n",
      "Surface training t=1465, loss=0.051590124145150185\n",
      "Surface training t=1466, loss=0.05275270342826843\n",
      "Surface training t=1467, loss=0.05197364278137684\n",
      "Surface training t=1468, loss=0.05274517461657524\n",
      "Surface training t=1469, loss=0.05153653211891651\n",
      "Surface training t=1470, loss=0.05440249480307102\n",
      "Surface training t=1471, loss=0.052966583520174026\n",
      "Surface training t=1472, loss=0.05290232598781586\n",
      "Surface training t=1473, loss=0.05121002905070782\n",
      "Surface training t=1474, loss=0.05247720144689083\n",
      "Surface training t=1475, loss=0.05344967171549797\n",
      "Surface training t=1476, loss=0.05100375413894653\n",
      "Surface training t=1477, loss=0.05230967327952385\n",
      "Surface training t=1478, loss=0.05274206958711147\n",
      "Surface training t=1479, loss=0.05347532592713833\n",
      "Surface training t=1480, loss=0.05299003608524799\n",
      "Surface training t=1481, loss=0.05273193493485451\n",
      "Surface training t=1482, loss=0.05151384696364403\n",
      "Surface training t=1483, loss=0.0519410315901041\n",
      "Surface training t=1484, loss=0.0534602515399456\n",
      "Surface training t=1485, loss=0.05233923718333244\n",
      "Surface training t=1486, loss=0.05037819407880306\n",
      "Surface training t=1487, loss=0.05078797973692417\n",
      "Surface training t=1488, loss=0.0506113525480032\n",
      "Surface training t=1489, loss=0.05150691792368889\n",
      "Surface training t=1490, loss=0.052206484600901604\n",
      "Surface training t=1491, loss=0.054081737995147705\n",
      "Surface training t=1492, loss=0.054456308484077454\n",
      "Surface training t=1493, loss=0.052625084295868874\n",
      "Surface training t=1494, loss=0.051225582137703896\n",
      "Surface training t=1495, loss=0.05044515058398247\n",
      "Surface training t=1496, loss=0.05258389934897423\n",
      "Surface training t=1497, loss=0.05001566559076309\n",
      "Surface training t=1498, loss=0.050728507339954376\n",
      "Surface training t=1499, loss=0.05376206897199154\n",
      "Surface training t=1500, loss=0.05402889847755432\n",
      "Surface training t=1501, loss=0.052531011402606964\n",
      "Surface training t=1502, loss=0.050851570442318916\n",
      "Surface training t=1503, loss=0.05053463205695152\n",
      "Surface training t=1504, loss=0.05102117918431759\n",
      "Surface training t=1505, loss=0.05000648647546768\n",
      "Surface training t=1506, loss=0.05114234238862991\n",
      "Surface training t=1507, loss=0.05115368217229843\n",
      "Surface training t=1508, loss=0.05288194678723812\n",
      "Surface training t=1509, loss=0.050698256120085716\n",
      "Surface training t=1510, loss=0.05323433689773083\n",
      "Surface training t=1511, loss=0.053070854395627975\n",
      "Surface training t=1512, loss=0.05255120061337948\n",
      "Surface training t=1513, loss=0.05167303420603275\n",
      "Surface training t=1514, loss=0.04956106096506119\n",
      "Surface training t=1515, loss=0.048609184101223946\n",
      "Surface training t=1516, loss=0.04822049289941788\n",
      "Surface training t=1517, loss=0.04666198417544365\n",
      "Surface training t=1518, loss=0.04792684316635132\n",
      "Surface training t=1519, loss=0.04916428215801716\n",
      "Surface training t=1520, loss=0.04672081954777241\n",
      "Surface training t=1521, loss=0.049303704872727394\n",
      "Surface training t=1522, loss=0.04900394566357136\n",
      "Surface training t=1523, loss=0.04950845055282116\n",
      "Surface training t=1524, loss=0.049259111285209656\n",
      "Surface training t=1525, loss=0.04702231101691723\n",
      "Surface training t=1526, loss=0.047706238925457\n",
      "Surface training t=1527, loss=0.04853341169655323\n",
      "Surface training t=1528, loss=0.04897055961191654\n",
      "Surface training t=1529, loss=0.04782146401703358\n",
      "Surface training t=1530, loss=0.04811715520918369\n",
      "Surface training t=1531, loss=0.048803823068737984\n",
      "Surface training t=1532, loss=0.048406923189759254\n",
      "Surface training t=1533, loss=0.04815364070236683\n",
      "Surface training t=1534, loss=0.04893017187714577\n",
      "Surface training t=1535, loss=0.0475285779684782\n",
      "Surface training t=1536, loss=0.04797232337296009\n",
      "Surface training t=1537, loss=0.04758820682764053\n",
      "Surface training t=1538, loss=0.046058570966124535\n",
      "Surface training t=1539, loss=0.04872792214155197\n",
      "Surface training t=1540, loss=0.04592149145901203\n",
      "Surface training t=1541, loss=0.04688880033791065\n",
      "Surface training t=1542, loss=0.049519529566168785\n",
      "Surface training t=1543, loss=0.04793434031307697\n",
      "Surface training t=1544, loss=0.04697608761489391\n",
      "Surface training t=1545, loss=0.049628566950559616\n",
      "Surface training t=1546, loss=0.04818732291460037\n",
      "Surface training t=1547, loss=0.049365706741809845\n",
      "Surface training t=1548, loss=0.04676898568868637\n",
      "Surface training t=1549, loss=0.04734368808567524\n",
      "Surface training t=1550, loss=0.04741600714623928\n",
      "Surface training t=1551, loss=0.04778298921883106\n",
      "Surface training t=1552, loss=0.04759679548442364\n",
      "Surface training t=1553, loss=0.04652174189686775\n",
      "Surface training t=1554, loss=0.046913785859942436\n",
      "Surface training t=1555, loss=0.047380246222019196\n",
      "Surface training t=1556, loss=0.04621601291000843\n",
      "Surface training t=1557, loss=0.047328731045126915\n",
      "Surface training t=1558, loss=0.048557789996266365\n",
      "Surface training t=1559, loss=0.049012191593647\n",
      "Surface training t=1560, loss=0.04824758134782314\n",
      "Surface training t=1561, loss=0.04850862920284271\n",
      "Surface training t=1562, loss=0.048029376193881035\n",
      "Surface training t=1563, loss=0.04725633189082146\n",
      "Surface training t=1564, loss=0.047541555017232895\n",
      "Surface training t=1565, loss=0.04844791814684868\n",
      "Surface training t=1566, loss=0.04676878824830055\n",
      "Surface training t=1567, loss=0.047468869015574455\n",
      "Surface training t=1568, loss=0.047517307102680206\n",
      "Surface training t=1569, loss=0.048353319987654686\n",
      "Surface training t=1570, loss=0.04757027328014374\n",
      "Surface training t=1571, loss=0.04859969578683376\n",
      "Surface training t=1572, loss=0.04714992083609104\n",
      "Surface training t=1573, loss=0.048359401524066925\n",
      "Surface training t=1574, loss=0.04591663181781769\n",
      "Surface training t=1575, loss=0.047026846557855606\n",
      "Surface training t=1576, loss=0.048112157732248306\n",
      "Surface training t=1577, loss=0.04857061803340912\n",
      "Surface training t=1578, loss=0.048437003046274185\n",
      "Surface training t=1579, loss=0.04778171703219414\n",
      "Surface training t=1580, loss=0.04927447810769081\n",
      "Surface training t=1581, loss=0.04940674267709255\n",
      "Surface training t=1582, loss=0.049707451835274696\n",
      "Surface training t=1583, loss=0.046416159719228745\n",
      "Surface training t=1584, loss=0.04768858291208744\n",
      "Surface training t=1585, loss=0.04999702051281929\n",
      "Surface training t=1586, loss=0.04816800355911255\n",
      "Surface training t=1587, loss=0.04784168116748333\n",
      "Surface training t=1588, loss=0.04753381386399269\n",
      "Surface training t=1589, loss=0.04890399985015392\n",
      "Surface training t=1590, loss=0.04890104942023754\n",
      "Surface training t=1591, loss=0.04984024539589882\n",
      "Surface training t=1592, loss=0.04679083451628685\n",
      "Surface training t=1593, loss=0.049247873947024345\n",
      "Surface training t=1594, loss=0.047624800354242325\n",
      "Surface training t=1595, loss=0.046461766585707664\n",
      "Surface training t=1596, loss=0.048468032851815224\n",
      "Surface training t=1597, loss=0.0487222783267498\n",
      "Surface training t=1598, loss=0.04760454595088959\n",
      "Surface training t=1599, loss=0.04785773530602455\n",
      "Surface training t=1600, loss=0.0487059336155653\n",
      "Surface training t=1601, loss=0.04654909484088421\n",
      "Surface training t=1602, loss=0.04860123246908188\n",
      "Surface training t=1603, loss=0.04877379909157753\n",
      "Surface training t=1604, loss=0.0495135523378849\n",
      "Surface training t=1605, loss=0.0483629796653986\n",
      "Surface training t=1606, loss=0.04639864340424538\n",
      "Surface training t=1607, loss=0.04950333014130592\n",
      "Surface training t=1608, loss=0.050243595615029335\n",
      "Surface training t=1609, loss=0.050167571753263474\n",
      "Surface training t=1610, loss=0.0502182487398386\n",
      "Surface training t=1611, loss=0.0489465594291687\n",
      "Surface training t=1612, loss=0.048960546031594276\n",
      "Surface training t=1613, loss=0.04912407882511616\n",
      "Surface training t=1614, loss=0.05064249783754349\n",
      "Surface training t=1615, loss=0.04946466349065304\n",
      "Surface training t=1616, loss=0.04608125053346157\n",
      "Surface training t=1617, loss=0.049816541373729706\n",
      "Surface training t=1618, loss=0.04723666049540043\n",
      "Surface training t=1619, loss=0.04744813032448292\n",
      "Surface training t=1620, loss=0.04617362096905708\n",
      "Surface training t=1621, loss=0.04661187715828419\n",
      "Surface training t=1622, loss=0.048001717776060104\n",
      "Surface training t=1623, loss=0.04687378741800785\n",
      "Surface training t=1624, loss=0.04799961484968662\n",
      "Surface training t=1625, loss=0.046367429196834564\n",
      "Surface training t=1626, loss=0.046886635944247246\n",
      "Surface training t=1627, loss=0.047121789306402206\n",
      "Surface training t=1628, loss=0.04778030328452587\n",
      "Surface training t=1629, loss=0.04712565802037716\n",
      "Surface training t=1630, loss=0.04704892262816429\n",
      "Surface training t=1631, loss=0.04953175596892834\n",
      "Surface training t=1632, loss=0.04730375111103058\n",
      "Surface training t=1633, loss=0.04847905412316322\n",
      "Surface training t=1634, loss=0.04986598342657089\n",
      "Surface training t=1635, loss=0.04840514250099659\n",
      "Surface training t=1636, loss=0.048620015382766724\n",
      "Surface training t=1637, loss=0.04881957173347473\n",
      "Surface training t=1638, loss=0.047801580280065536\n",
      "Surface training t=1639, loss=0.04837689734995365\n",
      "Surface training t=1640, loss=0.048041973263025284\n",
      "Surface training t=1641, loss=0.04766440950334072\n",
      "Surface training t=1642, loss=0.047592366114258766\n",
      "Surface training t=1643, loss=0.047630371525883675\n",
      "Surface training t=1644, loss=0.046231212094426155\n",
      "Surface training t=1645, loss=0.048482125625014305\n",
      "Surface training t=1646, loss=0.050924940034747124\n",
      "Surface training t=1647, loss=0.04915734380483627\n",
      "Surface training t=1648, loss=0.0496860072016716\n",
      "Surface training t=1649, loss=0.049017708748579025\n",
      "Surface training t=1650, loss=0.04993816278874874\n",
      "Surface training t=1651, loss=0.04819232597947121\n",
      "Surface training t=1652, loss=0.04915713332593441\n",
      "Surface training t=1653, loss=0.04754991829395294\n",
      "Surface training t=1654, loss=0.04654183238744736\n",
      "Surface training t=1655, loss=0.04916508309543133\n",
      "Surface training t=1656, loss=0.04783707298338413\n",
      "Surface training t=1657, loss=0.04749404080212116\n",
      "Surface training t=1658, loss=0.04808121547102928\n",
      "Surface training t=1659, loss=0.04791640676558018\n",
      "Surface training t=1660, loss=0.0486999936401844\n",
      "Surface training t=1661, loss=0.04675237648189068\n",
      "Surface training t=1662, loss=0.047870783135294914\n",
      "Surface training t=1663, loss=0.04789060726761818\n",
      "Surface training t=1664, loss=0.047593846917152405\n",
      "Surface training t=1665, loss=0.046598250046372414\n",
      "Surface training t=1666, loss=0.04809831641614437\n",
      "Surface training t=1667, loss=0.049210162833333015\n",
      "Surface training t=1668, loss=0.048114676028490067\n",
      "Surface training t=1669, loss=0.04757733456790447\n",
      "Surface training t=1670, loss=0.04955174773931503\n",
      "Surface training t=1671, loss=0.0466702226549387\n",
      "Surface training t=1672, loss=0.04691438563168049\n",
      "Surface training t=1673, loss=0.04778851754963398\n",
      "Surface training t=1674, loss=0.046316958963871\n",
      "Surface training t=1675, loss=0.04799486882984638\n",
      "Surface training t=1676, loss=0.04968073405325413\n",
      "Surface training t=1677, loss=0.04760345444083214\n",
      "Surface training t=1678, loss=0.04725519381463528\n",
      "Surface training t=1679, loss=0.0479102935642004\n",
      "Surface training t=1680, loss=0.04837232455611229\n",
      "Surface training t=1681, loss=0.04762990586459637\n",
      "Surface training t=1682, loss=0.04913818649947643\n",
      "Surface training t=1683, loss=0.04848087951540947\n",
      "Surface training t=1684, loss=0.046074604615569115\n",
      "Surface training t=1685, loss=0.048691876232624054\n",
      "Surface training t=1686, loss=0.04827077314257622\n",
      "Surface training t=1687, loss=0.05128089338541031\n",
      "Surface training t=1688, loss=0.048172727227211\n",
      "Surface training t=1689, loss=0.04741315916180611\n",
      "Surface training t=1690, loss=0.048478858545422554\n",
      "Surface training t=1691, loss=0.047402532771229744\n",
      "Surface training t=1692, loss=0.045562516897916794\n",
      "Surface training t=1693, loss=0.04718659818172455\n",
      "Surface training t=1694, loss=0.04662253335118294\n",
      "Surface training t=1695, loss=0.04833867959678173\n",
      "Surface training t=1696, loss=0.04888935945928097\n",
      "Surface training t=1697, loss=0.044852109625935555\n",
      "Surface training t=1698, loss=0.046957626938819885\n",
      "Surface training t=1699, loss=0.044629018753767014\n",
      "Surface training t=1700, loss=0.04727928899228573\n",
      "Surface training t=1701, loss=0.04605696722865105\n",
      "Surface training t=1702, loss=0.04639657586812973\n",
      "Surface training t=1703, loss=0.046090954914689064\n",
      "Surface training t=1704, loss=0.046950386837124825\n",
      "Surface training t=1705, loss=0.04847049526870251\n",
      "Surface training t=1706, loss=0.04867778345942497\n",
      "Surface training t=1707, loss=0.05115017294883728\n",
      "Surface training t=1708, loss=0.046641889959573746\n",
      "Surface training t=1709, loss=0.04697984270751476\n",
      "Surface training t=1710, loss=0.048169931396842\n",
      "Surface training t=1711, loss=0.048379840329289436\n",
      "Surface training t=1712, loss=0.04704425297677517\n",
      "Surface training t=1713, loss=0.04883096553385258\n",
      "Surface training t=1714, loss=0.0475830752402544\n",
      "Surface training t=1715, loss=0.04664252884685993\n",
      "Surface training t=1716, loss=0.046435922384262085\n",
      "Surface training t=1717, loss=0.04705249145627022\n",
      "Surface training t=1718, loss=0.048236094415187836\n",
      "Surface training t=1719, loss=0.047203073278069496\n",
      "Surface training t=1720, loss=0.04812123626470566\n",
      "Surface training t=1721, loss=0.048115482553839684\n",
      "Surface training t=1722, loss=0.047667063772678375\n",
      "Surface training t=1723, loss=0.049691807478666306\n",
      "Surface training t=1724, loss=0.04716614447534084\n",
      "Surface training t=1725, loss=0.04990244470536709\n",
      "Surface training t=1726, loss=0.04641124978661537\n",
      "Surface training t=1727, loss=0.04806656949222088\n",
      "Surface training t=1728, loss=0.04672546312212944\n",
      "Surface training t=1729, loss=0.04755139909684658\n",
      "Surface training t=1730, loss=0.04886634089052677\n",
      "Surface training t=1731, loss=0.04747632518410683\n",
      "Surface training t=1732, loss=0.04891298525035381\n",
      "Surface training t=1733, loss=0.04741985537111759\n",
      "Surface training t=1734, loss=0.04962283931672573\n",
      "Surface training t=1735, loss=0.0498739555478096\n",
      "Surface training t=1736, loss=0.0506680179387331\n",
      "Surface training t=1737, loss=0.04836416244506836\n",
      "Surface training t=1738, loss=0.04733790270984173\n",
      "Surface training t=1739, loss=0.049195459112524986\n",
      "Surface training t=1740, loss=0.04943164438009262\n",
      "Surface training t=1741, loss=0.051540035754442215\n",
      "Surface training t=1742, loss=0.05356510542333126\n",
      "Surface training t=1743, loss=0.05215281620621681\n",
      "Surface training t=1744, loss=0.05132942460477352\n",
      "Surface training t=1745, loss=0.05016680806875229\n",
      "Surface training t=1746, loss=0.04897625744342804\n",
      "Surface training t=1747, loss=0.04811737313866615\n",
      "Surface training t=1748, loss=0.04781525209546089\n",
      "Surface training t=1749, loss=0.04817032814025879\n",
      "Surface training t=1750, loss=0.04835965484380722\n",
      "Surface training t=1751, loss=0.046204933896660805\n",
      "Surface training t=1752, loss=0.047643691301345825\n",
      "Surface training t=1753, loss=0.04843364097177982\n",
      "Surface training t=1754, loss=0.04564767889678478\n",
      "Surface training t=1755, loss=0.048115091398358345\n",
      "Surface training t=1756, loss=0.04718935117125511\n",
      "Surface training t=1757, loss=0.047611866146326065\n",
      "Surface training t=1758, loss=0.04843280836939812\n",
      "Surface training t=1759, loss=0.0486264955252409\n",
      "Surface training t=1760, loss=0.049103349447250366\n",
      "Surface training t=1761, loss=0.050477106124162674\n",
      "Surface training t=1762, loss=0.04965540021657944\n",
      "Surface training t=1763, loss=0.047999948263168335\n",
      "Surface training t=1764, loss=0.05005276016891003\n",
      "Surface training t=1765, loss=0.047651272267103195\n",
      "Surface training t=1766, loss=0.04742974415421486\n",
      "Surface training t=1767, loss=0.04719609580934048\n",
      "Surface training t=1768, loss=0.04799786955118179\n",
      "Surface training t=1769, loss=0.04916982352733612\n",
      "Surface training t=1770, loss=0.04735859856009483\n",
      "Surface training t=1771, loss=0.047526318579912186\n",
      "Surface training t=1772, loss=0.049995627254247665\n",
      "Surface training t=1773, loss=0.0481273178011179\n",
      "Surface training t=1774, loss=0.0472981333732605\n",
      "Surface training t=1775, loss=0.04709293507039547\n",
      "Surface training t=1776, loss=0.046190040186047554\n",
      "Surface training t=1777, loss=0.04762943834066391\n",
      "Surface training t=1778, loss=0.0484861359000206\n",
      "Surface training t=1779, loss=0.04598892666399479\n",
      "Surface training t=1780, loss=0.04752136953175068\n",
      "Surface training t=1781, loss=0.04863537475466728\n",
      "Surface training t=1782, loss=0.04830949939787388\n",
      "Surface training t=1783, loss=0.04700046591460705\n",
      "Surface training t=1784, loss=0.04709573648869991\n",
      "Surface training t=1785, loss=0.04557175561785698\n",
      "Surface training t=1786, loss=0.04815983586013317\n",
      "Surface training t=1787, loss=0.04692361131310463\n",
      "Surface training t=1788, loss=0.04653885401785374\n",
      "Surface training t=1789, loss=0.04903894662857056\n",
      "Surface training t=1790, loss=0.04901212081313133\n",
      "Surface training t=1791, loss=0.046947503462433815\n",
      "Surface training t=1792, loss=0.047796303406357765\n",
      "Surface training t=1793, loss=0.04942935332655907\n",
      "Surface training t=1794, loss=0.04749591462314129\n",
      "Surface training t=1795, loss=0.047887712717056274\n",
      "Surface training t=1796, loss=0.047819068655371666\n",
      "Surface training t=1797, loss=0.04638400673866272\n",
      "Surface training t=1798, loss=0.048041919246315956\n",
      "Surface training t=1799, loss=0.04864195175468922\n",
      "Surface training t=1800, loss=0.04739195480942726\n",
      "Surface training t=1801, loss=0.04928421601653099\n",
      "Surface training t=1802, loss=0.04741011746227741\n",
      "Surface training t=1803, loss=0.05012372322380543\n",
      "Surface training t=1804, loss=0.048271458595991135\n",
      "Surface training t=1805, loss=0.04616898111999035\n",
      "Surface training t=1806, loss=0.04705977439880371\n",
      "Surface training t=1807, loss=0.04717732407152653\n",
      "Surface training t=1808, loss=0.04876271262764931\n",
      "Surface training t=1809, loss=0.047628652304410934\n",
      "Surface training t=1810, loss=0.04692745581269264\n",
      "Surface training t=1811, loss=0.04747939296066761\n",
      "Surface training t=1812, loss=0.04838130623102188\n",
      "Surface training t=1813, loss=0.04720064625144005\n",
      "Surface training t=1814, loss=0.04708131216466427\n",
      "Surface training t=1815, loss=0.04918919317424297\n",
      "Surface training t=1816, loss=0.04962068609893322\n",
      "Surface training t=1817, loss=0.048716314136981964\n",
      "Surface training t=1818, loss=0.04805212281644344\n",
      "Surface training t=1819, loss=0.0494075994938612\n",
      "Surface training t=1820, loss=0.04911588691174984\n",
      "Surface training t=1821, loss=0.0482938177883625\n",
      "Surface training t=1822, loss=0.04847673326730728\n",
      "Surface training t=1823, loss=0.04831131547689438\n",
      "Surface training t=1824, loss=0.0495577622205019\n",
      "Surface training t=1825, loss=0.047199368476867676\n",
      "Surface training t=1826, loss=0.047010263428092\n",
      "Surface training t=1827, loss=0.047286057844758034\n",
      "Surface training t=1828, loss=0.04815410636365414\n",
      "Surface training t=1829, loss=0.048681074753403664\n",
      "Surface training t=1830, loss=0.049907345324754715\n",
      "Surface training t=1831, loss=0.04790208861231804\n",
      "Surface training t=1832, loss=0.04642060771584511\n",
      "Surface training t=1833, loss=0.04937439225614071\n",
      "Surface training t=1834, loss=0.0493626669049263\n",
      "Surface training t=1835, loss=0.04819732904434204\n",
      "Surface training t=1836, loss=0.047171229496598244\n",
      "Surface training t=1837, loss=0.047663506120443344\n",
      "Surface training t=1838, loss=0.04683327488601208\n",
      "Surface training t=1839, loss=0.04803845472633839\n",
      "Surface training t=1840, loss=0.04811604507267475\n",
      "Surface training t=1841, loss=0.04706785827875137\n",
      "Surface training t=1842, loss=0.04698227718472481\n",
      "Surface training t=1843, loss=0.04987582191824913\n",
      "Surface training t=1844, loss=0.04714486002922058\n",
      "Surface training t=1845, loss=0.048999229446053505\n",
      "Surface training t=1846, loss=0.0495479553937912\n",
      "Surface training t=1847, loss=0.0494849719107151\n",
      "Surface training t=1848, loss=0.04652835428714752\n",
      "Surface training t=1849, loss=0.04750451445579529\n",
      "Surface training t=1850, loss=0.05027635022997856\n",
      "Surface training t=1851, loss=0.04787803255021572\n",
      "Surface training t=1852, loss=0.048416562378406525\n",
      "Surface training t=1853, loss=0.04849543794989586\n",
      "Surface training t=1854, loss=0.04974892921745777\n",
      "Surface training t=1855, loss=0.04783329367637634\n",
      "Surface training t=1856, loss=0.0467982217669487\n",
      "Surface training t=1857, loss=0.04517063684761524\n",
      "Surface training t=1858, loss=0.04721461981534958\n",
      "Surface training t=1859, loss=0.04696634039282799\n",
      "Surface training t=1860, loss=0.04736303351819515\n",
      "Surface training t=1861, loss=0.04825039766728878\n",
      "Surface training t=1862, loss=0.04677475988864899\n",
      "Surface training t=1863, loss=0.04548870027065277\n",
      "Surface training t=1864, loss=0.047814616933465004\n",
      "Surface training t=1865, loss=0.04780210740864277\n",
      "Surface training t=1866, loss=0.04772603139281273\n",
      "Surface training t=1867, loss=0.04775756038725376\n",
      "Surface training t=1868, loss=0.04866956174373627\n",
      "Surface training t=1869, loss=0.04742613062262535\n",
      "Surface training t=1870, loss=0.0476224422454834\n",
      "Surface training t=1871, loss=0.04638230614364147\n",
      "Surface training t=1872, loss=0.04732735455036163\n",
      "Surface training t=1873, loss=0.047108424827456474\n",
      "Surface training t=1874, loss=0.04705412499606609\n",
      "Surface training t=1875, loss=0.046025561168789864\n",
      "Surface training t=1876, loss=0.04764234833419323\n",
      "Surface training t=1877, loss=0.046238260343670845\n",
      "Surface training t=1878, loss=0.047471484169363976\n",
      "Surface training t=1879, loss=0.045763805508613586\n",
      "Surface training t=1880, loss=0.04617335647344589\n",
      "Surface training t=1881, loss=0.046093180775642395\n",
      "Surface training t=1882, loss=0.046225303784012794\n",
      "Surface training t=1883, loss=0.04861505143344402\n",
      "Surface training t=1884, loss=0.04749312624335289\n",
      "Surface training t=1885, loss=0.046446848660707474\n",
      "Surface training t=1886, loss=0.04813742637634277\n",
      "Surface training t=1887, loss=0.04663445986807346\n",
      "Surface training t=1888, loss=0.04688750579953194\n",
      "Surface training t=1889, loss=0.04692515358328819\n",
      "Surface training t=1890, loss=0.048368800431489944\n",
      "Surface training t=1891, loss=0.047314371913671494\n",
      "Surface training t=1892, loss=0.04754313640296459\n",
      "Surface training t=1893, loss=0.04756523109972477\n",
      "Surface training t=1894, loss=0.04847089946269989\n",
      "Surface training t=1895, loss=0.046405188739299774\n",
      "Surface training t=1896, loss=0.04773331433534622\n",
      "Surface training t=1897, loss=0.04852447099983692\n",
      "Surface training t=1898, loss=0.04747052304446697\n",
      "Surface training t=1899, loss=0.04737660847604275\n",
      "Surface training t=1900, loss=0.04879794642329216\n",
      "Surface training t=1901, loss=0.048339225351810455\n",
      "Surface training t=1902, loss=0.04871486313641071\n",
      "Surface training t=1903, loss=0.04832425154745579\n",
      "Surface training t=1904, loss=0.04636901430785656\n",
      "Surface training t=1905, loss=0.04556867852807045\n",
      "Surface training t=1906, loss=0.04692370072007179\n",
      "Surface training t=1907, loss=0.04586048610508442\n",
      "Surface training t=1908, loss=0.047404468059539795\n",
      "Surface training t=1909, loss=0.047573016956448555\n",
      "Surface training t=1910, loss=0.04578540846705437\n",
      "Surface training t=1911, loss=0.04649330675601959\n",
      "Surface training t=1912, loss=0.048829086124897\n",
      "Surface training t=1913, loss=0.04695262014865875\n",
      "Surface training t=1914, loss=0.04782840795814991\n",
      "Surface training t=1915, loss=0.048477428033947945\n",
      "Surface training t=1916, loss=0.048250943422317505\n",
      "Surface training t=1917, loss=0.04682376608252525\n",
      "Surface training t=1918, loss=0.04746856912970543\n",
      "Surface training t=1919, loss=0.04656200483441353\n",
      "Surface training t=1920, loss=0.0475624930113554\n",
      "Surface training t=1921, loss=0.048250170424580574\n",
      "Surface training t=1922, loss=0.047073207795619965\n",
      "Surface training t=1923, loss=0.0481533519923687\n",
      "Surface training t=1924, loss=0.04757930897176266\n",
      "Surface training t=1925, loss=0.04606376215815544\n",
      "Surface training t=1926, loss=0.048493798822164536\n",
      "Surface training t=1927, loss=0.04707631096243858\n",
      "Surface training t=1928, loss=0.04654991440474987\n",
      "Surface training t=1929, loss=0.04730934277176857\n",
      "Surface training t=1930, loss=0.04707997851073742\n",
      "Surface training t=1931, loss=0.046799756586551666\n",
      "Surface training t=1932, loss=0.047458501532673836\n",
      "Surface training t=1933, loss=0.047808581963181496\n",
      "Surface training t=1934, loss=0.04647875763475895\n",
      "Surface training t=1935, loss=0.04757286421954632\n",
      "Surface training t=1936, loss=0.04874393343925476\n",
      "Surface training t=1937, loss=0.04563378728926182\n",
      "Surface training t=1938, loss=0.04651588946580887\n",
      "Surface training t=1939, loss=0.045068178325891495\n",
      "Surface training t=1940, loss=0.04888756945729256\n",
      "Surface training t=1941, loss=0.04775277525186539\n",
      "Surface training t=1942, loss=0.04689762368798256\n",
      "Surface training t=1943, loss=0.04970182105898857\n",
      "Surface training t=1944, loss=0.05018285661935806\n",
      "Surface training t=1945, loss=0.047050923109054565\n",
      "Surface training t=1946, loss=0.050621697679162025\n",
      "Surface training t=1947, loss=0.05037962831556797\n",
      "Surface training t=1948, loss=0.05089157819747925\n",
      "Surface training t=1949, loss=0.05156455375254154\n",
      "Surface training t=1950, loss=0.05728836543858051\n",
      "Surface training t=1951, loss=0.05598553642630577\n",
      "Surface training t=1952, loss=0.05066385865211487\n",
      "Surface training t=1953, loss=0.05326086841523647\n",
      "Surface training t=1954, loss=0.0488334521651268\n",
      "Surface training t=1955, loss=0.05152474530041218\n",
      "Surface training t=1956, loss=0.05052690766751766\n",
      "Surface training t=1957, loss=0.05555986426770687\n",
      "Surface training t=1958, loss=0.054316356778144836\n",
      "Surface training t=1959, loss=0.05232178419828415\n",
      "Surface training t=1960, loss=0.052070217207074165\n",
      "Surface training t=1961, loss=0.052870262414216995\n",
      "Surface training t=1962, loss=0.0512661375105381\n",
      "Surface training t=1963, loss=0.05609111860394478\n",
      "Surface training t=1964, loss=0.0496344119310379\n",
      "Surface training t=1965, loss=0.051197269931435585\n",
      "Surface training t=1966, loss=0.04881284572184086\n",
      "Surface training t=1967, loss=0.047360558062791824\n",
      "Surface training t=1968, loss=0.051999034360051155\n",
      "Surface training t=1969, loss=0.0523367915302515\n",
      "Surface training t=1970, loss=0.054940514266490936\n",
      "Surface training t=1971, loss=0.05328289791941643\n",
      "Surface training t=1972, loss=0.04887576401233673\n",
      "Surface training t=1973, loss=0.05045551620423794\n",
      "Surface training t=1974, loss=0.04870346747338772\n",
      "Surface training t=1975, loss=0.05245291814208031\n",
      "Surface training t=1976, loss=0.05057742819190025\n",
      "Surface training t=1977, loss=0.05440066196024418\n",
      "Surface training t=1978, loss=0.05450914800167084\n",
      "Surface training t=1979, loss=0.05160355940461159\n",
      "Surface training t=1980, loss=0.050993483513593674\n",
      "Surface training t=1981, loss=0.051941998302936554\n",
      "Surface training t=1982, loss=0.050682151690125465\n",
      "Surface training t=1983, loss=0.04881472326815128\n",
      "Surface training t=1984, loss=0.04964068531990051\n",
      "Surface training t=1985, loss=0.050436414778232574\n",
      "Surface training t=1986, loss=0.05369178019464016\n",
      "Surface training t=1987, loss=0.05105050466954708\n",
      "Surface training t=1988, loss=0.05083192326128483\n",
      "Surface training t=1989, loss=0.05405214615166187\n",
      "Surface training t=1990, loss=0.053274914622306824\n",
      "Surface training t=1991, loss=0.04979792237281799\n",
      "Surface training t=1992, loss=0.049292150884866714\n",
      "Surface training t=1993, loss=0.049974506720900536\n",
      "Surface training t=1994, loss=0.04905608110129833\n",
      "Surface training t=1995, loss=0.04752849042415619\n",
      "Surface training t=1996, loss=0.04710636846721172\n",
      "Surface training t=1997, loss=0.048389097675681114\n",
      "Surface training t=1998, loss=0.04615015909075737\n",
      "Surface training t=1999, loss=0.048383135348558426\n",
      "Surface training t=2000, loss=0.047717951238155365\n",
      "Surface training t=2001, loss=0.04806811548769474\n",
      "Surface training t=2002, loss=0.04493173211812973\n",
      "Surface training t=2003, loss=0.04583015665411949\n",
      "Surface training t=2004, loss=0.04710149206221104\n",
      "Surface training t=2005, loss=0.047875892370939255\n",
      "Surface training t=2006, loss=0.046777769923210144\n",
      "Surface training t=2007, loss=0.04521574079990387\n",
      "Surface training t=2008, loss=0.04631192982196808\n",
      "Surface training t=2009, loss=0.04792150855064392\n",
      "Surface training t=2010, loss=0.0462593138217926\n",
      "Surface training t=2011, loss=0.04630047455430031\n",
      "Surface training t=2012, loss=0.046830885112285614\n",
      "Surface training t=2013, loss=0.04586794599890709\n",
      "Surface training t=2014, loss=0.048160772770643234\n",
      "Surface training t=2015, loss=0.04668261669576168\n",
      "Surface training t=2016, loss=0.04652681574225426\n",
      "Surface training t=2017, loss=0.04591369815170765\n",
      "Surface training t=2018, loss=0.046690091490745544\n",
      "Surface training t=2019, loss=0.04660186916589737\n",
      "Surface training t=2020, loss=0.04798137955367565\n",
      "Surface training t=2021, loss=0.0455268919467926\n",
      "Surface training t=2022, loss=0.04564882814884186\n",
      "Surface training t=2023, loss=0.04757994785904884\n",
      "Surface training t=2024, loss=0.04696343094110489\n",
      "Surface training t=2025, loss=0.047098953276872635\n",
      "Surface training t=2026, loss=0.04721647873520851\n",
      "Surface training t=2027, loss=0.04753979481756687\n",
      "Surface training t=2028, loss=0.04698185250163078\n",
      "Surface training t=2029, loss=0.04799911566078663\n",
      "Surface training t=2030, loss=0.04682114161550999\n",
      "Surface training t=2031, loss=0.046791039407253265\n",
      "Surface training t=2032, loss=0.047899581491947174\n",
      "Surface training t=2033, loss=0.04704059287905693\n",
      "Surface training t=2034, loss=0.04743881709873676\n",
      "Surface training t=2035, loss=0.04522152058780193\n",
      "Surface training t=2036, loss=0.04765523225069046\n",
      "Surface training t=2037, loss=0.04746326617896557\n",
      "Surface training t=2038, loss=0.04639158956706524\n",
      "Surface training t=2039, loss=0.04612702131271362\n",
      "Surface training t=2040, loss=0.04573962092399597\n",
      "Surface training t=2041, loss=0.04540775716304779\n",
      "Surface training t=2042, loss=0.04760455712676048\n",
      "Surface training t=2043, loss=0.04709958843886852\n",
      "Surface training t=2044, loss=0.04570625349879265\n",
      "Surface training t=2045, loss=0.04698824882507324\n",
      "Surface training t=2046, loss=0.04829144850373268\n",
      "Surface training t=2047, loss=0.04790027439594269\n",
      "Surface training t=2048, loss=0.04684087261557579\n",
      "Surface training t=2049, loss=0.046882012858986855\n",
      "Surface training t=2050, loss=0.047899311408400536\n",
      "Surface training t=2051, loss=0.04646117240190506\n",
      "Surface training t=2052, loss=0.04562785290181637\n",
      "Surface training t=2053, loss=0.04550306685268879\n",
      "Surface training t=2054, loss=0.04697314463555813\n",
      "Surface training t=2055, loss=0.04592476412653923\n",
      "Surface training t=2056, loss=0.04542575404047966\n",
      "Surface training t=2057, loss=0.04667680524289608\n",
      "Surface training t=2058, loss=0.04843035154044628\n",
      "Surface training t=2059, loss=0.04720872454345226\n",
      "Surface training t=2060, loss=0.04656355269253254\n",
      "Surface training t=2061, loss=0.04664194583892822\n",
      "Surface training t=2062, loss=0.04692770913243294\n",
      "Surface training t=2063, loss=0.046286528930068016\n",
      "Surface training t=2064, loss=0.047894835472106934\n",
      "Surface training t=2065, loss=0.04667939618229866\n",
      "Surface training t=2066, loss=0.04719114303588867\n",
      "Surface training t=2067, loss=0.04680259712040424\n",
      "Surface training t=2068, loss=0.04656706377863884\n",
      "Surface training t=2069, loss=0.04744300618767738\n",
      "Surface training t=2070, loss=0.04519019275903702\n",
      "Surface training t=2071, loss=0.04544146545231342\n",
      "Surface training t=2072, loss=0.04656199924647808\n",
      "Surface training t=2073, loss=0.04582888260483742\n",
      "Surface training t=2074, loss=0.047762203961610794\n",
      "Surface training t=2075, loss=0.04724155180156231\n",
      "Surface training t=2076, loss=0.04811704531311989\n",
      "Surface training t=2077, loss=0.0460701659321785\n",
      "Surface training t=2078, loss=0.047081610187888145\n",
      "Surface training t=2079, loss=0.046192917972803116\n",
      "Surface training t=2080, loss=0.046577733010053635\n",
      "Surface training t=2081, loss=0.04784535616636276\n",
      "Surface training t=2082, loss=0.0460414607077837\n",
      "Surface training t=2083, loss=0.046872956678271294\n",
      "Surface training t=2084, loss=0.045916713774204254\n",
      "Surface training t=2085, loss=0.047804178670048714\n",
      "Surface training t=2086, loss=0.04776124469935894\n",
      "Surface training t=2087, loss=0.046342816203832626\n",
      "Surface training t=2088, loss=0.04689321294426918\n",
      "Surface training t=2089, loss=0.045832086354494095\n",
      "Surface training t=2090, loss=0.04717166721820831\n",
      "Surface training t=2091, loss=0.048727622255682945\n",
      "Surface training t=2092, loss=0.04732477851212025\n",
      "Surface training t=2093, loss=0.04670597054064274\n",
      "Surface training t=2094, loss=0.048727238550782204\n",
      "Surface training t=2095, loss=0.0479664858430624\n",
      "Surface training t=2096, loss=0.04781997576355934\n",
      "Surface training t=2097, loss=0.04703252948820591\n",
      "Surface training t=2098, loss=0.047812992706894875\n",
      "Surface training t=2099, loss=0.046530911698937416\n",
      "Surface training t=2100, loss=0.047855764627456665\n",
      "Surface training t=2101, loss=0.048207029700279236\n",
      "Surface training t=2102, loss=0.04591540992259979\n",
      "Surface training t=2103, loss=0.047162676230072975\n",
      "Surface training t=2104, loss=0.0460352823138237\n",
      "Surface training t=2105, loss=0.04671327583491802\n",
      "Surface training t=2106, loss=0.04591706581413746\n",
      "Surface training t=2107, loss=0.04584085941314697\n",
      "Surface training t=2108, loss=0.04588953219354153\n",
      "Surface training t=2109, loss=0.04559777118265629\n",
      "Surface training t=2110, loss=0.04715614393353462\n",
      "Surface training t=2111, loss=0.04709893651306629\n",
      "Surface training t=2112, loss=0.04597357474267483\n",
      "Surface training t=2113, loss=0.0472415816038847\n",
      "Surface training t=2114, loss=0.046527011319994926\n",
      "Surface training t=2115, loss=0.04666305147111416\n",
      "Surface training t=2116, loss=0.0460804495960474\n",
      "Surface training t=2117, loss=0.047166211530566216\n",
      "Surface training t=2118, loss=0.04802419804036617\n",
      "Surface training t=2119, loss=0.04605386406183243\n",
      "Surface training t=2120, loss=0.04602274298667908\n",
      "Surface training t=2121, loss=0.04803651012480259\n",
      "Surface training t=2122, loss=0.04680151119828224\n",
      "Surface training t=2123, loss=0.04791167564690113\n",
      "Surface training t=2124, loss=0.047268571332097054\n",
      "Surface training t=2125, loss=0.04663012735545635\n",
      "Surface training t=2126, loss=0.046741167083382607\n",
      "Surface training t=2127, loss=0.046012116596102715\n",
      "Surface training t=2128, loss=0.04819893464446068\n",
      "Surface training t=2129, loss=0.04657819680869579\n",
      "Surface training t=2130, loss=0.047084854915738106\n",
      "Surface training t=2131, loss=0.04768695868551731\n",
      "Surface training t=2132, loss=0.04728987254202366\n",
      "Surface training t=2133, loss=0.0472487173974514\n",
      "Surface training t=2134, loss=0.0452777873724699\n",
      "Surface training t=2135, loss=0.04665263183414936\n",
      "Surface training t=2136, loss=0.047530243173241615\n",
      "Surface training t=2137, loss=0.04792086221277714\n",
      "Surface training t=2138, loss=0.04726313799619675\n",
      "Surface training t=2139, loss=0.04714515991508961\n",
      "Surface training t=2140, loss=0.046562474220991135\n",
      "Surface training t=2141, loss=0.047271812334656715\n",
      "Surface training t=2142, loss=0.04653013497591019\n",
      "Surface training t=2143, loss=0.04705801606178284\n",
      "Surface training t=2144, loss=0.04880047030746937\n",
      "Surface training t=2145, loss=0.04769998416304588\n",
      "Surface training t=2146, loss=0.046632710844278336\n",
      "Surface training t=2147, loss=0.04698462411761284\n",
      "Surface training t=2148, loss=0.048116423189640045\n",
      "Surface training t=2149, loss=0.04689070023596287\n",
      "Surface training t=2150, loss=0.047190314158797264\n",
      "Surface training t=2151, loss=0.04749994911253452\n",
      "Surface training t=2152, loss=0.04635389521718025\n",
      "Surface training t=2153, loss=0.04599165357649326\n",
      "Surface training t=2154, loss=0.04694342240691185\n",
      "Surface training t=2155, loss=0.0462060272693634\n",
      "Surface training t=2156, loss=0.04633784107863903\n",
      "Surface training t=2157, loss=0.046228423714637756\n",
      "Surface training t=2158, loss=0.0474281907081604\n",
      "Surface training t=2159, loss=0.045736681669950485\n",
      "Surface training t=2160, loss=0.0463991854339838\n",
      "Surface training t=2161, loss=0.04665020853281021\n",
      "Surface training t=2162, loss=0.047216735780239105\n",
      "Surface training t=2163, loss=0.047626977786421776\n",
      "Surface training t=2164, loss=0.04515348933637142\n",
      "Surface training t=2165, loss=0.04700955003499985\n",
      "Surface training t=2166, loss=0.04582721367478371\n",
      "Surface training t=2167, loss=0.04512708634138107\n",
      "Surface training t=2168, loss=0.04570786841213703\n",
      "Surface training t=2169, loss=0.04620933346450329\n",
      "Surface training t=2170, loss=0.0457141250371933\n",
      "Surface training t=2171, loss=0.046367621049284935\n",
      "Surface training t=2172, loss=0.04517637379467487\n",
      "Surface training t=2173, loss=0.04637465067207813\n",
      "Surface training t=2174, loss=0.04614841006696224\n",
      "Surface training t=2175, loss=0.04768276400864124\n",
      "Surface training t=2176, loss=0.046397045254707336\n",
      "Surface training t=2177, loss=0.04847329296171665\n",
      "Surface training t=2178, loss=0.04512215778231621\n",
      "Surface training t=2179, loss=0.04688018187880516\n",
      "Surface training t=2180, loss=0.047994913533329964\n",
      "Surface training t=2181, loss=0.04643412493169308\n",
      "Surface training t=2182, loss=0.04609466344118118\n",
      "Surface training t=2183, loss=0.0450991727411747\n",
      "Surface training t=2184, loss=0.04723013564944267\n",
      "Surface training t=2185, loss=0.04741898365318775\n",
      "Surface training t=2186, loss=0.045463643968105316\n",
      "Surface training t=2187, loss=0.048006415367126465\n",
      "Surface training t=2188, loss=0.047507187351584435\n",
      "Surface training t=2189, loss=0.046734897419810295\n",
      "Surface training t=2190, loss=0.047282855957746506\n",
      "Surface training t=2191, loss=0.045690976083278656\n",
      "Surface training t=2192, loss=0.046547114849090576\n",
      "Surface training t=2193, loss=0.04530825838446617\n",
      "Surface training t=2194, loss=0.04727560468018055\n",
      "Surface training t=2195, loss=0.04713679477572441\n",
      "Surface training t=2196, loss=0.04771677777171135\n",
      "Surface training t=2197, loss=0.048359861597418785\n",
      "Surface training t=2198, loss=0.04761164449155331\n",
      "Surface training t=2199, loss=0.04722953401505947\n",
      "Surface training t=2200, loss=0.047262636944651604\n",
      "Surface training t=2201, loss=0.04664863459765911\n",
      "Surface training t=2202, loss=0.04542555473744869\n",
      "Surface training t=2203, loss=0.04738203436136246\n",
      "Surface training t=2204, loss=0.04567906819283962\n",
      "Surface training t=2205, loss=0.04908258654177189\n",
      "Surface training t=2206, loss=0.047799235209822655\n",
      "Surface training t=2207, loss=0.047661347314715385\n",
      "Surface training t=2208, loss=0.046973349526524544\n",
      "Surface training t=2209, loss=0.04986202344298363\n",
      "Surface training t=2210, loss=0.04742397367954254\n",
      "Surface training t=2211, loss=0.045647453516721725\n",
      "Surface training t=2212, loss=0.047692349180579185\n",
      "Surface training t=2213, loss=0.04562780447304249\n",
      "Surface training t=2214, loss=0.04692296124994755\n",
      "Surface training t=2215, loss=0.04715745151042938\n",
      "Surface training t=2216, loss=0.0462791845202446\n",
      "Surface training t=2217, loss=0.044940099120140076\n",
      "Surface training t=2218, loss=0.04657268524169922\n",
      "Surface training t=2219, loss=0.04571973532438278\n",
      "Surface training t=2220, loss=0.04669014923274517\n",
      "Surface training t=2221, loss=0.049211399629712105\n",
      "Surface training t=2222, loss=0.04863985814154148\n",
      "Surface training t=2223, loss=0.04550967551767826\n",
      "Surface training t=2224, loss=0.048734668642282486\n",
      "Surface training t=2225, loss=0.04897122271358967\n",
      "Surface training t=2226, loss=0.04835193604230881\n",
      "Surface training t=2227, loss=0.046206289902329445\n",
      "Surface training t=2228, loss=0.045099860057234764\n",
      "Surface training t=2229, loss=0.04592260904610157\n",
      "Surface training t=2230, loss=0.04561142064630985\n",
      "Surface training t=2231, loss=0.045693445950746536\n",
      "Surface training t=2232, loss=0.04891861975193024\n",
      "Surface training t=2233, loss=0.04659046791493893\n",
      "Surface training t=2234, loss=0.04590905085206032\n",
      "Surface training t=2235, loss=0.048232609406113625\n",
      "Surface training t=2236, loss=0.04732850007712841\n",
      "Surface training t=2237, loss=0.04601740092039108\n",
      "Surface training t=2238, loss=0.045787299051880836\n",
      "Surface training t=2239, loss=0.04789544269442558\n",
      "Surface training t=2240, loss=0.04641663283109665\n",
      "Surface training t=2241, loss=0.04571092501282692\n",
      "Surface training t=2242, loss=0.046593671664595604\n",
      "Surface training t=2243, loss=0.04467981308698654\n",
      "Surface training t=2244, loss=0.04626414552330971\n",
      "Surface training t=2245, loss=0.046264564618468285\n",
      "Surface training t=2246, loss=0.045527927577495575\n",
      "Surface training t=2247, loss=0.04619110748171806\n",
      "Surface training t=2248, loss=0.04479098692536354\n",
      "Surface training t=2249, loss=0.04730245843529701\n",
      "Surface training t=2250, loss=0.04456016793847084\n",
      "Surface training t=2251, loss=0.04630531556904316\n",
      "Surface training t=2252, loss=0.04515477828681469\n",
      "Surface training t=2253, loss=0.04743996076285839\n",
      "Surface training t=2254, loss=0.04594544693827629\n",
      "Surface training t=2255, loss=0.04576130025088787\n",
      "Surface training t=2256, loss=0.04642103053629398\n",
      "Surface training t=2257, loss=0.04626505263149738\n",
      "Surface training t=2258, loss=0.04586782306432724\n",
      "Surface training t=2259, loss=0.04478307254612446\n",
      "Surface training t=2260, loss=0.04666728898882866\n",
      "Surface training t=2261, loss=0.04708450101315975\n",
      "Surface training t=2262, loss=0.047681620344519615\n",
      "Surface training t=2263, loss=0.045758217573165894\n",
      "Surface training t=2264, loss=0.048120515421032906\n",
      "Surface training t=2265, loss=0.046966882422566414\n",
      "Surface training t=2266, loss=0.047199672088027\n",
      "Surface training t=2267, loss=0.04633947275578976\n",
      "Surface training t=2268, loss=0.04591906815767288\n",
      "Surface training t=2269, loss=0.04646339640021324\n",
      "Surface training t=2270, loss=0.04537408612668514\n",
      "Surface training t=2271, loss=0.04856594651937485\n",
      "Surface training t=2272, loss=0.047059565782547\n",
      "Surface training t=2273, loss=0.04715448617935181\n",
      "Surface training t=2274, loss=0.0467060636729002\n",
      "Surface training t=2275, loss=0.04629833810031414\n",
      "Surface training t=2276, loss=0.04696720838546753\n",
      "Surface training t=2277, loss=0.04850514233112335\n",
      "Surface training t=2278, loss=0.04622556082904339\n",
      "Surface training t=2279, loss=0.047240953892469406\n",
      "Surface training t=2280, loss=0.04789390042424202\n",
      "Surface training t=2281, loss=0.04517382010817528\n",
      "Surface training t=2282, loss=0.04842611029744148\n",
      "Surface training t=2283, loss=0.04588637873530388\n",
      "Surface training t=2284, loss=0.04604766517877579\n",
      "Surface training t=2285, loss=0.048617441207170486\n",
      "Surface training t=2286, loss=0.04738844558596611\n",
      "Surface training t=2287, loss=0.046226395294070244\n",
      "Surface training t=2288, loss=0.04531995579600334\n",
      "Surface training t=2289, loss=0.045338449999690056\n",
      "Surface training t=2290, loss=0.04552542231976986\n",
      "Surface training t=2291, loss=0.045525144785642624\n",
      "Surface training t=2292, loss=0.046734899282455444\n",
      "Surface training t=2293, loss=0.045028507709503174\n",
      "Surface training t=2294, loss=0.04503711871802807\n",
      "Surface training t=2295, loss=0.04585961624979973\n",
      "Surface training t=2296, loss=0.044453805312514305\n",
      "Surface training t=2297, loss=0.046341342851519585\n",
      "Surface training t=2298, loss=0.045465027913451195\n",
      "Surface training t=2299, loss=0.04549863189458847\n",
      "Surface training t=2300, loss=0.04619663581252098\n",
      "Surface training t=2301, loss=0.046462247148156166\n",
      "Surface training t=2302, loss=0.04570411704480648\n",
      "Surface training t=2303, loss=0.047360215336084366\n",
      "Surface training t=2304, loss=0.044670386239886284\n",
      "Surface training t=2305, loss=0.045817138627171516\n",
      "Surface training t=2306, loss=0.046098923310637474\n",
      "Surface training t=2307, loss=0.04661654308438301\n",
      "Surface training t=2308, loss=0.046497294679284096\n",
      "Surface training t=2309, loss=0.04717387445271015\n",
      "Surface training t=2310, loss=0.045961299911141396\n",
      "Surface training t=2311, loss=0.046230072155594826\n",
      "Surface training t=2312, loss=0.045200590044260025\n",
      "Surface training t=2313, loss=0.04512329772114754\n",
      "Surface training t=2314, loss=0.047013645991683006\n",
      "Surface training t=2315, loss=0.044923556968569756\n",
      "Surface training t=2316, loss=0.045009786263108253\n",
      "Surface training t=2317, loss=0.04680955223739147\n",
      "Surface training t=2318, loss=0.04651270993053913\n",
      "Surface training t=2319, loss=0.04752925597131252\n",
      "Surface training t=2320, loss=0.046722766011953354\n",
      "Surface training t=2321, loss=0.048145269975066185\n",
      "Surface training t=2322, loss=0.04686823673546314\n",
      "Surface training t=2323, loss=0.046791305765509605\n",
      "Surface training t=2324, loss=0.04884129390120506\n",
      "Surface training t=2325, loss=0.04771728254854679\n",
      "Surface training t=2326, loss=0.047024939209222794\n",
      "Surface training t=2327, loss=0.04733465798199177\n",
      "Surface training t=2328, loss=0.04662421531975269\n",
      "Surface training t=2329, loss=0.04559841565787792\n",
      "Surface training t=2330, loss=0.04520324803888798\n",
      "Surface training t=2331, loss=0.04412228800356388\n",
      "Surface training t=2332, loss=0.045505670830607414\n",
      "Surface training t=2333, loss=0.04496615566313267\n",
      "Surface training t=2334, loss=0.045878760516643524\n",
      "Surface training t=2335, loss=0.04523552395403385\n",
      "Surface training t=2336, loss=0.04476908594369888\n",
      "Surface training t=2337, loss=0.046565452590584755\n",
      "Surface training t=2338, loss=0.04598020017147064\n",
      "Surface training t=2339, loss=0.04624038375914097\n",
      "Surface training t=2340, loss=0.04550027847290039\n",
      "Surface training t=2341, loss=0.046676717698574066\n",
      "Surface training t=2342, loss=0.04750700294971466\n",
      "Surface training t=2343, loss=0.0470283180475235\n",
      "Surface training t=2344, loss=0.04563564620912075\n",
      "Surface training t=2345, loss=0.044666850939393044\n",
      "Surface training t=2346, loss=0.046204375103116035\n",
      "Surface training t=2347, loss=0.04578229412436485\n",
      "Surface training t=2348, loss=0.04462914168834686\n",
      "Surface training t=2349, loss=0.04661959409713745\n",
      "Surface training t=2350, loss=0.046568071469664574\n",
      "Surface training t=2351, loss=0.04521957412362099\n",
      "Surface training t=2352, loss=0.044561758637428284\n",
      "Surface training t=2353, loss=0.04599888436496258\n",
      "Surface training t=2354, loss=0.0472355242818594\n",
      "Surface training t=2355, loss=0.046783456578850746\n",
      "Surface training t=2356, loss=0.04869791306555271\n",
      "Surface training t=2357, loss=0.04729040712118149\n",
      "Surface training t=2358, loss=0.045741330832242966\n",
      "Surface training t=2359, loss=0.04686709865927696\n",
      "Surface training t=2360, loss=0.04938607104122639\n",
      "Surface training t=2361, loss=0.04509871080517769\n",
      "Surface training t=2362, loss=0.046755893155932426\n",
      "Surface training t=2363, loss=0.04803056828677654\n",
      "Surface training t=2364, loss=0.04670224338769913\n",
      "Surface training t=2365, loss=0.04824606887996197\n",
      "Surface training t=2366, loss=0.04713071323931217\n",
      "Surface training t=2367, loss=0.04738808237016201\n",
      "Surface training t=2368, loss=0.04655120149254799\n",
      "Surface training t=2369, loss=0.04763099551200867\n",
      "Surface training t=2370, loss=0.04446641355752945\n",
      "Surface training t=2371, loss=0.046051280573010445\n",
      "Surface training t=2372, loss=0.04587395861744881\n",
      "Surface training t=2373, loss=0.04609518498182297\n",
      "Surface training t=2374, loss=0.046353477984666824\n",
      "Surface training t=2375, loss=0.047454969957470894\n",
      "Surface training t=2376, loss=0.04707246460020542\n",
      "Surface training t=2377, loss=0.046693235635757446\n",
      "Surface training t=2378, loss=0.0460921972990036\n",
      "Surface training t=2379, loss=0.04452217556536198\n",
      "Surface training t=2380, loss=0.045422833412885666\n",
      "Surface training t=2381, loss=0.04467507265508175\n",
      "Surface training t=2382, loss=0.04515902325510979\n",
      "Surface training t=2383, loss=0.0444004163146019\n",
      "Surface training t=2384, loss=0.04573017917573452\n",
      "Surface training t=2385, loss=0.04623779095709324\n",
      "Surface training t=2386, loss=0.046848494559526443\n",
      "Surface training t=2387, loss=0.04675561934709549\n",
      "Surface training t=2388, loss=0.046572424471378326\n",
      "Surface training t=2389, loss=0.0464528352022171\n",
      "Surface training t=2390, loss=0.04568858444690704\n",
      "Surface training t=2391, loss=0.04788666591048241\n",
      "Surface training t=2392, loss=0.04470498114824295\n",
      "Surface training t=2393, loss=0.046715935692191124\n",
      "Surface training t=2394, loss=0.04582195356488228\n",
      "Surface training t=2395, loss=0.04698794148862362\n",
      "Surface training t=2396, loss=0.04561593569815159\n",
      "Surface training t=2397, loss=0.04764232784509659\n",
      "Surface training t=2398, loss=0.04705863818526268\n",
      "Surface training t=2399, loss=0.045627277344465256\n",
      "Surface training t=2400, loss=0.04456540755927563\n",
      "Surface training t=2401, loss=0.04620984196662903\n",
      "Surface training t=2402, loss=0.04452654719352722\n",
      "Surface training t=2403, loss=0.046193402260541916\n",
      "Surface training t=2404, loss=0.04755159839987755\n",
      "Surface training t=2405, loss=0.04517020098865032\n",
      "Surface training t=2406, loss=0.045977598056197166\n",
      "Surface training t=2407, loss=0.04604805074632168\n",
      "Surface training t=2408, loss=0.046326640993356705\n",
      "Surface training t=2409, loss=0.04646303504705429\n",
      "Surface training t=2410, loss=0.04544989578425884\n",
      "Surface training t=2411, loss=0.04735938645899296\n",
      "Surface training t=2412, loss=0.04587169177830219\n",
      "Surface training t=2413, loss=0.04633583500981331\n",
      "Surface training t=2414, loss=0.045210208743810654\n",
      "Surface training t=2415, loss=0.04719897545874119\n",
      "Surface training t=2416, loss=0.04433685168623924\n",
      "Surface training t=2417, loss=0.04894759692251682\n",
      "Surface training t=2418, loss=0.04780265875160694\n",
      "Surface training t=2419, loss=0.045905234292149544\n",
      "Surface training t=2420, loss=0.04749613627791405\n",
      "Surface training t=2421, loss=0.04929344542324543\n",
      "Surface training t=2422, loss=0.045739421620965004\n",
      "Surface training t=2423, loss=0.046195317059755325\n",
      "Surface training t=2424, loss=0.04836704954504967\n",
      "Surface training t=2425, loss=0.04961121641099453\n",
      "Surface training t=2426, loss=0.04641973040997982\n",
      "Surface training t=2427, loss=0.045913150534033775\n",
      "Surface training t=2428, loss=0.045194217935204506\n",
      "Surface training t=2429, loss=0.04620329476892948\n",
      "Surface training t=2430, loss=0.04552365094423294\n",
      "Surface training t=2431, loss=0.04556388407945633\n",
      "Surface training t=2432, loss=0.044705240055918694\n",
      "Surface training t=2433, loss=0.04396246373653412\n",
      "Surface training t=2434, loss=0.046078696846961975\n",
      "Surface training t=2435, loss=0.047182464972138405\n",
      "Surface training t=2436, loss=0.04595665819942951\n",
      "Surface training t=2437, loss=0.046014634892344475\n",
      "Surface training t=2438, loss=0.0449335090816021\n",
      "Surface training t=2439, loss=0.0466558076441288\n",
      "Surface training t=2440, loss=0.04438552260398865\n",
      "Surface training t=2441, loss=0.04674042761325836\n",
      "Surface training t=2442, loss=0.04568985663354397\n",
      "Surface training t=2443, loss=0.04521356523036957\n",
      "Surface training t=2444, loss=0.04463288001716137\n",
      "Surface training t=2445, loss=0.04708615317940712\n",
      "Surface training t=2446, loss=0.04566192999482155\n",
      "Surface training t=2447, loss=0.04505489394068718\n",
      "Surface training t=2448, loss=0.049351589754223824\n",
      "Surface training t=2449, loss=0.046190476045012474\n",
      "Surface training t=2450, loss=0.04674488492310047\n",
      "Surface training t=2451, loss=0.04962389916181564\n",
      "Surface training t=2452, loss=0.04778170399367809\n",
      "Surface training t=2453, loss=0.04723438248038292\n",
      "Surface training t=2454, loss=0.04733894020318985\n",
      "Surface training t=2455, loss=0.04665691778063774\n",
      "Surface training t=2456, loss=0.04648861661553383\n",
      "Surface training t=2457, loss=0.04418821260333061\n",
      "Surface training t=2458, loss=0.0464521124958992\n",
      "Surface training t=2459, loss=0.044410135596990585\n",
      "Surface training t=2460, loss=0.046535713598132133\n",
      "Surface training t=2461, loss=0.045135658234357834\n",
      "Surface training t=2462, loss=0.04578549973666668\n",
      "Surface training t=2463, loss=0.04594125784933567\n",
      "Surface training t=2464, loss=0.04551093839108944\n",
      "Surface training t=2465, loss=0.045170437544584274\n",
      "Surface training t=2466, loss=0.04614430107176304\n",
      "Surface training t=2467, loss=0.04723767377436161\n",
      "Surface training t=2468, loss=0.04523482732474804\n",
      "Surface training t=2469, loss=0.04619696922600269\n",
      "Surface training t=2470, loss=0.04648164287209511\n",
      "Surface training t=2471, loss=0.04525836557149887\n",
      "Surface training t=2472, loss=0.04474371671676636\n",
      "Surface training t=2473, loss=0.04542292095720768\n",
      "Surface training t=2474, loss=0.04536880925297737\n",
      "Surface training t=2475, loss=0.04616680555045605\n",
      "Surface training t=2476, loss=0.04594323970377445\n",
      "Surface training t=2477, loss=0.0464349240064621\n",
      "Surface training t=2478, loss=0.0458985660225153\n",
      "Surface training t=2479, loss=0.046830011531710625\n",
      "Surface training t=2480, loss=0.046291226521134377\n",
      "Surface training t=2481, loss=0.04565412923693657\n",
      "Surface training t=2482, loss=0.04678444378077984\n",
      "Surface training t=2483, loss=0.04845874570310116\n",
      "Surface training t=2484, loss=0.04559248685836792\n",
      "Surface training t=2485, loss=0.04503908194601536\n",
      "Surface training t=2486, loss=0.048090338706970215\n",
      "Surface training t=2487, loss=0.046996334567666054\n",
      "Surface training t=2488, loss=0.04718082956969738\n",
      "Surface training t=2489, loss=0.045732349157333374\n",
      "Surface training t=2490, loss=0.044861411675810814\n",
      "Surface training t=2491, loss=0.045424990355968475\n",
      "Surface training t=2492, loss=0.04674072004854679\n",
      "Surface training t=2493, loss=0.044821590185165405\n",
      "Surface training t=2494, loss=0.046743398532271385\n",
      "Surface training t=2495, loss=0.045521266758441925\n",
      "Surface training t=2496, loss=0.04466690868139267\n",
      "Surface training t=2497, loss=0.04653060808777809\n",
      "Surface training t=2498, loss=0.04552422650158405\n",
      "Surface training t=2499, loss=0.04601808451116085\n",
      "Surface training t=2500, loss=0.04574712738394737\n",
      "Surface training t=2501, loss=0.044534832239151\n",
      "Surface training t=2502, loss=0.045897457748651505\n",
      "Surface training t=2503, loss=0.04691189154982567\n",
      "Surface training t=2504, loss=0.045011840760707855\n",
      "Surface training t=2505, loss=0.0458336528390646\n",
      "Surface training t=2506, loss=0.04658554494380951\n",
      "Surface training t=2507, loss=0.046029724180698395\n",
      "Surface training t=2508, loss=0.045010726898908615\n",
      "Surface training t=2509, loss=0.04474633000791073\n",
      "Surface training t=2510, loss=0.04651257023215294\n",
      "Surface training t=2511, loss=0.04538533836603165\n",
      "Surface training t=2512, loss=0.045697981491684914\n",
      "Surface training t=2513, loss=0.04592502489686012\n",
      "Surface training t=2514, loss=0.04553287290036678\n",
      "Surface training t=2515, loss=0.04571516253054142\n",
      "Surface training t=2516, loss=0.045871835201978683\n",
      "Surface training t=2517, loss=0.045963918790221214\n",
      "Surface training t=2518, loss=0.04706797935068607\n",
      "Surface training t=2519, loss=0.04741962440311909\n",
      "Surface training t=2520, loss=0.045719217509031296\n",
      "Surface training t=2521, loss=0.04505827836692333\n",
      "Surface training t=2522, loss=0.04606718011200428\n",
      "Surface training t=2523, loss=0.04533722996711731\n",
      "Surface training t=2524, loss=0.046297430992126465\n",
      "Surface training t=2525, loss=0.04652629420161247\n",
      "Surface training t=2526, loss=0.045134397223591805\n",
      "Surface training t=2527, loss=0.046567849814891815\n",
      "Surface training t=2528, loss=0.04602108523249626\n",
      "Surface training t=2529, loss=0.04611058346927166\n",
      "Surface training t=2530, loss=0.04672420397400856\n",
      "Surface training t=2531, loss=0.04610370472073555\n",
      "Surface training t=2532, loss=0.045589033514261246\n",
      "Surface training t=2533, loss=0.04547907039523125\n",
      "Surface training t=2534, loss=0.04498149827122688\n",
      "Surface training t=2535, loss=0.04581191577017307\n",
      "Surface training t=2536, loss=0.04459446296095848\n",
      "Surface training t=2537, loss=0.046173229813575745\n",
      "Surface training t=2538, loss=0.044892266392707825\n",
      "Surface training t=2539, loss=0.04551549069583416\n",
      "Surface training t=2540, loss=0.045933982357382774\n",
      "Surface training t=2541, loss=0.04649166576564312\n",
      "Surface training t=2542, loss=0.04606073722243309\n",
      "Surface training t=2543, loss=0.04475752077996731\n",
      "Surface training t=2544, loss=0.046273283660411835\n",
      "Surface training t=2545, loss=0.046308595687150955\n",
      "Surface training t=2546, loss=0.046260276809334755\n",
      "Surface training t=2547, loss=0.04739777371287346\n",
      "Surface training t=2548, loss=0.04498112387955189\n",
      "Surface training t=2549, loss=0.04513453133404255\n",
      "Surface training t=2550, loss=0.044093791395425797\n",
      "Surface training t=2551, loss=0.046649759635329247\n",
      "Surface training t=2552, loss=0.04535248130559921\n",
      "Surface training t=2553, loss=0.04617917351424694\n",
      "Surface training t=2554, loss=0.045631686225533485\n",
      "Surface training t=2555, loss=0.0458014328032732\n",
      "Surface training t=2556, loss=0.04575381055474281\n",
      "Surface training t=2557, loss=0.04822361283004284\n",
      "Surface training t=2558, loss=0.04492800496518612\n",
      "Surface training t=2559, loss=0.0459175705909729\n",
      "Surface training t=2560, loss=0.04881291650235653\n",
      "Surface training t=2561, loss=0.04622666351497173\n",
      "Surface training t=2562, loss=0.04646953567862511\n",
      "Surface training t=2563, loss=0.04537929967045784\n",
      "Surface training t=2564, loss=0.0509323850274086\n",
      "Surface training t=2565, loss=0.04641009867191315\n",
      "Surface training t=2566, loss=0.047722380608320236\n",
      "Surface training t=2567, loss=0.0480921845883131\n",
      "Surface training t=2568, loss=0.04545047879219055\n",
      "Surface training t=2569, loss=0.04467507638037205\n",
      "Surface training t=2570, loss=0.04646047577261925\n",
      "Surface training t=2571, loss=0.04490799643099308\n",
      "Surface training t=2572, loss=0.04741315543651581\n",
      "Surface training t=2573, loss=0.04577878676354885\n",
      "Surface training t=2574, loss=0.04712036810815334\n",
      "Surface training t=2575, loss=0.04719836078584194\n",
      "Surface training t=2576, loss=0.044982144609093666\n",
      "Surface training t=2577, loss=0.045742398127913475\n",
      "Surface training t=2578, loss=0.04768507182598114\n",
      "Surface training t=2579, loss=0.047118375077843666\n",
      "Surface training t=2580, loss=0.04537597857415676\n",
      "Surface training t=2581, loss=0.04735291190445423\n",
      "Surface training t=2582, loss=0.04740756191313267\n",
      "Surface training t=2583, loss=0.045556481927633286\n",
      "Surface training t=2584, loss=0.047637440264225006\n",
      "Surface training t=2585, loss=0.04581894911825657\n",
      "Surface training t=2586, loss=0.044681694358587265\n",
      "Surface training t=2587, loss=0.047084057703614235\n",
      "Surface training t=2588, loss=0.04655169136822224\n",
      "Surface training t=2589, loss=0.043822284787893295\n",
      "Surface training t=2590, loss=0.046491650864481926\n",
      "Surface training t=2591, loss=0.04631019942462444\n",
      "Surface training t=2592, loss=0.046578558161854744\n",
      "Surface training t=2593, loss=0.04529391974210739\n",
      "Surface training t=2594, loss=0.04484446719288826\n",
      "Surface training t=2595, loss=0.04634878225624561\n",
      "Surface training t=2596, loss=0.04652414470911026\n",
      "Surface training t=2597, loss=0.048228977248072624\n",
      "Surface training t=2598, loss=0.0464143231511116\n",
      "Surface training t=2599, loss=0.047184741124510765\n",
      "Surface training t=2600, loss=0.04774344339966774\n",
      "Surface training t=2601, loss=0.04651641845703125\n",
      "Surface training t=2602, loss=0.04606485553085804\n",
      "Surface training t=2603, loss=0.04834047891199589\n",
      "Surface training t=2604, loss=0.04648130387067795\n",
      "Surface training t=2605, loss=0.04605739191174507\n",
      "Surface training t=2606, loss=0.04465366527438164\n",
      "Surface training t=2607, loss=0.04703957214951515\n",
      "Surface training t=2608, loss=0.047166282311081886\n",
      "Surface training t=2609, loss=0.04582599550485611\n",
      "Surface training t=2610, loss=0.04572669975459576\n",
      "Surface training t=2611, loss=0.04793727770447731\n",
      "Surface training t=2612, loss=0.04577768221497536\n",
      "Surface training t=2613, loss=0.04748006351292133\n",
      "Surface training t=2614, loss=0.0452603492885828\n",
      "Surface training t=2615, loss=0.04505036026239395\n",
      "Surface training t=2616, loss=0.04491990618407726\n",
      "Surface training t=2617, loss=0.04539096727967262\n",
      "Surface training t=2618, loss=0.04623769782483578\n",
      "Surface training t=2619, loss=0.04542135260999203\n",
      "Surface training t=2620, loss=0.04649553261697292\n",
      "Surface training t=2621, loss=0.045556265860795975\n",
      "Surface training t=2622, loss=0.046125466004014015\n",
      "Surface training t=2623, loss=0.044761428609490395\n",
      "Surface training t=2624, loss=0.04612681083381176\n",
      "Surface training t=2625, loss=0.0460443701595068\n",
      "Surface training t=2626, loss=0.045605460181832314\n",
      "Surface training t=2627, loss=0.04466376081109047\n",
      "Surface training t=2628, loss=0.04637056402862072\n",
      "Surface training t=2629, loss=0.045525018125772476\n",
      "Surface training t=2630, loss=0.0458044670522213\n",
      "Surface training t=2631, loss=0.04565254971385002\n",
      "Surface training t=2632, loss=0.04587552696466446\n",
      "Surface training t=2633, loss=0.04659508913755417\n",
      "Surface training t=2634, loss=0.0450130682438612\n",
      "Surface training t=2635, loss=0.04523671977221966\n",
      "Surface training t=2636, loss=0.04478173702955246\n",
      "Surface training t=2637, loss=0.0460471548140049\n",
      "Surface training t=2638, loss=0.04512779787182808\n",
      "Surface training t=2639, loss=0.044865505769848824\n",
      "Surface training t=2640, loss=0.045940591022372246\n",
      "Surface training t=2641, loss=0.04678590968251228\n",
      "Surface training t=2642, loss=0.04547116532921791\n",
      "Surface training t=2643, loss=0.04655960947275162\n",
      "Surface training t=2644, loss=0.04596422798931599\n",
      "Surface training t=2645, loss=0.045211534947156906\n",
      "Surface training t=2646, loss=0.04583008214831352\n",
      "Surface training t=2647, loss=0.045511139556765556\n",
      "Surface training t=2648, loss=0.04635540954768658\n",
      "Surface training t=2649, loss=0.04413263686001301\n",
      "Surface training t=2650, loss=0.046140531077980995\n",
      "Surface training t=2651, loss=0.044431524351239204\n",
      "Surface training t=2652, loss=0.04601265862584114\n",
      "Surface training t=2653, loss=0.04727373085916042\n",
      "Surface training t=2654, loss=0.04617287777364254\n",
      "Surface training t=2655, loss=0.04520651698112488\n",
      "Surface training t=2656, loss=0.04602297581732273\n",
      "Surface training t=2657, loss=0.04540395922958851\n",
      "Surface training t=2658, loss=0.04521876014769077\n",
      "Surface training t=2659, loss=0.04565109498798847\n",
      "Surface training t=2660, loss=0.044584065675735474\n",
      "Surface training t=2661, loss=0.04531575366854668\n",
      "Surface training t=2662, loss=0.045691508799791336\n",
      "Surface training t=2663, loss=0.04569215141236782\n",
      "Surface training t=2664, loss=0.045834703370928764\n",
      "Surface training t=2665, loss=0.04635830223560333\n",
      "Surface training t=2666, loss=0.04576599970459938\n",
      "Surface training t=2667, loss=0.046302568167448044\n",
      "Surface training t=2668, loss=0.04431406036019325\n",
      "Surface training t=2669, loss=0.04504657909274101\n",
      "Surface training t=2670, loss=0.04465252906084061\n",
      "Surface training t=2671, loss=0.04549861699342728\n",
      "Surface training t=2672, loss=0.04410629905760288\n",
      "Surface training t=2673, loss=0.044173238798975945\n",
      "Surface training t=2674, loss=0.04420514591038227\n",
      "Surface training t=2675, loss=0.04629678837954998\n",
      "Surface training t=2676, loss=0.04538478143513203\n",
      "Surface training t=2677, loss=0.04677924886345863\n",
      "Surface training t=2678, loss=0.04623718932271004\n",
      "Surface training t=2679, loss=0.046123383566737175\n",
      "Surface training t=2680, loss=0.04502669535577297\n",
      "Surface training t=2681, loss=0.044923678040504456\n",
      "Surface training t=2682, loss=0.04434693232178688\n",
      "Surface training t=2683, loss=0.04456385225057602\n",
      "Surface training t=2684, loss=0.046819547191262245\n",
      "Surface training t=2685, loss=0.044620100408792496\n",
      "Surface training t=2686, loss=0.04527948796749115\n",
      "Surface training t=2687, loss=0.044201379641890526\n",
      "Surface training t=2688, loss=0.04614377208054066\n",
      "Surface training t=2689, loss=0.04525204002857208\n",
      "Surface training t=2690, loss=0.04757399670779705\n",
      "Surface training t=2691, loss=0.04467346705496311\n",
      "Surface training t=2692, loss=0.04461457207798958\n",
      "Surface training t=2693, loss=0.04431670345366001\n",
      "Surface training t=2694, loss=0.04606032557785511\n",
      "Surface training t=2695, loss=0.04443022422492504\n",
      "Surface training t=2696, loss=0.04615930840373039\n",
      "Surface training t=2697, loss=0.04403337277472019\n",
      "Surface training t=2698, loss=0.04558003321290016\n",
      "Surface training t=2699, loss=0.046606263145804405\n",
      "Surface training t=2700, loss=0.04553060233592987\n",
      "Surface training t=2701, loss=0.047174589708447456\n",
      "Surface training t=2702, loss=0.046265434473752975\n",
      "Surface training t=2703, loss=0.04692770726978779\n",
      "Surface training t=2704, loss=0.04867454431951046\n",
      "Surface training t=2705, loss=0.044658362865448\n",
      "Surface training t=2706, loss=0.046233950182795525\n",
      "Surface training t=2707, loss=0.04520046152174473\n",
      "Surface training t=2708, loss=0.045976923778653145\n",
      "Surface training t=2709, loss=0.045740144327282906\n",
      "Surface training t=2710, loss=0.04468085058033466\n",
      "Surface training t=2711, loss=0.046077974140644073\n",
      "Surface training t=2712, loss=0.04787263087928295\n",
      "Surface training t=2713, loss=0.04643499292433262\n",
      "Surface training t=2714, loss=0.04527786746621132\n",
      "Surface training t=2715, loss=0.04761624336242676\n",
      "Surface training t=2716, loss=0.04791680723428726\n",
      "Surface training t=2717, loss=0.045368269085884094\n",
      "Surface training t=2718, loss=0.045639097690582275\n",
      "Surface training t=2719, loss=0.04647089168429375\n",
      "Surface training t=2720, loss=0.045292096212506294\n",
      "Surface training t=2721, loss=0.046924151480197906\n",
      "Surface training t=2722, loss=0.04435540363192558\n",
      "Surface training t=2723, loss=0.045636748895049095\n",
      "Surface training t=2724, loss=0.045402729883790016\n",
      "Surface training t=2725, loss=0.04485374316573143\n",
      "Surface training t=2726, loss=0.04627933166921139\n",
      "Surface training t=2727, loss=0.044206446036696434\n",
      "Surface training t=2728, loss=0.04651639983057976\n",
      "Surface training t=2729, loss=0.046053048223257065\n",
      "Surface training t=2730, loss=0.045992108061909676\n",
      "Surface training t=2731, loss=0.04829586297273636\n",
      "Surface training t=2732, loss=0.048974575474858284\n",
      "Surface training t=2733, loss=0.047130679711699486\n",
      "Surface training t=2734, loss=0.045331135392189026\n",
      "Surface training t=2735, loss=0.04668295010924339\n",
      "Surface training t=2736, loss=0.04522267356514931\n",
      "Surface training t=2737, loss=0.04513515532016754\n",
      "Surface training t=2738, loss=0.0461804810911417\n",
      "Surface training t=2739, loss=0.04542236961424351\n",
      "Surface training t=2740, loss=0.04654083028435707\n",
      "Surface training t=2741, loss=0.04452211409807205\n",
      "Surface training t=2742, loss=0.045665618032217026\n",
      "Surface training t=2743, loss=0.046596720814704895\n",
      "Surface training t=2744, loss=0.045810261741280556\n",
      "Surface training t=2745, loss=0.0452913586050272\n",
      "Surface training t=2746, loss=0.04486307315528393\n",
      "Surface training t=2747, loss=0.04430759511888027\n",
      "Surface training t=2748, loss=0.04328620992600918\n",
      "Surface training t=2749, loss=0.04525417648255825\n",
      "Surface training t=2750, loss=0.04684820584952831\n",
      "Surface training t=2751, loss=0.044299857690930367\n",
      "Surface training t=2752, loss=0.04491096921265125\n",
      "Surface training t=2753, loss=0.045448144897818565\n",
      "Surface training t=2754, loss=0.042991358786821365\n",
      "Surface training t=2755, loss=0.046110231429338455\n",
      "Surface training t=2756, loss=0.04498625174164772\n",
      "Surface training t=2757, loss=0.04660333879292011\n",
      "Surface training t=2758, loss=0.04433058947324753\n",
      "Surface training t=2759, loss=0.04805814474821091\n",
      "Surface training t=2760, loss=0.04751569777727127\n",
      "Surface training t=2761, loss=0.04399964213371277\n",
      "Surface training t=2762, loss=0.04744848422706127\n",
      "Surface training t=2763, loss=0.045113470405340195\n",
      "Surface training t=2764, loss=0.04580723121762276\n",
      "Surface training t=2765, loss=0.047467516735196114\n",
      "Surface training t=2766, loss=0.04661688581109047\n",
      "Surface training t=2767, loss=0.045417988672852516\n",
      "Surface training t=2768, loss=0.04648736119270325\n",
      "Surface training t=2769, loss=0.04611278511583805\n",
      "Surface training t=2770, loss=0.04555116966366768\n",
      "Surface training t=2771, loss=0.04659857600927353\n",
      "Surface training t=2772, loss=0.04389195516705513\n",
      "Surface training t=2773, loss=0.04499364085495472\n",
      "Surface training t=2774, loss=0.046126289293169975\n",
      "Surface training t=2775, loss=0.04506937973201275\n",
      "Surface training t=2776, loss=0.04605443961918354\n",
      "Surface training t=2777, loss=0.045042986050248146\n",
      "Surface training t=2778, loss=0.04490319453179836\n",
      "Surface training t=2779, loss=0.046992162242531776\n",
      "Surface training t=2780, loss=0.045421671122312546\n",
      "Surface training t=2781, loss=0.045313652604818344\n",
      "Surface training t=2782, loss=0.046814264729619026\n",
      "Surface training t=2783, loss=0.04615763947367668\n",
      "Surface training t=2784, loss=0.04515659436583519\n",
      "Surface training t=2785, loss=0.04499674215912819\n",
      "Surface training t=2786, loss=0.04661514051258564\n",
      "Surface training t=2787, loss=0.04546516202390194\n",
      "Surface training t=2788, loss=0.044034821912646294\n",
      "Surface training t=2789, loss=0.04562651552259922\n",
      "Surface training t=2790, loss=0.04610658809542656\n",
      "Surface training t=2791, loss=0.046094486489892006\n",
      "Surface training t=2792, loss=0.04670584015548229\n",
      "Surface training t=2793, loss=0.045648910105228424\n",
      "Surface training t=2794, loss=0.04723322205245495\n",
      "Surface training t=2795, loss=0.04672274552285671\n",
      "Surface training t=2796, loss=0.047031352296471596\n",
      "Surface training t=2797, loss=0.045794904232025146\n",
      "Surface training t=2798, loss=0.04464288800954819\n",
      "Surface training t=2799, loss=0.04661725088953972\n",
      "Surface training t=2800, loss=0.04773838259279728\n",
      "Surface training t=2801, loss=0.04593069665133953\n",
      "Surface training t=2802, loss=0.04715272784233093\n",
      "Surface training t=2803, loss=0.04412703029811382\n",
      "Surface training t=2804, loss=0.04562235809862614\n",
      "Surface training t=2805, loss=0.0460235308855772\n",
      "Surface training t=2806, loss=0.04804045148193836\n",
      "Surface training t=2807, loss=0.04690912552177906\n",
      "Surface training t=2808, loss=0.04638510197401047\n",
      "Surface training t=2809, loss=0.043783266097307205\n",
      "Surface training t=2810, loss=0.046019673347473145\n",
      "Surface training t=2811, loss=0.0472247488796711\n",
      "Surface training t=2812, loss=0.045170826837420464\n",
      "Surface training t=2813, loss=0.047333406284451485\n",
      "Surface training t=2814, loss=0.04735969752073288\n",
      "Surface training t=2815, loss=0.04626665823161602\n",
      "Surface training t=2816, loss=0.04622146859765053\n",
      "Surface training t=2817, loss=0.04778250865638256\n",
      "Surface training t=2818, loss=0.04745234176516533\n",
      "Surface training t=2819, loss=0.04708618298172951\n",
      "Surface training t=2820, loss=0.04650771804153919\n",
      "Surface training t=2821, loss=0.04579119384288788\n",
      "Surface training t=2822, loss=0.04544217884540558\n",
      "Surface training t=2823, loss=0.04681403748691082\n",
      "Surface training t=2824, loss=0.045312291011214256\n",
      "Surface training t=2825, loss=0.04456073045730591\n",
      "Surface training t=2826, loss=0.04556957259774208\n",
      "Surface training t=2827, loss=0.04636421240866184\n",
      "Surface training t=2828, loss=0.04540654830634594\n",
      "Surface training t=2829, loss=0.045649126172065735\n",
      "Surface training t=2830, loss=0.046439291909337044\n",
      "Surface training t=2831, loss=0.042991941794753075\n",
      "Surface training t=2832, loss=0.045733097940683365\n",
      "Surface training t=2833, loss=0.044699519872665405\n",
      "Surface training t=2834, loss=0.046412618830800056\n",
      "Surface training t=2835, loss=0.044779641553759575\n",
      "Surface training t=2836, loss=0.04428846016526222\n",
      "Surface training t=2837, loss=0.045652059838175774\n",
      "Surface training t=2838, loss=0.045687271282076836\n",
      "Surface training t=2839, loss=0.043766142800450325\n",
      "Surface training t=2840, loss=0.04370350390672684\n",
      "Surface training t=2841, loss=0.044613827019929886\n",
      "Surface training t=2842, loss=0.045398179441690445\n",
      "Surface training t=2843, loss=0.04495304264128208\n",
      "Surface training t=2844, loss=0.04458330385386944\n",
      "Surface training t=2845, loss=0.04586242511868477\n",
      "Surface training t=2846, loss=0.044310469180345535\n",
      "Surface training t=2847, loss=0.04667166620492935\n",
      "Surface training t=2848, loss=0.04542381130158901\n",
      "Surface training t=2849, loss=0.046004803851246834\n",
      "Surface training t=2850, loss=0.04346760921180248\n",
      "Surface training t=2851, loss=0.04399818181991577\n",
      "Surface training t=2852, loss=0.04492116533219814\n",
      "Surface training t=2853, loss=0.044443024322390556\n",
      "Surface training t=2854, loss=0.04530724324285984\n",
      "Surface training t=2855, loss=0.04404370114207268\n",
      "Surface training t=2856, loss=0.04415767267346382\n",
      "Surface training t=2857, loss=0.04626849107444286\n",
      "Surface training t=2858, loss=0.0440545529127121\n",
      "Surface training t=2859, loss=0.04459741339087486\n",
      "Surface training t=2860, loss=0.04490886814892292\n",
      "Surface training t=2861, loss=0.043549470603466034\n",
      "Surface training t=2862, loss=0.04673975519835949\n",
      "Surface training t=2863, loss=0.045309482142329216\n",
      "Surface training t=2864, loss=0.04795081354677677\n",
      "Surface training t=2865, loss=0.04581686854362488\n",
      "Surface training t=2866, loss=0.046939630061388016\n",
      "Surface training t=2867, loss=0.04506263695657253\n",
      "Surface training t=2868, loss=0.045497579500079155\n",
      "Surface training t=2869, loss=0.04510098695755005\n",
      "Surface training t=2870, loss=0.04486958310008049\n",
      "Surface training t=2871, loss=0.04402855597436428\n",
      "Surface training t=2872, loss=0.045136358588933945\n",
      "Surface training t=2873, loss=0.044525228440761566\n",
      "Surface training t=2874, loss=0.04512861929833889\n",
      "Surface training t=2875, loss=0.044353922829031944\n",
      "Surface training t=2876, loss=0.04631713591516018\n",
      "Surface training t=2877, loss=0.04560948722064495\n",
      "Surface training t=2878, loss=0.045590875670313835\n",
      "Surface training t=2879, loss=0.04459474794566631\n",
      "Surface training t=2880, loss=0.044228192418813705\n",
      "Surface training t=2881, loss=0.04517606273293495\n",
      "Surface training t=2882, loss=0.04816518723964691\n",
      "Surface training t=2883, loss=0.045440906658768654\n",
      "Surface training t=2884, loss=0.046021439135074615\n",
      "Surface training t=2885, loss=0.04661307856440544\n",
      "Surface training t=2886, loss=0.04470479115843773\n",
      "Surface training t=2887, loss=0.045782458037137985\n",
      "Surface training t=2888, loss=0.04425845295190811\n",
      "Surface training t=2889, loss=0.04533631540834904\n",
      "Surface training t=2890, loss=0.04391337186098099\n",
      "Surface training t=2891, loss=0.04516098462045193\n",
      "Surface training t=2892, loss=0.04387343302369118\n",
      "Surface training t=2893, loss=0.044353172183036804\n",
      "Surface training t=2894, loss=0.04566200263798237\n",
      "Surface training t=2895, loss=0.04477447643876076\n",
      "Surface training t=2896, loss=0.04670974425971508\n",
      "Surface training t=2897, loss=0.044480107724666595\n",
      "Surface training t=2898, loss=0.04718026518821716\n",
      "Surface training t=2899, loss=0.046242909505963326\n",
      "Surface training t=2900, loss=0.04677797295153141\n",
      "Surface training t=2901, loss=0.04550427570939064\n",
      "Surface training t=2902, loss=0.045678885653615\n",
      "Surface training t=2903, loss=0.04402168653905392\n",
      "Surface training t=2904, loss=0.04575424641370773\n",
      "Surface training t=2905, loss=0.047233620658516884\n",
      "Surface training t=2906, loss=0.04605001583695412\n",
      "Surface training t=2907, loss=0.04486054740846157\n",
      "Surface training t=2908, loss=0.04576904699206352\n",
      "Surface training t=2909, loss=0.045666519552469254\n",
      "Surface training t=2910, loss=0.04394879378378391\n",
      "Surface training t=2911, loss=0.04757556691765785\n",
      "Surface training t=2912, loss=0.04607575945556164\n",
      "Surface training t=2913, loss=0.046214375644922256\n",
      "Surface training t=2914, loss=0.04641913063824177\n",
      "Surface training t=2915, loss=0.048480672761797905\n",
      "Surface training t=2916, loss=0.04519285447895527\n",
      "Surface training t=2917, loss=0.04523695260286331\n",
      "Surface training t=2918, loss=0.0458783321082592\n",
      "Surface training t=2919, loss=0.04441915266215801\n",
      "Surface training t=2920, loss=0.04605790413916111\n",
      "Surface training t=2921, loss=0.04498914256691933\n",
      "Surface training t=2922, loss=0.04703673720359802\n",
      "Surface training t=2923, loss=0.0431851577013731\n",
      "Surface training t=2924, loss=0.04457331262528896\n",
      "Surface training t=2925, loss=0.04389810562133789\n",
      "Surface training t=2926, loss=0.04359477385878563\n",
      "Surface training t=2927, loss=0.044772807508707047\n",
      "Surface training t=2928, loss=0.045598454773426056\n",
      "Surface training t=2929, loss=0.044474491849541664\n",
      "Surface training t=2930, loss=0.04353455640375614\n",
      "Surface training t=2931, loss=0.0440220981836319\n",
      "Surface training t=2932, loss=0.0445275641977787\n",
      "Surface training t=2933, loss=0.04389110580086708\n",
      "Surface training t=2934, loss=0.044177114963531494\n",
      "Surface training t=2935, loss=0.04549809731543064\n",
      "Surface training t=2936, loss=0.04331720061600208\n",
      "Surface training t=2937, loss=0.04498260095715523\n",
      "Surface training t=2938, loss=0.043736781924963\n",
      "Surface training t=2939, loss=0.04439970292150974\n",
      "Surface training t=2940, loss=0.04483444802463055\n",
      "Surface training t=2941, loss=0.045696528628468513\n",
      "Surface training t=2942, loss=0.04535972885787487\n",
      "Surface training t=2943, loss=0.04427323117852211\n",
      "Surface training t=2944, loss=0.04459756053984165\n",
      "Surface training t=2945, loss=0.046538183465600014\n",
      "Surface training t=2946, loss=0.04648750647902489\n",
      "Surface training t=2947, loss=0.047230957075953484\n",
      "Surface training t=2948, loss=0.0462585911154747\n",
      "Surface training t=2949, loss=0.0450251679867506\n",
      "Surface training t=2950, loss=0.047086201608181\n",
      "Surface training t=2951, loss=0.0452753622084856\n",
      "Surface training t=2952, loss=0.04644856974482536\n",
      "Surface training t=2953, loss=0.045376794412732124\n",
      "Surface training t=2954, loss=0.04644237458705902\n",
      "Surface training t=2955, loss=0.045479945838451385\n",
      "Surface training t=2956, loss=0.04679936543107033\n",
      "Surface training t=2957, loss=0.04600716009736061\n",
      "Surface training t=2958, loss=0.04355929233133793\n",
      "Surface training t=2959, loss=0.04675469547510147\n",
      "Surface training t=2960, loss=0.0460189376026392\n",
      "Surface training t=2961, loss=0.04529944434762001\n",
      "Surface training t=2962, loss=0.042752111330628395\n",
      "Surface training t=2963, loss=0.04404681921005249\n",
      "Surface training t=2964, loss=0.04390490986406803\n",
      "Surface training t=2965, loss=0.04474015720188618\n",
      "Surface training t=2966, loss=0.04435078985989094\n",
      "Surface training t=2967, loss=0.044521113857626915\n",
      "Surface training t=2968, loss=0.043896524235606194\n",
      "Surface training t=2969, loss=0.044123757630586624\n",
      "Surface training t=2970, loss=0.04697084054350853\n",
      "Surface training t=2971, loss=0.045042604207992554\n",
      "Surface training t=2972, loss=0.04560726135969162\n",
      "Surface training t=2973, loss=0.048282187432050705\n",
      "Surface training t=2974, loss=0.047946734353899956\n",
      "Surface training t=2975, loss=0.045676464214921\n",
      "Surface training t=2976, loss=0.04504692927002907\n",
      "Surface training t=2977, loss=0.04708356596529484\n",
      "Surface training t=2978, loss=0.04396313801407814\n",
      "Surface training t=2979, loss=0.04520717263221741\n",
      "Surface training t=2980, loss=0.044697146862745285\n",
      "Surface training t=2981, loss=0.04688839428126812\n",
      "Surface training t=2982, loss=0.045481959357857704\n",
      "Surface training t=2983, loss=0.045339638367295265\n",
      "Surface training t=2984, loss=0.04447370767593384\n",
      "Surface training t=2985, loss=0.044288838282227516\n",
      "Surface training t=2986, loss=0.044640855863690376\n",
      "Surface training t=2987, loss=0.04498688504099846\n",
      "Surface training t=2988, loss=0.045161930844187737\n",
      "Surface training t=2989, loss=0.04362325556576252\n",
      "Surface training t=2990, loss=0.04369855672121048\n",
      "Surface training t=2991, loss=0.04718048311769962\n",
      "Surface training t=2992, loss=0.043682388961315155\n",
      "Surface training t=2993, loss=0.04467196390032768\n",
      "Surface training t=2994, loss=0.045178210362792015\n",
      "Surface training t=2995, loss=0.04402477294206619\n",
      "Surface training t=2996, loss=0.04694818705320358\n",
      "Surface training t=2997, loss=0.04520934633910656\n",
      "Surface training t=2998, loss=0.043847743421792984\n",
      "Surface training t=2999, loss=0.04483509808778763\n",
      "Surface training t=3000, loss=0.04436041973531246\n",
      "Surface training t=3001, loss=0.04353939741849899\n",
      "Surface training t=3002, loss=0.04520287737250328\n",
      "Surface training t=3003, loss=0.045138031244277954\n",
      "Surface training t=3004, loss=0.04372766055166721\n",
      "Surface training t=3005, loss=0.045557525008916855\n",
      "Surface training t=3006, loss=0.043721096590161324\n",
      "Surface training t=3007, loss=0.043383678421378136\n",
      "Surface training t=3008, loss=0.043744346126914024\n",
      "Surface training t=3009, loss=0.045168982818722725\n",
      "Surface training t=3010, loss=0.043910715728998184\n",
      "Surface training t=3011, loss=0.043536609038710594\n",
      "Surface training t=3012, loss=0.04527821205556393\n",
      "Surface training t=3013, loss=0.046400995925068855\n",
      "Surface training t=3014, loss=0.04476896859705448\n",
      "Surface training t=3015, loss=0.04471338167786598\n",
      "Surface training t=3016, loss=0.0442969836294651\n",
      "Surface training t=3017, loss=0.0443658959120512\n",
      "Surface training t=3018, loss=0.04465040564537048\n",
      "Surface training t=3019, loss=0.046067025512456894\n",
      "Surface training t=3020, loss=0.045650970190763474\n",
      "Surface training t=3021, loss=0.04360989294946194\n",
      "Surface training t=3022, loss=0.04413662664592266\n",
      "Surface training t=3023, loss=0.043701715767383575\n",
      "Surface training t=3024, loss=0.04476010613143444\n",
      "Surface training t=3025, loss=0.04498465545475483\n",
      "Surface training t=3026, loss=0.04546205885708332\n",
      "Surface training t=3027, loss=0.04573826678097248\n",
      "Surface training t=3028, loss=0.045380499213933945\n",
      "Surface training t=3029, loss=0.04410933516919613\n",
      "Surface training t=3030, loss=0.043660376220941544\n",
      "Surface training t=3031, loss=0.043726300820708275\n",
      "Surface training t=3032, loss=0.04384123720228672\n",
      "Surface training t=3033, loss=0.04564676061272621\n",
      "Surface training t=3034, loss=0.04501040652394295\n",
      "Surface training t=3035, loss=0.04530016519129276\n",
      "Surface training t=3036, loss=0.04421777091920376\n",
      "Surface training t=3037, loss=0.04572925344109535\n",
      "Surface training t=3038, loss=0.04403867945075035\n",
      "Surface training t=3039, loss=0.044127751141786575\n",
      "Surface training t=3040, loss=0.046027958393096924\n",
      "Surface training t=3041, loss=0.04565764404833317\n",
      "Surface training t=3042, loss=0.04366931691765785\n",
      "Surface training t=3043, loss=0.04474414139986038\n",
      "Surface training t=3044, loss=0.044737428426742554\n",
      "Surface training t=3045, loss=0.044830746948719025\n",
      "Surface training t=3046, loss=0.0436989925801754\n",
      "Surface training t=3047, loss=0.04620598070323467\n",
      "Surface training t=3048, loss=0.04337309114634991\n",
      "Surface training t=3049, loss=0.04425865598022938\n",
      "Surface training t=3050, loss=0.0441090352833271\n",
      "Surface training t=3051, loss=0.04580749198794365\n",
      "Surface training t=3052, loss=0.044846586883068085\n",
      "Surface training t=3053, loss=0.043284252285957336\n",
      "Surface training t=3054, loss=0.044684477150440216\n",
      "Surface training t=3055, loss=0.04449388384819031\n",
      "Surface training t=3056, loss=0.04446231201291084\n",
      "Surface training t=3057, loss=0.045165661722421646\n",
      "Surface training t=3058, loss=0.04341758415102959\n",
      "Surface training t=3059, loss=0.04525189474225044\n",
      "Surface training t=3060, loss=0.04191318340599537\n",
      "Surface training t=3061, loss=0.04551400616765022\n",
      "Surface training t=3062, loss=0.04406722076237202\n",
      "Surface training t=3063, loss=0.0443103089928627\n",
      "Surface training t=3064, loss=0.043417492881417274\n",
      "Surface training t=3065, loss=0.04507228173315525\n",
      "Surface training t=3066, loss=0.04541422799229622\n",
      "Surface training t=3067, loss=0.04387318715453148\n",
      "Surface training t=3068, loss=0.04543633759021759\n",
      "Surface training t=3069, loss=0.0438904482871294\n",
      "Surface training t=3070, loss=0.04461609944701195\n",
      "Surface training t=3071, loss=0.04352831654250622\n",
      "Surface training t=3072, loss=0.043482379987835884\n",
      "Surface training t=3073, loss=0.04383993148803711\n",
      "Surface training t=3074, loss=0.04598008655011654\n",
      "Surface training t=3075, loss=0.0427149273455143\n",
      "Surface training t=3076, loss=0.04285095073282719\n",
      "Surface training t=3077, loss=0.04364628903567791\n",
      "Surface training t=3078, loss=0.04421079531311989\n",
      "Surface training t=3079, loss=0.04324538633227348\n",
      "Surface training t=3080, loss=0.04449140280485153\n",
      "Surface training t=3081, loss=0.043258655816316605\n",
      "Surface training t=3082, loss=0.04435807093977928\n",
      "Surface training t=3083, loss=0.04411784000694752\n",
      "Surface training t=3084, loss=0.0446687676012516\n",
      "Surface training t=3085, loss=0.044649362564086914\n",
      "Surface training t=3086, loss=0.0457982812076807\n",
      "Surface training t=3087, loss=0.04553726315498352\n",
      "Surface training t=3088, loss=0.0439329668879509\n",
      "Surface training t=3089, loss=0.04514402151107788\n",
      "Surface training t=3090, loss=0.043826377019286156\n",
      "Surface training t=3091, loss=0.044221339747309685\n",
      "Surface training t=3092, loss=0.04411466047167778\n",
      "Surface training t=3093, loss=0.04465978965163231\n",
      "Surface training t=3094, loss=0.04467563331127167\n",
      "Surface training t=3095, loss=0.045005278661847115\n",
      "Surface training t=3096, loss=0.04381324350833893\n",
      "Surface training t=3097, loss=0.04323472082614899\n",
      "Surface training t=3098, loss=0.04544462077319622\n",
      "Surface training t=3099, loss=0.04486938565969467\n",
      "Surface training t=3100, loss=0.04459543526172638\n",
      "Surface training t=3101, loss=0.04399252124130726\n",
      "Surface training t=3102, loss=0.046872926875948906\n",
      "Surface training t=3103, loss=0.04442809335887432\n",
      "Surface training t=3104, loss=0.04511295445263386\n",
      "Surface training t=3105, loss=0.0433716606348753\n",
      "Surface training t=3106, loss=0.04484042897820473\n",
      "Surface training t=3107, loss=0.04138595797121525\n",
      "Surface training t=3108, loss=0.044891396537423134\n",
      "Surface training t=3109, loss=0.04335343651473522\n",
      "Surface training t=3110, loss=0.04402019456028938\n",
      "Surface training t=3111, loss=0.04564359411597252\n",
      "Surface training t=3112, loss=0.04595812037587166\n",
      "Surface training t=3113, loss=0.04715052805840969\n",
      "Surface training t=3114, loss=0.04730163887143135\n",
      "Surface training t=3115, loss=0.04423145018517971\n",
      "Surface training t=3116, loss=0.04646433889865875\n",
      "Surface training t=3117, loss=0.04767253063619137\n",
      "Surface training t=3118, loss=0.046339137479662895\n",
      "Surface training t=3119, loss=0.04468217492103577\n",
      "Surface training t=3120, loss=0.04489403963088989\n",
      "Surface training t=3121, loss=0.04334905184805393\n",
      "Surface training t=3122, loss=0.04515248164534569\n",
      "Surface training t=3123, loss=0.04366123862564564\n",
      "Surface training t=3124, loss=0.044221384450793266\n",
      "Surface training t=3125, loss=0.04775881767272949\n",
      "Surface training t=3126, loss=0.04468591883778572\n",
      "Surface training t=3127, loss=0.04699442535638809\n",
      "Surface training t=3128, loss=0.04497081972658634\n",
      "Surface training t=3129, loss=0.04723796062171459\n",
      "Surface training t=3130, loss=0.04267960786819458\n",
      "Surface training t=3131, loss=0.04651764780282974\n",
      "Surface training t=3132, loss=0.04411721229553223\n",
      "Surface training t=3133, loss=0.044995833188295364\n",
      "Surface training t=3134, loss=0.0454461220651865\n",
      "Surface training t=3135, loss=0.04497555084526539\n",
      "Surface training t=3136, loss=0.04555908404290676\n",
      "Surface training t=3137, loss=0.045794904232025146\n",
      "Surface training t=3138, loss=0.04724372178316116\n",
      "Surface training t=3139, loss=0.043562283739447594\n",
      "Surface training t=3140, loss=0.04555119760334492\n",
      "Surface training t=3141, loss=0.044082069769501686\n",
      "Surface training t=3142, loss=0.04523301310837269\n",
      "Surface training t=3143, loss=0.04700223542749882\n",
      "Surface training t=3144, loss=0.0453662034124136\n",
      "Surface training t=3145, loss=0.0439772829413414\n",
      "Surface training t=3146, loss=0.04594223201274872\n",
      "Surface training t=3147, loss=0.04676731862127781\n",
      "Surface training t=3148, loss=0.043882105499506\n",
      "Surface training t=3149, loss=0.04381336085498333\n",
      "Surface training t=3150, loss=0.045115046203136444\n",
      "Surface training t=3151, loss=0.04315386712551117\n",
      "Surface training t=3152, loss=0.04380432330071926\n",
      "Surface training t=3153, loss=0.04337539151310921\n",
      "Surface training t=3154, loss=0.04480223171412945\n",
      "Surface training t=3155, loss=0.04319087974727154\n",
      "Surface training t=3156, loss=0.04563017934560776\n",
      "Surface training t=3157, loss=0.04476025886833668\n",
      "Surface training t=3158, loss=0.044141147285699844\n",
      "Surface training t=3159, loss=0.04349405877292156\n",
      "Surface training t=3160, loss=0.04426572844386101\n",
      "Surface training t=3161, loss=0.04436499439179897\n",
      "Surface training t=3162, loss=0.04469512403011322\n",
      "Surface training t=3163, loss=0.043215030804276466\n",
      "Surface training t=3164, loss=0.04376573860645294\n",
      "Surface training t=3165, loss=0.04454940930008888\n",
      "Surface training t=3166, loss=0.043470609933137894\n",
      "Surface training t=3167, loss=0.043829215690493584\n",
      "Surface training t=3168, loss=0.04084315896034241\n",
      "Surface training t=3169, loss=0.042051276192069054\n",
      "Surface training t=3170, loss=0.04263378866016865\n",
      "Surface training t=3171, loss=0.0432258453220129\n",
      "Surface training t=3172, loss=0.04320644214749336\n",
      "Surface training t=3173, loss=0.04355754889547825\n",
      "Surface training t=3174, loss=0.04306425713002682\n",
      "Surface training t=3175, loss=0.043461062014102936\n",
      "Surface training t=3176, loss=0.04345603659749031\n",
      "Surface training t=3177, loss=0.04452185705304146\n",
      "Surface training t=3178, loss=0.0444742813706398\n",
      "Surface training t=3179, loss=0.0445804949849844\n",
      "Surface training t=3180, loss=0.0451589971780777\n",
      "Surface training t=3181, loss=0.04270582273602486\n",
      "Surface training t=3182, loss=0.044385770335793495\n",
      "Surface training t=3183, loss=0.04577738232910633\n",
      "Surface training t=3184, loss=0.04558343254029751\n",
      "Surface training t=3185, loss=0.04484212584793568\n",
      "Surface training t=3186, loss=0.04659164696931839\n",
      "Surface training t=3187, loss=0.044692788273096085\n",
      "Surface training t=3188, loss=0.04396976716816425\n",
      "Surface training t=3189, loss=0.04482002556324005\n",
      "Surface training t=3190, loss=0.04502401500940323\n",
      "Surface training t=3191, loss=0.04393254406750202\n",
      "Surface training t=3192, loss=0.04679727181792259\n",
      "Surface training t=3193, loss=0.04309555143117905\n",
      "Surface training t=3194, loss=0.04376836493611336\n",
      "Surface training t=3195, loss=0.04494067281484604\n",
      "Surface training t=3196, loss=0.04559594579041004\n",
      "Surface training t=3197, loss=0.04494627192616463\n",
      "Surface training t=3198, loss=0.04452282749116421\n",
      "Surface training t=3199, loss=0.04385069198906422\n",
      "Surface training t=3200, loss=0.04373379237949848\n",
      "Surface training t=3201, loss=0.04399155639111996\n",
      "Surface training t=3202, loss=0.043660836294293404\n",
      "Surface training t=3203, loss=0.044515328481793404\n",
      "Surface training t=3204, loss=0.044082483276724815\n",
      "Surface training t=3205, loss=0.044613661244511604\n",
      "Surface training t=3206, loss=0.04434012807905674\n",
      "Surface training t=3207, loss=0.045308563858270645\n",
      "Surface training t=3208, loss=0.04384882189333439\n",
      "Surface training t=3209, loss=0.043031398206949234\n",
      "Surface training t=3210, loss=0.043817974627017975\n",
      "Surface training t=3211, loss=0.04300061613321304\n",
      "Surface training t=3212, loss=0.04471382312476635\n",
      "Surface training t=3213, loss=0.046477654948830605\n",
      "Surface training t=3214, loss=0.04421388544142246\n",
      "Surface training t=3215, loss=0.04514407739043236\n",
      "Surface training t=3216, loss=0.04398024082183838\n",
      "Surface training t=3217, loss=0.04588698595762253\n",
      "Surface training t=3218, loss=0.042376765981316566\n",
      "Surface training t=3219, loss=0.04563836567103863\n",
      "Surface training t=3220, loss=0.046529581770300865\n",
      "Surface training t=3221, loss=0.04446326196193695\n",
      "Surface training t=3222, loss=0.043787725269794464\n",
      "Surface training t=3223, loss=0.04465097561478615\n",
      "Surface training t=3224, loss=0.043833281844854355\n",
      "Surface training t=3225, loss=0.04472528584301472\n",
      "Surface training t=3226, loss=0.044740645214915276\n",
      "Surface training t=3227, loss=0.0446057952940464\n",
      "Surface training t=3228, loss=0.04542997106909752\n",
      "Surface training t=3229, loss=0.04415731132030487\n",
      "Surface training t=3230, loss=0.044344622641801834\n",
      "Surface training t=3231, loss=0.04360023885965347\n",
      "Surface training t=3232, loss=0.044466450810432434\n",
      "Surface training t=3233, loss=0.0430403184145689\n",
      "Surface training t=3234, loss=0.04171282611787319\n",
      "Surface training t=3235, loss=0.04310251586139202\n",
      "Surface training t=3236, loss=0.04249577969312668\n",
      "Surface training t=3237, loss=0.04484420828521252\n",
      "Surface training t=3238, loss=0.042782578617334366\n",
      "Surface training t=3239, loss=0.044335199519991875\n",
      "Surface training t=3240, loss=0.04440681263804436\n",
      "Surface training t=3241, loss=0.04372600093483925\n",
      "Surface training t=3242, loss=0.044037723913788795\n",
      "Surface training t=3243, loss=0.044813599437475204\n",
      "Surface training t=3244, loss=0.04202421195805073\n",
      "Surface training t=3245, loss=0.04231971874833107\n",
      "Surface training t=3246, loss=0.04244224354624748\n",
      "Surface training t=3247, loss=0.04296947456896305\n",
      "Surface training t=3248, loss=0.0437083188444376\n",
      "Surface training t=3249, loss=0.04308713227510452\n",
      "Surface training t=3250, loss=0.043152084574103355\n",
      "Surface training t=3251, loss=0.04425031319260597\n",
      "Surface training t=3252, loss=0.04390828125178814\n",
      "Surface training t=3253, loss=0.04326169565320015\n",
      "Surface training t=3254, loss=0.0443594753742218\n",
      "Surface training t=3255, loss=0.042006270959973335\n",
      "Surface training t=3256, loss=0.04262883961200714\n",
      "Surface training t=3257, loss=0.0427075270563364\n",
      "Surface training t=3258, loss=0.04304952546954155\n",
      "Surface training t=3259, loss=0.04263010993599892\n",
      "Surface training t=3260, loss=0.043057046830654144\n",
      "Surface training t=3261, loss=0.043648723512887955\n",
      "Surface training t=3262, loss=0.04246885888278484\n",
      "Surface training t=3263, loss=0.044059060513973236\n",
      "Surface training t=3264, loss=0.0438222661614418\n",
      "Surface training t=3265, loss=0.04475945048034191\n",
      "Surface training t=3266, loss=0.044898366555571556\n",
      "Surface training t=3267, loss=0.04270307160913944\n",
      "Surface training t=3268, loss=0.042910339310765266\n",
      "Surface training t=3269, loss=0.04385443963110447\n",
      "Surface training t=3270, loss=0.04345661774277687\n",
      "Surface training t=3271, loss=0.04442439414560795\n",
      "Surface training t=3272, loss=0.04410714469850063\n",
      "Surface training t=3273, loss=0.04352357052266598\n",
      "Surface training t=3274, loss=0.04355134814977646\n",
      "Surface training t=3275, loss=0.043123042210936546\n",
      "Surface training t=3276, loss=0.042454302310943604\n",
      "Surface training t=3277, loss=0.04343392141163349\n",
      "Surface training t=3278, loss=0.04356823489069939\n",
      "Surface training t=3279, loss=0.04346735030412674\n",
      "Surface training t=3280, loss=0.042720409110188484\n",
      "Surface training t=3281, loss=0.043145064264535904\n",
      "Surface training t=3282, loss=0.04413115233182907\n",
      "Surface training t=3283, loss=0.04269540496170521\n",
      "Surface training t=3284, loss=0.04321889579296112\n",
      "Surface training t=3285, loss=0.04582219384610653\n",
      "Surface training t=3286, loss=0.043452026322484016\n",
      "Surface training t=3287, loss=0.044269656762480736\n",
      "Surface training t=3288, loss=0.04459541104733944\n",
      "Surface training t=3289, loss=0.044242409989237785\n",
      "Surface training t=3290, loss=0.04373081773519516\n",
      "Surface training t=3291, loss=0.04509469494223595\n",
      "Surface training t=3292, loss=0.04559316672384739\n",
      "Surface training t=3293, loss=0.04476608335971832\n",
      "Surface training t=3294, loss=0.0434165820479393\n",
      "Surface training t=3295, loss=0.04465272277593613\n",
      "Surface training t=3296, loss=0.04477069154381752\n",
      "Surface training t=3297, loss=0.044082680717110634\n",
      "Surface training t=3298, loss=0.04191490262746811\n",
      "Surface training t=3299, loss=0.0428532250225544\n",
      "Surface training t=3300, loss=0.04355139844119549\n",
      "Surface training t=3301, loss=0.04345466382801533\n",
      "Surface training t=3302, loss=0.0445057637989521\n",
      "Surface training t=3303, loss=0.042655015364289284\n",
      "Surface training t=3304, loss=0.043321846053004265\n",
      "Surface training t=3305, loss=0.042910393327474594\n",
      "Surface training t=3306, loss=0.046588290482759476\n",
      "Surface training t=3307, loss=0.04388959892094135\n",
      "Surface training t=3308, loss=0.04607489891350269\n",
      "Surface training t=3309, loss=0.04620733670890331\n",
      "Surface training t=3310, loss=0.045925432816147804\n",
      "Surface training t=3311, loss=0.04474786855280399\n",
      "Surface training t=3312, loss=0.04457925632596016\n",
      "Surface training t=3313, loss=0.04574991762638092\n",
      "Surface training t=3314, loss=0.0452963225543499\n",
      "Surface training t=3315, loss=0.04527479223906994\n",
      "Surface training t=3316, loss=0.04349926672875881\n",
      "Surface training t=3317, loss=0.04422551952302456\n",
      "Surface training t=3318, loss=0.044014232233166695\n",
      "Surface training t=3319, loss=0.04481254518032074\n",
      "Surface training t=3320, loss=0.0453312024474144\n",
      "Surface training t=3321, loss=0.04512408934533596\n",
      "Surface training t=3322, loss=0.04467891715466976\n",
      "Surface training t=3323, loss=0.045666495338082314\n",
      "Surface training t=3324, loss=0.04363795183598995\n",
      "Surface training t=3325, loss=0.04316133260726929\n",
      "Surface training t=3326, loss=0.04296756163239479\n",
      "Surface training t=3327, loss=0.043480681255459785\n",
      "Surface training t=3328, loss=0.04386088438332081\n",
      "Surface training t=3329, loss=0.04181651212275028\n",
      "Surface training t=3330, loss=0.04398762993514538\n",
      "Surface training t=3331, loss=0.04408370144665241\n",
      "Surface training t=3332, loss=0.043663132935762405\n",
      "Surface training t=3333, loss=0.04453844390809536\n",
      "Surface training t=3334, loss=0.04483727738261223\n",
      "Surface training t=3335, loss=0.043796712532639503\n",
      "Surface training t=3336, loss=0.043844398111104965\n",
      "Surface training t=3337, loss=0.04327808693051338\n",
      "Surface training t=3338, loss=0.04444735683500767\n",
      "Surface training t=3339, loss=0.044739192351698875\n",
      "Surface training t=3340, loss=0.044594014063477516\n",
      "Surface training t=3341, loss=0.042973801493644714\n",
      "Surface training t=3342, loss=0.04357118904590607\n",
      "Surface training t=3343, loss=0.04315592348575592\n",
      "Surface training t=3344, loss=0.04600280337035656\n",
      "Surface training t=3345, loss=0.0440511591732502\n",
      "Surface training t=3346, loss=0.04415741376578808\n",
      "Surface training t=3347, loss=0.046553364023566246\n",
      "Surface training t=3348, loss=0.04361311160027981\n",
      "Surface training t=3349, loss=0.043640853837132454\n",
      "Surface training t=3350, loss=0.04436166770756245\n",
      "Surface training t=3351, loss=0.0447849053889513\n",
      "Surface training t=3352, loss=0.04339948855340481\n",
      "Surface training t=3353, loss=0.04405304044485092\n",
      "Surface training t=3354, loss=0.04370134882628918\n",
      "Surface training t=3355, loss=0.04451150447130203\n",
      "Surface training t=3356, loss=0.04346981458365917\n",
      "Surface training t=3357, loss=0.044114941731095314\n",
      "Surface training t=3358, loss=0.04226202145218849\n",
      "Surface training t=3359, loss=0.04451166093349457\n",
      "Surface training t=3360, loss=0.042287128046154976\n",
      "Surface training t=3361, loss=0.04534253291785717\n",
      "Surface training t=3362, loss=0.04605397582054138\n",
      "Surface training t=3363, loss=0.04448488913476467\n",
      "Surface training t=3364, loss=0.043555350974202156\n",
      "Surface training t=3365, loss=0.04537849873304367\n",
      "Surface training t=3366, loss=0.042265305295586586\n",
      "Surface training t=3367, loss=0.04320036806166172\n",
      "Surface training t=3368, loss=0.044560497626662254\n",
      "Surface training t=3369, loss=0.04331324063241482\n",
      "Surface training t=3370, loss=0.04293611831963062\n",
      "Surface training t=3371, loss=0.042903484776616096\n",
      "Surface training t=3372, loss=0.04450889863073826\n",
      "Surface training t=3373, loss=0.0431278832256794\n",
      "Surface training t=3374, loss=0.04388052970170975\n",
      "Surface training t=3375, loss=0.043573761358857155\n",
      "Surface training t=3376, loss=0.04450954869389534\n",
      "Surface training t=3377, loss=0.0434249434620142\n",
      "Surface training t=3378, loss=0.04344196245074272\n",
      "Surface training t=3379, loss=0.043288493528962135\n",
      "Surface training t=3380, loss=0.042161623015999794\n",
      "Surface training t=3381, loss=0.04415976442396641\n",
      "Surface training t=3382, loss=0.044246308505535126\n",
      "Surface training t=3383, loss=0.043984049931168556\n",
      "Surface training t=3384, loss=0.04347905516624451\n",
      "Surface training t=3385, loss=0.04426000081002712\n",
      "Surface training t=3386, loss=0.04332929104566574\n",
      "Surface training t=3387, loss=0.04470341093838215\n",
      "Surface training t=3388, loss=0.04463505558669567\n",
      "Surface training t=3389, loss=0.042540449649095535\n",
      "Surface training t=3390, loss=0.04337674379348755\n",
      "Surface training t=3391, loss=0.043340783566236496\n",
      "Surface training t=3392, loss=0.0404844805598259\n",
      "Surface training t=3393, loss=0.04285103641450405\n",
      "Surface training t=3394, loss=0.04239601828157902\n",
      "Surface training t=3395, loss=0.04316315986216068\n",
      "Surface training t=3396, loss=0.042558081448078156\n",
      "Surface training t=3397, loss=0.04245171882212162\n",
      "Surface training t=3398, loss=0.044926274567842484\n",
      "Surface training t=3399, loss=0.043353209272027016\n",
      "Surface training t=3400, loss=0.04316168092191219\n",
      "Surface training t=3401, loss=0.04379640333354473\n",
      "Surface training t=3402, loss=0.04376945085823536\n",
      "Surface training t=3403, loss=0.042989179491996765\n",
      "Surface training t=3404, loss=0.04294915869832039\n",
      "Surface training t=3405, loss=0.04546074569225311\n",
      "Surface training t=3406, loss=0.045735493302345276\n",
      "Surface training t=3407, loss=0.04471241682767868\n",
      "Surface training t=3408, loss=0.043942805379629135\n",
      "Surface training t=3409, loss=0.04625308886170387\n",
      "Surface training t=3410, loss=0.04608053341507912\n",
      "Surface training t=3411, loss=0.04452446475625038\n",
      "Surface training t=3412, loss=0.042146025225520134\n",
      "Surface training t=3413, loss=0.04342718794941902\n",
      "Surface training t=3414, loss=0.04271558113396168\n",
      "Surface training t=3415, loss=0.04318655654788017\n",
      "Surface training t=3416, loss=0.042862387374043465\n",
      "Surface training t=3417, loss=0.043614476919174194\n",
      "Surface training t=3418, loss=0.043699659407138824\n",
      "Surface training t=3419, loss=0.042317602783441544\n",
      "Surface training t=3420, loss=0.04312194138765335\n",
      "Surface training t=3421, loss=0.043634023517370224\n",
      "Surface training t=3422, loss=0.04204307496547699\n",
      "Surface training t=3423, loss=0.04190564714372158\n",
      "Surface training t=3424, loss=0.04271026886999607\n",
      "Surface training t=3425, loss=0.04304267093539238\n",
      "Surface training t=3426, loss=0.043152278289198875\n",
      "Surface training t=3427, loss=0.044269414618611336\n",
      "Surface training t=3428, loss=0.04312668368220329\n",
      "Surface training t=3429, loss=0.042411284521222115\n",
      "Surface training t=3430, loss=0.044735776260495186\n",
      "Surface training t=3431, loss=0.043367164209485054\n",
      "Surface training t=3432, loss=0.042681626975536346\n",
      "Surface training t=3433, loss=0.04271182045340538\n",
      "Surface training t=3434, loss=0.04397181235253811\n",
      "Surface training t=3435, loss=0.04609196074306965\n",
      "Surface training t=3436, loss=0.044822774827480316\n",
      "Surface training t=3437, loss=0.04250023700296879\n",
      "Surface training t=3438, loss=0.0449556540697813\n",
      "Surface training t=3439, loss=0.04577854089438915\n",
      "Surface training t=3440, loss=0.043125322088599205\n",
      "Surface training t=3441, loss=0.043288497254252434\n",
      "Surface training t=3442, loss=0.045591989532113075\n",
      "Surface training t=3443, loss=0.04387907311320305\n",
      "Surface training t=3444, loss=0.04453980177640915\n",
      "Surface training t=3445, loss=0.04341468773782253\n",
      "Surface training t=3446, loss=0.0439606998115778\n",
      "Surface training t=3447, loss=0.042817844077944756\n",
      "Surface training t=3448, loss=0.04396628960967064\n",
      "Surface training t=3449, loss=0.04337806813418865\n",
      "Surface training t=3450, loss=0.04401645436882973\n",
      "Surface training t=3451, loss=0.04311455599963665\n",
      "Surface training t=3452, loss=0.04329913854598999\n",
      "Surface training t=3453, loss=0.04455006495118141\n",
      "Surface training t=3454, loss=0.0422329381108284\n",
      "Surface training t=3455, loss=0.04148765094578266\n",
      "Surface training t=3456, loss=0.0416883360594511\n",
      "Surface training t=3457, loss=0.043589117005467415\n",
      "Surface training t=3458, loss=0.04277420416474342\n",
      "Surface training t=3459, loss=0.042851055040955544\n",
      "Surface training t=3460, loss=0.04189007915556431\n",
      "Surface training t=3461, loss=0.04372926987707615\n",
      "Surface training t=3462, loss=0.044557755813002586\n",
      "Surface training t=3463, loss=0.042595893144607544\n",
      "Surface training t=3464, loss=0.04095216654241085\n",
      "Surface training t=3465, loss=0.043298427015542984\n",
      "Surface training t=3466, loss=0.04323318041861057\n",
      "Surface training t=3467, loss=0.043342193588614464\n",
      "Surface training t=3468, loss=0.04176533408463001\n",
      "Surface training t=3469, loss=0.04234151355922222\n",
      "Surface training t=3470, loss=0.0454692505300045\n",
      "Surface training t=3471, loss=0.044575056061148643\n",
      "Surface training t=3472, loss=0.04485832341015339\n",
      "Surface training t=3473, loss=0.0456094816327095\n",
      "Surface training t=3474, loss=0.043643275275826454\n",
      "Surface training t=3475, loss=0.043692950159311295\n",
      "Surface training t=3476, loss=0.043195826932787895\n",
      "Surface training t=3477, loss=0.04136868007481098\n",
      "Surface training t=3478, loss=0.04305979982018471\n",
      "Surface training t=3479, loss=0.043090565130114555\n",
      "Surface training t=3480, loss=0.041853513568639755\n",
      "Surface training t=3481, loss=0.04618746042251587\n",
      "Surface training t=3482, loss=0.04160986468195915\n",
      "Surface training t=3483, loss=0.04411125183105469\n",
      "Surface training t=3484, loss=0.04392378404736519\n",
      "Surface training t=3485, loss=0.0429182481020689\n",
      "Surface training t=3486, loss=0.042369209229946136\n",
      "Surface training t=3487, loss=0.043601591140031815\n",
      "Surface training t=3488, loss=0.041867852210998535\n",
      "Surface training t=3489, loss=0.04276546277105808\n",
      "Surface training t=3490, loss=0.043555716052651405\n",
      "Surface training t=3491, loss=0.04373598098754883\n",
      "Surface training t=3492, loss=0.04313044808804989\n",
      "Surface training t=3493, loss=0.04469779320061207\n",
      "Surface training t=3494, loss=0.04435260593891144\n",
      "Surface training t=3495, loss=0.04379983991384506\n",
      "Surface training t=3496, loss=0.04549113102257252\n",
      "Surface training t=3497, loss=0.04450972378253937\n",
      "Surface training t=3498, loss=0.04337581805884838\n",
      "Surface training t=3499, loss=0.04509906843304634\n",
      "Surface training t=3500, loss=0.0441846065223217\n",
      "Surface training t=3501, loss=0.04402310587465763\n",
      "Surface training t=3502, loss=0.04401019029319286\n",
      "Surface training t=3503, loss=0.04296746477484703\n",
      "Surface training t=3504, loss=0.04197463393211365\n",
      "Surface training t=3505, loss=0.04352954775094986\n",
      "Surface training t=3506, loss=0.042221954092383385\n",
      "Surface training t=3507, loss=0.04278931953012943\n",
      "Surface training t=3508, loss=0.04259723238646984\n",
      "Surface training t=3509, loss=0.043047817423939705\n",
      "Surface training t=3510, loss=0.044215891510248184\n",
      "Surface training t=3511, loss=0.043021177873015404\n",
      "Surface training t=3512, loss=0.043154943734407425\n",
      "Surface training t=3513, loss=0.044282130897045135\n",
      "Surface training t=3514, loss=0.04330948553979397\n",
      "Surface training t=3515, loss=0.044280728325247765\n",
      "Surface training t=3516, loss=0.04269321262836456\n",
      "Surface training t=3517, loss=0.04281173273921013\n",
      "Surface training t=3518, loss=0.04376862943172455\n",
      "Surface training t=3519, loss=0.04346677102148533\n",
      "Surface training t=3520, loss=0.04313449747860432\n",
      "Surface training t=3521, loss=0.044190892949700356\n",
      "Surface training t=3522, loss=0.04366549663245678\n",
      "Surface training t=3523, loss=0.04197040945291519\n",
      "Surface training t=3524, loss=0.04088667407631874\n",
      "Surface training t=3525, loss=0.04418996907770634\n",
      "Surface training t=3526, loss=0.042630983516573906\n",
      "Surface training t=3527, loss=0.04410332255065441\n",
      "Surface training t=3528, loss=0.045737745240330696\n",
      "Surface training t=3529, loss=0.04373395815491676\n",
      "Surface training t=3530, loss=0.04438167065382004\n",
      "Surface training t=3531, loss=0.04235522635281086\n",
      "Surface training t=3532, loss=0.04186157137155533\n",
      "Surface training t=3533, loss=0.042962370440363884\n",
      "Surface training t=3534, loss=0.04407022334635258\n",
      "Surface training t=3535, loss=0.04337274096906185\n",
      "Surface training t=3536, loss=0.0434467401355505\n",
      "Surface training t=3537, loss=0.04218229837715626\n",
      "Surface training t=3538, loss=0.04106254316866398\n",
      "Surface training t=3539, loss=0.04100489988923073\n",
      "Surface training t=3540, loss=0.04359917715191841\n",
      "Surface training t=3541, loss=0.04247877560555935\n",
      "Surface training t=3542, loss=0.04243596829473972\n",
      "Surface training t=3543, loss=0.044037939980626106\n",
      "Surface training t=3544, loss=0.04244602285325527\n",
      "Surface training t=3545, loss=0.042704684659838676\n",
      "Surface training t=3546, loss=0.041466379538178444\n",
      "Surface training t=3547, loss=0.04256126470863819\n",
      "Surface training t=3548, loss=0.0436361376196146\n",
      "Surface training t=3549, loss=0.0437674131244421\n",
      "Surface training t=3550, loss=0.04234126955270767\n",
      "Surface training t=3551, loss=0.043887149542570114\n",
      "Surface training t=3552, loss=0.04334842041134834\n",
      "Surface training t=3553, loss=0.042616069316864014\n",
      "Surface training t=3554, loss=0.0425808634608984\n",
      "Surface training t=3555, loss=0.04166868329048157\n",
      "Surface training t=3556, loss=0.04370524361729622\n",
      "Surface training t=3557, loss=0.04313280247151852\n",
      "Surface training t=3558, loss=0.04453933425247669\n",
      "Surface training t=3559, loss=0.04302062280476093\n",
      "Surface training t=3560, loss=0.04324367083609104\n",
      "Surface training t=3561, loss=0.04360675625503063\n",
      "Surface training t=3562, loss=0.043442586436867714\n",
      "Surface training t=3563, loss=0.044007591903209686\n",
      "Surface training t=3564, loss=0.044802017509937286\n",
      "Surface training t=3565, loss=0.04162628948688507\n",
      "Surface training t=3566, loss=0.04296479374170303\n",
      "Surface training t=3567, loss=0.042791564017534256\n",
      "Surface training t=3568, loss=0.04391236789524555\n",
      "Surface training t=3569, loss=0.04181407392024994\n",
      "Surface training t=3570, loss=0.04204375110566616\n",
      "Surface training t=3571, loss=0.04334777407348156\n",
      "Surface training t=3572, loss=0.043719660490751266\n",
      "Surface training t=3573, loss=0.042455900460481644\n",
      "Surface training t=3574, loss=0.04152497835457325\n",
      "Surface training t=3575, loss=0.041505735367536545\n",
      "Surface training t=3576, loss=0.0441781897097826\n",
      "Surface training t=3577, loss=0.04386277124285698\n",
      "Surface training t=3578, loss=0.044885873794555664\n",
      "Surface training t=3579, loss=0.04247056134045124\n",
      "Surface training t=3580, loss=0.04301396757364273\n",
      "Surface training t=3581, loss=0.04459575563669205\n",
      "Surface training t=3582, loss=0.041802726686000824\n",
      "Surface training t=3583, loss=0.04145641811192036\n",
      "Surface training t=3584, loss=0.0424820426851511\n",
      "Surface training t=3585, loss=0.04058277793228626\n",
      "Surface training t=3586, loss=0.042492445558309555\n",
      "Surface training t=3587, loss=0.04249047487974167\n",
      "Surface training t=3588, loss=0.04123448580503464\n",
      "Surface training t=3589, loss=0.041508475318551064\n",
      "Surface training t=3590, loss=0.043538397178053856\n",
      "Surface training t=3591, loss=0.041613584384322166\n",
      "Surface training t=3592, loss=0.042197635397315025\n",
      "Surface training t=3593, loss=0.04285421222448349\n",
      "Surface training t=3594, loss=0.04241361282765865\n",
      "Surface training t=3595, loss=0.041534673422575\n",
      "Surface training t=3596, loss=0.04131021164357662\n",
      "Surface training t=3597, loss=0.0430128239095211\n",
      "Surface training t=3598, loss=0.04354451783001423\n",
      "Surface training t=3599, loss=0.043986156582832336\n",
      "Surface training t=3600, loss=0.041747577488422394\n",
      "Surface training t=3601, loss=0.0433480329811573\n",
      "Surface training t=3602, loss=0.04283110797405243\n",
      "Surface training t=3603, loss=0.043219417333602905\n",
      "Surface training t=3604, loss=0.04239182360470295\n",
      "Surface training t=3605, loss=0.04374653846025467\n",
      "Surface training t=3606, loss=0.041910598054528236\n",
      "Surface training t=3607, loss=0.043639134615659714\n",
      "Surface training t=3608, loss=0.041328100487589836\n",
      "Surface training t=3609, loss=0.044684069231152534\n",
      "Surface training t=3610, loss=0.04168637841939926\n",
      "Surface training t=3611, loss=0.043073732405900955\n",
      "Surface training t=3612, loss=0.040750350803136826\n",
      "Surface training t=3613, loss=0.0416547954082489\n",
      "Surface training t=3614, loss=0.04166236333549023\n",
      "Surface training t=3615, loss=0.04125973396003246\n",
      "Surface training t=3616, loss=0.041751815006136894\n",
      "Surface training t=3617, loss=0.04314149729907513\n",
      "Surface training t=3618, loss=0.04301140084862709\n",
      "Surface training t=3619, loss=0.043718015775084496\n",
      "Surface training t=3620, loss=0.04495663568377495\n",
      "Surface training t=3621, loss=0.04287038370966911\n",
      "Surface training t=3622, loss=0.04325219988822937\n",
      "Surface training t=3623, loss=0.045563533902168274\n",
      "Surface training t=3624, loss=0.04743723012506962\n",
      "Surface training t=3625, loss=0.057878170162439346\n",
      "Surface training t=3626, loss=0.04902354069054127\n",
      "Surface training t=3627, loss=0.04289079084992409\n",
      "Surface training t=3628, loss=0.046506552025675774\n",
      "Surface training t=3629, loss=0.048656266182661057\n",
      "Surface training t=3630, loss=0.054654696956276894\n",
      "Surface training t=3631, loss=0.04987195320427418\n",
      "Surface training t=3632, loss=0.04730217903852463\n",
      "Surface training t=3633, loss=0.045867955312132835\n",
      "Surface training t=3634, loss=0.04837364889681339\n",
      "Surface training t=3635, loss=0.049658115953207016\n",
      "Surface training t=3636, loss=0.0504019521176815\n",
      "Surface training t=3637, loss=0.046920834109187126\n",
      "Surface training t=3638, loss=0.0460853036493063\n",
      "Surface training t=3639, loss=0.04841374047100544\n",
      "Surface training t=3640, loss=0.04865900240838528\n",
      "Surface training t=3641, loss=0.050427330657839775\n",
      "Surface training t=3642, loss=0.04805607162415981\n",
      "Surface training t=3643, loss=0.04672844335436821\n",
      "Surface training t=3644, loss=0.047686800360679626\n",
      "Surface training t=3645, loss=0.04754355549812317\n",
      "Surface training t=3646, loss=0.046620072796940804\n",
      "Surface training t=3647, loss=0.049500273540616035\n",
      "Surface training t=3648, loss=0.04629664868116379\n",
      "Surface training t=3649, loss=0.047529952600598335\n",
      "Surface training t=3650, loss=0.046470221132040024\n",
      "Surface training t=3651, loss=0.04659516364336014\n",
      "Surface training t=3652, loss=0.04833905212581158\n",
      "Surface training t=3653, loss=0.04485459811985493\n",
      "Surface training t=3654, loss=0.04560547508299351\n",
      "Surface training t=3655, loss=0.04779929295182228\n",
      "Surface training t=3656, loss=0.04919836483895779\n",
      "Surface training t=3657, loss=0.04965214431285858\n",
      "Surface training t=3658, loss=0.04521249607205391\n",
      "Surface training t=3659, loss=0.04350542090833187\n",
      "Surface training t=3660, loss=0.04545988887548447\n",
      "Surface training t=3661, loss=0.04630732350051403\n",
      "Surface training t=3662, loss=0.04574540629982948\n",
      "Surface training t=3663, loss=0.04543173685669899\n",
      "Surface training t=3664, loss=0.04636904411017895\n",
      "Surface training t=3665, loss=0.04796364903450012\n",
      "Surface training t=3666, loss=0.04941990412771702\n",
      "Surface training t=3667, loss=0.04937053471803665\n",
      "Surface training t=3668, loss=0.04630678705871105\n",
      "Surface training t=3669, loss=0.04549803212285042\n",
      "Surface training t=3670, loss=0.044100066646933556\n",
      "Surface training t=3671, loss=0.0441986545920372\n",
      "Surface training t=3672, loss=0.042808594182133675\n",
      "Surface training t=3673, loss=0.04264968819916248\n",
      "Surface training t=3674, loss=0.04387749172747135\n",
      "Surface training t=3675, loss=0.04187680035829544\n",
      "Surface training t=3676, loss=0.043394213542342186\n",
      "Surface training t=3677, loss=0.04376085847616196\n",
      "Surface training t=3678, loss=0.04023492895066738\n",
      "Surface training t=3679, loss=0.042084693908691406\n",
      "Surface training t=3680, loss=0.041054924950003624\n",
      "Surface training t=3681, loss=0.04101014882326126\n",
      "Surface training t=3682, loss=0.04327968321740627\n",
      "Surface training t=3683, loss=0.04138241708278656\n",
      "Surface training t=3684, loss=0.043309079483151436\n",
      "Surface training t=3685, loss=0.041856689378619194\n",
      "Surface training t=3686, loss=0.041023630648851395\n",
      "Surface training t=3687, loss=0.04115709476172924\n",
      "Surface training t=3688, loss=0.0412920955568552\n",
      "Surface training t=3689, loss=0.042291438207030296\n",
      "Surface training t=3690, loss=0.04084097594022751\n",
      "Surface training t=3691, loss=0.04174511507153511\n",
      "Surface training t=3692, loss=0.04216780140995979\n",
      "Surface training t=3693, loss=0.040982140228152275\n",
      "Surface training t=3694, loss=0.04076216369867325\n",
      "Surface training t=3695, loss=0.04155387356877327\n",
      "Surface training t=3696, loss=0.04048699326813221\n",
      "Surface training t=3697, loss=0.04290664941072464\n",
      "Surface training t=3698, loss=0.042807405814528465\n",
      "Surface training t=3699, loss=0.042161792516708374\n",
      "Surface training t=3700, loss=0.04281572252511978\n",
      "Surface training t=3701, loss=0.04340953379869461\n",
      "Surface training t=3702, loss=0.04269889555871487\n",
      "Surface training t=3703, loss=0.042801957577466965\n",
      "Surface training t=3704, loss=0.04107999429106712\n",
      "Surface training t=3705, loss=0.04171228967607021\n",
      "Surface training t=3706, loss=0.0441710539162159\n",
      "Surface training t=3707, loss=0.04175609536468983\n",
      "Surface training t=3708, loss=0.04117443598806858\n",
      "Surface training t=3709, loss=0.04210635460913181\n",
      "Surface training t=3710, loss=0.04208991676568985\n",
      "Surface training t=3711, loss=0.04234464466571808\n",
      "Surface training t=3712, loss=0.042905887588858604\n",
      "Surface training t=3713, loss=0.04232610575854778\n",
      "Surface training t=3714, loss=0.041023341938853264\n",
      "Surface training t=3715, loss=0.04153795540332794\n",
      "Surface training t=3716, loss=0.04137147217988968\n",
      "Surface training t=3717, loss=0.042152099311351776\n",
      "Surface training t=3718, loss=0.042102597653865814\n",
      "Surface training t=3719, loss=0.042579535394907\n",
      "Surface training t=3720, loss=0.0412211287766695\n",
      "Surface training t=3721, loss=0.043209463357925415\n",
      "Surface training t=3722, loss=0.041663758456707\n",
      "Surface training t=3723, loss=0.04117604158818722\n",
      "Surface training t=3724, loss=0.04093668423593044\n",
      "Surface training t=3725, loss=0.04012652859091759\n",
      "Surface training t=3726, loss=0.04244173690676689\n",
      "Surface training t=3727, loss=0.04442381113767624\n",
      "Surface training t=3728, loss=0.04062867909669876\n",
      "Surface training t=3729, loss=0.04252895712852478\n",
      "Surface training t=3730, loss=0.042357293888926506\n",
      "Surface training t=3731, loss=0.04078633897006512\n",
      "Surface training t=3732, loss=0.04085141234099865\n",
      "Surface training t=3733, loss=0.04238101840019226\n",
      "Surface training t=3734, loss=0.0414432268589735\n",
      "Surface training t=3735, loss=0.041243650019168854\n",
      "Surface training t=3736, loss=0.04216935113072395\n",
      "Surface training t=3737, loss=0.04211195185780525\n",
      "Surface training t=3738, loss=0.04135751165449619\n",
      "Surface training t=3739, loss=0.043888043612241745\n",
      "Surface training t=3740, loss=0.04329308494925499\n",
      "Surface training t=3741, loss=0.04245362617075443\n",
      "Surface training t=3742, loss=0.04262770712375641\n",
      "Surface training t=3743, loss=0.0415207426995039\n",
      "Surface training t=3744, loss=0.041486117988824844\n",
      "Surface training t=3745, loss=0.040725650265812874\n",
      "Surface training t=3746, loss=0.04071830213069916\n",
      "Surface training t=3747, loss=0.040681805461645126\n",
      "Surface training t=3748, loss=0.04136346094310284\n",
      "Surface training t=3749, loss=0.041946906596422195\n",
      "Surface training t=3750, loss=0.041402069851756096\n",
      "Surface training t=3751, loss=0.04091646522283554\n",
      "Surface training t=3752, loss=0.041675085201859474\n",
      "Surface training t=3753, loss=0.04069718159735203\n",
      "Surface training t=3754, loss=0.0419063288718462\n",
      "Surface training t=3755, loss=0.04250379651784897\n",
      "Surface training t=3756, loss=0.041835250332951546\n",
      "Surface training t=3757, loss=0.04190498776733875\n",
      "Surface training t=3758, loss=0.04132816009223461\n",
      "Surface training t=3759, loss=0.04064572975039482\n",
      "Surface training t=3760, loss=0.03929726779460907\n",
      "Surface training t=3761, loss=0.04157816991209984\n",
      "Surface training t=3762, loss=0.041751980781555176\n",
      "Surface training t=3763, loss=0.04210890457034111\n",
      "Surface training t=3764, loss=0.04005495645105839\n",
      "Surface training t=3765, loss=0.04193556681275368\n",
      "Surface training t=3766, loss=0.03993174806237221\n",
      "Surface training t=3767, loss=0.041657788679003716\n",
      "Surface training t=3768, loss=0.04159031808376312\n",
      "Surface training t=3769, loss=0.043044568970799446\n",
      "Surface training t=3770, loss=0.04159220680594444\n",
      "Surface training t=3771, loss=0.04189588315784931\n",
      "Surface training t=3772, loss=0.04097273759543896\n",
      "Surface training t=3773, loss=0.03989529609680176\n",
      "Surface training t=3774, loss=0.043610602617263794\n",
      "Surface training t=3775, loss=0.04024215042591095\n",
      "Surface training t=3776, loss=0.04224076680839062\n",
      "Surface training t=3777, loss=0.04174721986055374\n",
      "Surface training t=3778, loss=0.041215356439352036\n",
      "Surface training t=3779, loss=0.041337983682751656\n",
      "Surface training t=3780, loss=0.04123656824231148\n",
      "Surface training t=3781, loss=0.04114013537764549\n",
      "Surface training t=3782, loss=0.041454266756772995\n",
      "Surface training t=3783, loss=0.04043577052652836\n",
      "Surface training t=3784, loss=0.041203731670975685\n",
      "Surface training t=3785, loss=0.04263540543615818\n",
      "Surface training t=3786, loss=0.04233153350651264\n",
      "Surface training t=3787, loss=0.04270469211041927\n",
      "Surface training t=3788, loss=0.04199312627315521\n",
      "Surface training t=3789, loss=0.041224248707294464\n",
      "Surface training t=3790, loss=0.04265346936881542\n",
      "Surface training t=3791, loss=0.04226095601916313\n",
      "Surface training t=3792, loss=0.04206802882254124\n",
      "Surface training t=3793, loss=0.04217255860567093\n",
      "Surface training t=3794, loss=0.0423220656812191\n",
      "Surface training t=3795, loss=0.04134148731827736\n",
      "Surface training t=3796, loss=0.04267531633377075\n",
      "Surface training t=3797, loss=0.04146812669932842\n",
      "Surface training t=3798, loss=0.04124677926301956\n",
      "Surface training t=3799, loss=0.04058280400931835\n",
      "Surface training t=3800, loss=0.04283985495567322\n",
      "Surface training t=3801, loss=0.04071135260164738\n",
      "Surface training t=3802, loss=0.04241173528134823\n",
      "Surface training t=3803, loss=0.041512325406074524\n",
      "Surface training t=3804, loss=0.04174475744366646\n",
      "Surface training t=3805, loss=0.0408327542245388\n",
      "Surface training t=3806, loss=0.04267706163227558\n",
      "Surface training t=3807, loss=0.04216391406953335\n",
      "Surface training t=3808, loss=0.041208596900105476\n",
      "Surface training t=3809, loss=0.04030060954391956\n",
      "Surface training t=3810, loss=0.041940921917557716\n",
      "Surface training t=3811, loss=0.04287656955420971\n",
      "Surface training t=3812, loss=0.04197457805275917\n",
      "Surface training t=3813, loss=0.04225219413638115\n",
      "Surface training t=3814, loss=0.04204092361032963\n",
      "Surface training t=3815, loss=0.04284163936972618\n",
      "Surface training t=3816, loss=0.04222369007766247\n",
      "Surface training t=3817, loss=0.04256253503262997\n",
      "Surface training t=3818, loss=0.042635831981897354\n",
      "Surface training t=3819, loss=0.04236767254769802\n",
      "Surface training t=3820, loss=0.04212764650583267\n",
      "Surface training t=3821, loss=0.04281890392303467\n",
      "Surface training t=3822, loss=0.041334012523293495\n",
      "Surface training t=3823, loss=0.04068196378648281\n",
      "Surface training t=3824, loss=0.04019344039261341\n",
      "Surface training t=3825, loss=0.04116310551762581\n",
      "Surface training t=3826, loss=0.0405091866850853\n",
      "Surface training t=3827, loss=0.04109334200620651\n",
      "Surface training t=3828, loss=0.04203578270971775\n",
      "Surface training t=3829, loss=0.04085230082273483\n",
      "Surface training t=3830, loss=0.040606383234262466\n",
      "Surface training t=3831, loss=0.040776316076517105\n",
      "Surface training t=3832, loss=0.04124653339385986\n",
      "Surface training t=3833, loss=0.040833963081240654\n",
      "Surface training t=3834, loss=0.04435008391737938\n",
      "Surface training t=3835, loss=0.04316380247473717\n",
      "Surface training t=3836, loss=0.04329165071249008\n",
      "Surface training t=3837, loss=0.04245961643755436\n",
      "Surface training t=3838, loss=0.04335932619869709\n",
      "Surface training t=3839, loss=0.04145329259335995\n",
      "Surface training t=3840, loss=0.04187428951263428\n",
      "Surface training t=3841, loss=0.04035974107682705\n",
      "Surface training t=3842, loss=0.04172486439347267\n",
      "Surface training t=3843, loss=0.040940502658486366\n",
      "Surface training t=3844, loss=0.041770368814468384\n",
      "Surface training t=3845, loss=0.04078972898423672\n",
      "Surface training t=3846, loss=0.041684987023472786\n",
      "Surface training t=3847, loss=0.04172780364751816\n",
      "Surface training t=3848, loss=0.04101145267486572\n",
      "Surface training t=3849, loss=0.04183968901634216\n",
      "Surface training t=3850, loss=0.0418951828032732\n",
      "Surface training t=3851, loss=0.041462577879428864\n",
      "Surface training t=3852, loss=0.041212378069758415\n",
      "Surface training t=3853, loss=0.040992241352796555\n",
      "Surface training t=3854, loss=0.04240397736430168\n",
      "Surface training t=3855, loss=0.040261441841721535\n",
      "Surface training t=3856, loss=0.043328506872057915\n",
      "Surface training t=3857, loss=0.04099458456039429\n",
      "Surface training t=3858, loss=0.0432500671595335\n",
      "Surface training t=3859, loss=0.043707817792892456\n",
      "Surface training t=3860, loss=0.04359893873333931\n",
      "Surface training t=3861, loss=0.04552244953811169\n",
      "Surface training t=3862, loss=0.044914089143276215\n",
      "Surface training t=3863, loss=0.04585960879921913\n",
      "Surface training t=3864, loss=0.04632394760847092\n",
      "Surface training t=3865, loss=0.04472823068499565\n",
      "Surface training t=3866, loss=0.04395733028650284\n",
      "Surface training t=3867, loss=0.04261290840804577\n",
      "Surface training t=3868, loss=0.042838823050260544\n",
      "Surface training t=3869, loss=0.04366115294396877\n",
      "Surface training t=3870, loss=0.042567526921629906\n",
      "Surface training t=3871, loss=0.042570656165480614\n",
      "Surface training t=3872, loss=0.041664568707346916\n",
      "Surface training t=3873, loss=0.0417118389159441\n",
      "Surface training t=3874, loss=0.044047316536307335\n",
      "Surface training t=3875, loss=0.042632775381207466\n",
      "Surface training t=3876, loss=0.0391535647213459\n",
      "Surface training t=3877, loss=0.04110107198357582\n",
      "Surface training t=3878, loss=0.04244094528257847\n",
      "Surface training t=3879, loss=0.041085755452513695\n",
      "Surface training t=3880, loss=0.04274550452828407\n",
      "Surface training t=3881, loss=0.04089033417403698\n",
      "Surface training t=3882, loss=0.04234861023724079\n",
      "Surface training t=3883, loss=0.04267171584069729\n",
      "Surface training t=3884, loss=0.041192248463630676\n",
      "Surface training t=3885, loss=0.04314035549759865\n",
      "Surface training t=3886, loss=0.04301838018000126\n",
      "Surface training t=3887, loss=0.043305207043886185\n",
      "Surface training t=3888, loss=0.040824221447110176\n",
      "Surface training t=3889, loss=0.043307334184646606\n",
      "Surface training t=3890, loss=0.04223068244755268\n",
      "Surface training t=3891, loss=0.04203055799007416\n",
      "Surface training t=3892, loss=0.04273957945406437\n",
      "Surface training t=3893, loss=0.04040497913956642\n",
      "Surface training t=3894, loss=0.04376283846795559\n",
      "Surface training t=3895, loss=0.042321888729929924\n",
      "Surface training t=3896, loss=0.04165908508002758\n",
      "Surface training t=3897, loss=0.040817828848958015\n",
      "Surface training t=3898, loss=0.041300591081380844\n",
      "Surface training t=3899, loss=0.04020680487155914\n",
      "Surface training t=3900, loss=0.04050286486744881\n",
      "Surface training t=3901, loss=0.04188980907201767\n",
      "Surface training t=3902, loss=0.04149814508855343\n",
      "Surface training t=3903, loss=0.04222065582871437\n",
      "Surface training t=3904, loss=0.03932882659137249\n",
      "Surface training t=3905, loss=0.040843766182661057\n",
      "Surface training t=3906, loss=0.041811512783169746\n",
      "Surface training t=3907, loss=0.04109584353864193\n",
      "Surface training t=3908, loss=0.04087930545210838\n",
      "Surface training t=3909, loss=0.041998445987701416\n",
      "Surface training t=3910, loss=0.04183739051222801\n",
      "Surface training t=3911, loss=0.03883613832294941\n",
      "Surface training t=3912, loss=0.04105263575911522\n",
      "Surface training t=3913, loss=0.04157675430178642\n",
      "Surface training t=3914, loss=0.041538383811712265\n",
      "Surface training t=3915, loss=0.041457854211330414\n",
      "Surface training t=3916, loss=0.041854701936244965\n",
      "Surface training t=3917, loss=0.04213949292898178\n",
      "Surface training t=3918, loss=0.040373923256993294\n",
      "Surface training t=3919, loss=0.041692132130265236\n",
      "Surface training t=3920, loss=0.04101760499179363\n",
      "Surface training t=3921, loss=0.03960457444190979\n",
      "Surface training t=3922, loss=0.04041944444179535\n",
      "Surface training t=3923, loss=0.041082531213760376\n",
      "Surface training t=3924, loss=0.042016927152872086\n",
      "Surface training t=3925, loss=0.042260363698005676\n",
      "Surface training t=3926, loss=0.0428557563573122\n",
      "Surface training t=3927, loss=0.04055795259773731\n",
      "Surface training t=3928, loss=0.039958300068974495\n",
      "Surface training t=3929, loss=0.040293535217642784\n",
      "Surface training t=3930, loss=0.03875916264951229\n",
      "Surface training t=3931, loss=0.039990853518247604\n",
      "Surface training t=3932, loss=0.04295644722878933\n",
      "Surface training t=3933, loss=0.04051806777715683\n",
      "Surface training t=3934, loss=0.04172689467668533\n",
      "Surface training t=3935, loss=0.039895640686154366\n",
      "Surface training t=3936, loss=0.041307974606752396\n",
      "Surface training t=3937, loss=0.04128254950046539\n",
      "Surface training t=3938, loss=0.04175772704184055\n",
      "Surface training t=3939, loss=0.040660224854946136\n",
      "Surface training t=3940, loss=0.04091332666575909\n",
      "Surface training t=3941, loss=0.04157141596078873\n",
      "Surface training t=3942, loss=0.04252008721232414\n",
      "Surface training t=3943, loss=0.04048672318458557\n",
      "Surface training t=3944, loss=0.04035337641835213\n",
      "Surface training t=3945, loss=0.040848104283213615\n",
      "Surface training t=3946, loss=0.039805999025702477\n",
      "Surface training t=3947, loss=0.039752982556819916\n",
      "Surface training t=3948, loss=0.04196729138493538\n",
      "Surface training t=3949, loss=0.03895215503871441\n",
      "Surface training t=3950, loss=0.04081539250910282\n",
      "Surface training t=3951, loss=0.04005956090986729\n",
      "Surface training t=3952, loss=0.0394111592322588\n",
      "Surface training t=3953, loss=0.04125918447971344\n",
      "Surface training t=3954, loss=0.03965873643755913\n",
      "Surface training t=3955, loss=0.04079755023121834\n",
      "Surface training t=3956, loss=0.04183053784072399\n",
      "Surface training t=3957, loss=0.04062251187860966\n",
      "Surface training t=3958, loss=0.04257240332663059\n",
      "Surface training t=3959, loss=0.04099360853433609\n",
      "Surface training t=3960, loss=0.042331062257289886\n",
      "Surface training t=3961, loss=0.04210761934518814\n",
      "Surface training t=3962, loss=0.04004406929016113\n",
      "Surface training t=3963, loss=0.04177449084818363\n",
      "Surface training t=3964, loss=0.04007522203028202\n",
      "Surface training t=3965, loss=0.041320592164993286\n",
      "Surface training t=3966, loss=0.040316877886652946\n",
      "Surface training t=3967, loss=0.040963027626276016\n",
      "Surface training t=3968, loss=0.04105931520462036\n",
      "Surface training t=3969, loss=0.040501127019524574\n",
      "Surface training t=3970, loss=0.04079989157617092\n",
      "Surface training t=3971, loss=0.04177079536020756\n",
      "Surface training t=3972, loss=0.0408699344843626\n",
      "Surface training t=3973, loss=0.03953992389142513\n",
      "Surface training t=3974, loss=0.042378850281238556\n",
      "Surface training t=3975, loss=0.04087849147617817\n",
      "Surface training t=3976, loss=0.04137478396296501\n",
      "Surface training t=3977, loss=0.042307259514927864\n",
      "Surface training t=3978, loss=0.04150501638650894\n",
      "Surface training t=3979, loss=0.042028067633509636\n",
      "Surface training t=3980, loss=0.04212887957692146\n",
      "Surface training t=3981, loss=0.04102291539311409\n",
      "Surface training t=3982, loss=0.04234777390956879\n",
      "Surface training t=3983, loss=0.04334048740565777\n",
      "Surface training t=3984, loss=0.042810194194316864\n",
      "Surface training t=3985, loss=0.04077235609292984\n",
      "Surface training t=3986, loss=0.040508562698960304\n",
      "Surface training t=3987, loss=0.04063940979540348\n",
      "Surface training t=3988, loss=0.04168075509369373\n",
      "Surface training t=3989, loss=0.042403122410178185\n",
      "Surface training t=3990, loss=0.04106447100639343\n",
      "Surface training t=3991, loss=0.04072439670562744\n",
      "Surface training t=3992, loss=0.04140536114573479\n",
      "Surface training t=3993, loss=0.039580851793289185\n",
      "Surface training t=3994, loss=0.04003944806754589\n",
      "Surface training t=3995, loss=0.04148977994918823\n",
      "Surface training t=3996, loss=0.039344651624560356\n",
      "Surface training t=3997, loss=0.041931867599487305\n",
      "Surface training t=3998, loss=0.04164332523941994\n",
      "Surface training t=3999, loss=0.04283523932099342\n",
      "Surface training t=4000, loss=0.0415443517267704\n",
      "Surface training t=4001, loss=0.041320210322737694\n",
      "Surface training t=4002, loss=0.040681200101971626\n",
      "Surface training t=4003, loss=0.04049537517130375\n",
      "Surface training t=4004, loss=0.04041990637779236\n",
      "Surface training t=4005, loss=0.03942481614649296\n",
      "Surface training t=4006, loss=0.04027676396071911\n",
      "Surface training t=4007, loss=0.04058596305549145\n",
      "Surface training t=4008, loss=0.041231051087379456\n",
      "Surface training t=4009, loss=0.04102983511984348\n",
      "Surface training t=4010, loss=0.041351230815052986\n",
      "Surface training t=4011, loss=0.04031903110444546\n",
      "Surface training t=4012, loss=0.04219493828713894\n",
      "Surface training t=4013, loss=0.041545506566762924\n",
      "Surface training t=4014, loss=0.041095614433288574\n",
      "Surface training t=4015, loss=0.04052634723484516\n",
      "Surface training t=4016, loss=0.04181896895170212\n",
      "Surface training t=4017, loss=0.04188256897032261\n",
      "Surface training t=4018, loss=0.04181201010942459\n",
      "Surface training t=4019, loss=0.03917576186358929\n",
      "Surface training t=4020, loss=0.04062896780669689\n",
      "Surface training t=4021, loss=0.041646627709269524\n",
      "Surface training t=4022, loss=0.039394840598106384\n",
      "Surface training t=4023, loss=0.04113282077014446\n",
      "Surface training t=4024, loss=0.039350664243102074\n",
      "Surface training t=4025, loss=0.04036225192248821\n",
      "Surface training t=4026, loss=0.040290504693984985\n",
      "Surface training t=4027, loss=0.040546685457229614\n",
      "Surface training t=4028, loss=0.040830451995134354\n",
      "Surface training t=4029, loss=0.04042085073888302\n",
      "Surface training t=4030, loss=0.03941722586750984\n",
      "Surface training t=4031, loss=0.04190518707036972\n",
      "Surface training t=4032, loss=0.04084699973464012\n",
      "Surface training t=4033, loss=0.04114348068833351\n",
      "Surface training t=4034, loss=0.041044676676392555\n",
      "Surface training t=4035, loss=0.0414064172655344\n",
      "Surface training t=4036, loss=0.03957747109234333\n",
      "Surface training t=4037, loss=0.039268823340535164\n",
      "Surface training t=4038, loss=0.03994641825556755\n",
      "Surface training t=4039, loss=0.04144809581339359\n",
      "Surface training t=4040, loss=0.03959137760102749\n",
      "Surface training t=4041, loss=0.03934207744896412\n",
      "Surface training t=4042, loss=0.04105520807206631\n",
      "Surface training t=4043, loss=0.040367649868130684\n",
      "Surface training t=4044, loss=0.039551226422190666\n",
      "Surface training t=4045, loss=0.04005739279091358\n",
      "Surface training t=4046, loss=0.039581477642059326\n",
      "Surface training t=4047, loss=0.04042857699096203\n",
      "Surface training t=4048, loss=0.03945058211684227\n",
      "Surface training t=4049, loss=0.03919929824769497\n",
      "Surface training t=4050, loss=0.03919341415166855\n",
      "Surface training t=4051, loss=0.040550656616687775\n",
      "Surface training t=4052, loss=0.04038572870194912\n",
      "Surface training t=4053, loss=0.042433200404047966\n",
      "Surface training t=4054, loss=0.04307079128921032\n",
      "Surface training t=4055, loss=0.039909953251481056\n",
      "Surface training t=4056, loss=0.04082127660512924\n",
      "Surface training t=4057, loss=0.03944915719330311\n",
      "Surface training t=4058, loss=0.04229300282895565\n",
      "Surface training t=4059, loss=0.039085252210497856\n",
      "Surface training t=4060, loss=0.04191773943603039\n",
      "Surface training t=4061, loss=0.04252628609538078\n",
      "Surface training t=4062, loss=0.04386058263480663\n",
      "Surface training t=4063, loss=0.042399682104587555\n",
      "Surface training t=4064, loss=0.04117266647517681\n",
      "Surface training t=4065, loss=0.04103630408644676\n",
      "Surface training t=4066, loss=0.04252306930720806\n",
      "Surface training t=4067, loss=0.04204230196774006\n",
      "Surface training t=4068, loss=0.040739964693784714\n",
      "Surface training t=4069, loss=0.04250569827854633\n",
      "Surface training t=4070, loss=0.04654168151319027\n",
      "Surface training t=4071, loss=0.04469771310687065\n",
      "Surface training t=4072, loss=0.050662023946642876\n",
      "Surface training t=4073, loss=0.04785958305001259\n",
      "Surface training t=4074, loss=0.04595826007425785\n",
      "Surface training t=4075, loss=0.04259893111884594\n",
      "Surface training t=4076, loss=0.040652044117450714\n",
      "Surface training t=4077, loss=0.042103828862309456\n",
      "Surface training t=4078, loss=0.041566675528883934\n",
      "Surface training t=4079, loss=0.040895745158195496\n",
      "Surface training t=4080, loss=0.04176967032253742\n",
      "Surface training t=4081, loss=0.042054308578372\n",
      "Surface training t=4082, loss=0.04096507094800472\n",
      "Surface training t=4083, loss=0.04156645946204662\n",
      "Surface training t=4084, loss=0.04102358780801296\n",
      "Surface training t=4085, loss=0.04067826457321644\n",
      "Surface training t=4086, loss=0.04154542274773121\n",
      "Surface training t=4087, loss=0.04150768183171749\n",
      "Surface training t=4088, loss=0.042117562144994736\n",
      "Surface training t=4089, loss=0.042582063004374504\n",
      "Surface training t=4090, loss=0.04094775207340717\n",
      "Surface training t=4091, loss=0.03968144580721855\n",
      "Surface training t=4092, loss=0.041446613147854805\n",
      "Surface training t=4093, loss=0.03877407871186733\n",
      "Surface training t=4094, loss=0.0406075045466423\n",
      "Surface training t=4095, loss=0.0412325169891119\n",
      "Surface training t=4096, loss=0.03789696842432022\n",
      "Surface training t=4097, loss=0.039537759497761726\n",
      "Surface training t=4098, loss=0.0395986195653677\n",
      "Surface training t=4099, loss=0.04172947816550732\n",
      "Surface training t=4100, loss=0.039811450988054276\n",
      "Surface training t=4101, loss=0.040592582896351814\n",
      "Surface training t=4102, loss=0.03992294520139694\n",
      "Surface training t=4103, loss=0.03910250589251518\n",
      "Surface training t=4104, loss=0.04258831404149532\n",
      "Surface training t=4105, loss=0.04043977148830891\n",
      "Surface training t=4106, loss=0.04052645154297352\n",
      "Surface training t=4107, loss=0.03921920247375965\n",
      "Surface training t=4108, loss=0.03970927745103836\n",
      "Surface training t=4109, loss=0.04164659418165684\n",
      "Surface training t=4110, loss=0.0396091528236866\n",
      "Surface training t=4111, loss=0.03977685607969761\n",
      "Surface training t=4112, loss=0.0400167815387249\n",
      "Surface training t=4113, loss=0.03931874595582485\n",
      "Surface training t=4114, loss=0.03961899317800999\n",
      "Surface training t=4115, loss=0.04112113453447819\n",
      "Surface training t=4116, loss=0.039478158578276634\n",
      "Surface training t=4117, loss=0.038662245497107506\n",
      "Surface training t=4118, loss=0.039673708379268646\n",
      "Surface training t=4119, loss=0.04017142206430435\n",
      "Surface training t=4120, loss=0.04065982811152935\n",
      "Surface training t=4121, loss=0.04265611805021763\n",
      "Surface training t=4122, loss=0.04513408616185188\n",
      "Surface training t=4123, loss=0.049013981595635414\n",
      "Surface training t=4124, loss=0.04810360446572304\n",
      "Surface training t=4125, loss=0.04736636020243168\n",
      "Surface training t=4126, loss=0.04576273076236248\n",
      "Surface training t=4127, loss=0.04483115114271641\n",
      "Surface training t=4128, loss=0.044890930876135826\n",
      "Surface training t=4129, loss=0.04572713375091553\n",
      "Surface training t=4130, loss=0.04681720957159996\n",
      "Surface training t=4131, loss=0.04326442815363407\n",
      "Surface training t=4132, loss=0.045912303030490875\n",
      "Surface training t=4133, loss=0.04507673904299736\n",
      "Surface training t=4134, loss=0.04644842632114887\n",
      "Surface training t=4135, loss=0.04580174572765827\n",
      "Surface training t=4136, loss=0.04250616393983364\n",
      "Surface training t=4137, loss=0.0442215371876955\n",
      "Surface training t=4138, loss=0.04465744458138943\n",
      "Surface training t=4139, loss=0.04656767472624779\n",
      "Surface training t=4140, loss=0.04629780910909176\n",
      "Surface training t=4141, loss=0.04402799345552921\n",
      "Surface training t=4142, loss=0.04406886734068394\n",
      "Surface training t=4143, loss=0.04662369191646576\n",
      "Surface training t=4144, loss=0.0455307774245739\n",
      "Surface training t=4145, loss=0.04537917301058769\n",
      "Surface training t=4146, loss=0.04400709457695484\n",
      "Surface training t=4147, loss=0.042870353907346725\n",
      "Surface training t=4148, loss=0.04354478046298027\n",
      "Surface training t=4149, loss=0.04355943575501442\n",
      "Surface training t=4150, loss=0.04306088574230671\n",
      "Surface training t=4151, loss=0.044122012332081795\n",
      "Surface training t=4152, loss=0.04531284421682358\n",
      "Surface training t=4153, loss=0.04344886541366577\n",
      "Surface training t=4154, loss=0.04376593977212906\n",
      "Surface training t=4155, loss=0.04332469776272774\n",
      "Surface training t=4156, loss=0.04581991769373417\n",
      "Surface training t=4157, loss=0.04411747492849827\n",
      "Surface training t=4158, loss=0.04467817209661007\n",
      "Surface training t=4159, loss=0.04469019174575806\n",
      "Surface training t=4160, loss=0.04411313682794571\n",
      "Surface training t=4161, loss=0.043594641610980034\n",
      "Surface training t=4162, loss=0.04456778056919575\n",
      "Surface training t=4163, loss=0.043777912855148315\n",
      "Surface training t=4164, loss=0.044826582074165344\n",
      "Surface training t=4165, loss=0.04382333159446716\n",
      "Surface training t=4166, loss=0.04275928996503353\n",
      "Surface training t=4167, loss=0.04318384453654289\n",
      "Surface training t=4168, loss=0.04289613477885723\n",
      "Surface training t=4169, loss=0.04287687875330448\n",
      "Surface training t=4170, loss=0.04396831430494785\n",
      "Surface training t=4171, loss=0.046068090945482254\n",
      "Surface training t=4172, loss=0.043708840385079384\n",
      "Surface training t=4173, loss=0.04300810024142265\n",
      "Surface training t=4174, loss=0.04239121451973915\n",
      "Surface training t=4175, loss=0.04041086882352829\n",
      "Surface training t=4176, loss=0.04187268204987049\n",
      "Surface training t=4177, loss=0.04154406860470772\n",
      "Surface training t=4178, loss=0.04141727089881897\n",
      "Surface training t=4179, loss=0.04062015190720558\n",
      "Surface training t=4180, loss=0.04155348427593708\n",
      "Surface training t=4181, loss=0.03989604115486145\n",
      "Surface training t=4182, loss=0.040429433807730675\n",
      "Surface training t=4183, loss=0.038895197212696075\n",
      "Surface training t=4184, loss=0.04151228256523609\n",
      "Surface training t=4185, loss=0.040147991850972176\n",
      "Surface training t=4186, loss=0.039761021733284\n",
      "Surface training t=4187, loss=0.039088306948542595\n",
      "Surface training t=4188, loss=0.03943483158946037\n",
      "Surface training t=4189, loss=0.03965207003057003\n",
      "Surface training t=4190, loss=0.039192091673612595\n",
      "Surface training t=4191, loss=0.03792236000299454\n",
      "Surface training t=4192, loss=0.03943066671490669\n",
      "Surface training t=4193, loss=0.03874434530735016\n",
      "Surface training t=4194, loss=0.03977525793015957\n",
      "Surface training t=4195, loss=0.040304481983184814\n",
      "Surface training t=4196, loss=0.039086176082491875\n",
      "Surface training t=4197, loss=0.038630831986665726\n",
      "Surface training t=4198, loss=0.03945104777812958\n",
      "Surface training t=4199, loss=0.03886416181921959\n",
      "Surface training t=4200, loss=0.03946316987276077\n",
      "Surface training t=4201, loss=0.03838333301246166\n",
      "Surface training t=4202, loss=0.039519213140010834\n",
      "Surface training t=4203, loss=0.03826930373907089\n",
      "Surface training t=4204, loss=0.03816782496869564\n",
      "Surface training t=4205, loss=0.03899315930902958\n",
      "Surface training t=4206, loss=0.038629695773124695\n",
      "Surface training t=4207, loss=0.038376882672309875\n",
      "Surface training t=4208, loss=0.03917794115841389\n",
      "Surface training t=4209, loss=0.039124855771660805\n",
      "Surface training t=4210, loss=0.03909347765147686\n",
      "Surface training t=4211, loss=0.04026193730533123\n",
      "Surface training t=4212, loss=0.03863540291786194\n",
      "Surface training t=4213, loss=0.03962703421711922\n",
      "Surface training t=4214, loss=0.03944734297692776\n",
      "Surface training t=4215, loss=0.039546363055706024\n",
      "Surface training t=4216, loss=0.039772119373083115\n",
      "Surface training t=4217, loss=0.03884994238615036\n",
      "Surface training t=4218, loss=0.038266709074378014\n",
      "Surface training t=4219, loss=0.03899758495390415\n",
      "Surface training t=4220, loss=0.036822929978370667\n",
      "Surface training t=4221, loss=0.03845423460006714\n",
      "Surface training t=4222, loss=0.038955217227339745\n",
      "Surface training t=4223, loss=0.03886842541396618\n",
      "Surface training t=4224, loss=0.039268821477890015\n",
      "Surface training t=4225, loss=0.0393137913197279\n",
      "Surface training t=4226, loss=0.038699569180607796\n",
      "Surface training t=4227, loss=0.03978563845157623\n",
      "Surface training t=4228, loss=0.03891225345432758\n",
      "Surface training t=4229, loss=0.036991676315665245\n",
      "Surface training t=4230, loss=0.039685288444161415\n",
      "Surface training t=4231, loss=0.0383390448987484\n",
      "Surface training t=4232, loss=0.03900652751326561\n",
      "Surface training t=4233, loss=0.03846622072160244\n",
      "Surface training t=4234, loss=0.038613688200712204\n",
      "Surface training t=4235, loss=0.03867926262319088\n",
      "Surface training t=4236, loss=0.03846130333840847\n",
      "Surface training t=4237, loss=0.03839743696153164\n",
      "Surface training t=4238, loss=0.037904758006334305\n",
      "Surface training t=4239, loss=0.037672266364097595\n",
      "Surface training t=4240, loss=0.03695579804480076\n",
      "Surface training t=4241, loss=0.04046373814344406\n",
      "Surface training t=4242, loss=0.03849417343735695\n",
      "Surface training t=4243, loss=0.03871021792292595\n",
      "Surface training t=4244, loss=0.0385166872292757\n",
      "Surface training t=4245, loss=0.03924276493489742\n",
      "Surface training t=4246, loss=0.038474585860967636\n",
      "Surface training t=4247, loss=0.03834634646773338\n",
      "Surface training t=4248, loss=0.04045536182820797\n",
      "Surface training t=4249, loss=0.03871110826730728\n",
      "Surface training t=4250, loss=0.03844579681754112\n",
      "Surface training t=4251, loss=0.03973183035850525\n",
      "Surface training t=4252, loss=0.03902269899845123\n",
      "Surface training t=4253, loss=0.038211364299058914\n",
      "Surface training t=4254, loss=0.039547862485051155\n",
      "Surface training t=4255, loss=0.04021657072007656\n",
      "Surface training t=4256, loss=0.039492933079600334\n",
      "Surface training t=4257, loss=0.03943506255745888\n",
      "Surface training t=4258, loss=0.03987838327884674\n",
      "Surface training t=4259, loss=0.03856688179075718\n",
      "Surface training t=4260, loss=0.03783074952661991\n",
      "Surface training t=4261, loss=0.03884590603411198\n",
      "Surface training t=4262, loss=0.0381249263882637\n",
      "Surface training t=4263, loss=0.03894647769629955\n",
      "Surface training t=4264, loss=0.039962273091077805\n",
      "Surface training t=4265, loss=0.03744218684732914\n",
      "Surface training t=4266, loss=0.03895268402993679\n",
      "Surface training t=4267, loss=0.039566194638609886\n",
      "Surface training t=4268, loss=0.03971250168979168\n",
      "Surface training t=4269, loss=0.0383074339479208\n",
      "Surface training t=4270, loss=0.03844107501208782\n",
      "Surface training t=4271, loss=0.03926267847418785\n",
      "Surface training t=4272, loss=0.038549141958355904\n",
      "Surface training t=4273, loss=0.03897668980062008\n",
      "Surface training t=4274, loss=0.03918846510350704\n",
      "Surface training t=4275, loss=0.0397917702794075\n",
      "Surface training t=4276, loss=0.04006985016167164\n",
      "Surface training t=4277, loss=0.03920369781553745\n",
      "Surface training t=4278, loss=0.03948645293712616\n",
      "Surface training t=4279, loss=0.03774471767246723\n",
      "Surface training t=4280, loss=0.03988215513527393\n",
      "Surface training t=4281, loss=0.040002044290304184\n",
      "Surface training t=4282, loss=0.03894392400979996\n",
      "Surface training t=4283, loss=0.03879781626164913\n",
      "Surface training t=4284, loss=0.03890501707792282\n",
      "Surface training t=4285, loss=0.039902543649077415\n",
      "Surface training t=4286, loss=0.040499452501535416\n",
      "Surface training t=4287, loss=0.03878949210047722\n",
      "Surface training t=4288, loss=0.037846606224775314\n",
      "Surface training t=4289, loss=0.03936773166060448\n",
      "Surface training t=4290, loss=0.038479363545775414\n",
      "Surface training t=4291, loss=0.039481960237026215\n",
      "Surface training t=4292, loss=0.04025670140981674\n",
      "Surface training t=4293, loss=0.039617666974663734\n",
      "Surface training t=4294, loss=0.0382615365087986\n",
      "Surface training t=4295, loss=0.039949290454387665\n",
      "Surface training t=4296, loss=0.03847861848771572\n",
      "Surface training t=4297, loss=0.037867721170186996\n",
      "Surface training t=4298, loss=0.03810092806816101\n",
      "Surface training t=4299, loss=0.039050498977303505\n",
      "Surface training t=4300, loss=0.03826534003019333\n",
      "Surface training t=4301, loss=0.039531346410512924\n",
      "Surface training t=4302, loss=0.039199208840727806\n",
      "Surface training t=4303, loss=0.03811190277338028\n",
      "Surface training t=4304, loss=0.03884540684521198\n",
      "Surface training t=4305, loss=0.03944570757448673\n",
      "Surface training t=4306, loss=0.03838639333844185\n",
      "Surface training t=4307, loss=0.03923527151346207\n",
      "Surface training t=4308, loss=0.037314217537641525\n",
      "Surface training t=4309, loss=0.039974696934223175\n",
      "Surface training t=4310, loss=0.04080670885741711\n",
      "Surface training t=4311, loss=0.03908814862370491\n",
      "Surface training t=4312, loss=0.039091844111680984\n",
      "Surface training t=4313, loss=0.041598813608288765\n",
      "Surface training t=4314, loss=0.045733917504549026\n",
      "Surface training t=4315, loss=0.049795275554060936\n",
      "Surface training t=4316, loss=0.04287898913025856\n",
      "Surface training t=4317, loss=0.0412985123693943\n",
      "Surface training t=4318, loss=0.0425246711820364\n",
      "Surface training t=4319, loss=0.046477723866701126\n",
      "Surface training t=4320, loss=0.04712195321917534\n",
      "Surface training t=4321, loss=0.04358695633709431\n",
      "Surface training t=4322, loss=0.043763747438788414\n",
      "Surface training t=4323, loss=0.04485239088535309\n",
      "Surface training t=4324, loss=0.04269992373883724\n",
      "Surface training t=4325, loss=0.041169263422489166\n",
      "Surface training t=4326, loss=0.039673078805208206\n",
      "Surface training t=4327, loss=0.04062245599925518\n",
      "Surface training t=4328, loss=0.03989109769463539\n",
      "Surface training t=4329, loss=0.04006274417042732\n",
      "Surface training t=4330, loss=0.0378880500793457\n",
      "Surface training t=4331, loss=0.03922050632536411\n",
      "Surface training t=4332, loss=0.03801439329981804\n",
      "Surface training t=4333, loss=0.037625182420015335\n",
      "Surface training t=4334, loss=0.03730248473584652\n",
      "Surface training t=4335, loss=0.03869118541479111\n",
      "Surface training t=4336, loss=0.0388084277510643\n",
      "Surface training t=4337, loss=0.038505857810378075\n",
      "Surface training t=4338, loss=0.03822599537670612\n",
      "Surface training t=4339, loss=0.039168089628219604\n",
      "Surface training t=4340, loss=0.03722110390663147\n",
      "Surface training t=4341, loss=0.03999663516879082\n",
      "Surface training t=4342, loss=0.04083821550011635\n",
      "Surface training t=4343, loss=0.04093235358595848\n",
      "Surface training t=4344, loss=0.037566324695944786\n",
      "Surface training t=4345, loss=0.038632508367300034\n",
      "Surface training t=4346, loss=0.0389449056237936\n",
      "Surface training t=4347, loss=0.039178917184472084\n",
      "Surface training t=4348, loss=0.037069424986839294\n",
      "Surface training t=4349, loss=0.03863857313990593\n",
      "Surface training t=4350, loss=0.03898352012038231\n",
      "Surface training t=4351, loss=0.03832332417368889\n",
      "Surface training t=4352, loss=0.03800025396049023\n",
      "Surface training t=4353, loss=0.03785213828086853\n",
      "Surface training t=4354, loss=0.038673341274261475\n",
      "Surface training t=4355, loss=0.04064783453941345\n",
      "Surface training t=4356, loss=0.040294673293828964\n",
      "Surface training t=4357, loss=0.03762802481651306\n",
      "Surface training t=4358, loss=0.03916710615158081\n",
      "Surface training t=4359, loss=0.03793920390307903\n",
      "Surface training t=4360, loss=0.0369078665971756\n",
      "Surface training t=4361, loss=0.038720592856407166\n",
      "Surface training t=4362, loss=0.037534430623054504\n",
      "Surface training t=4363, loss=0.039568519219756126\n",
      "Surface training t=4364, loss=0.03703983686864376\n",
      "Surface training t=4365, loss=0.03829612955451012\n",
      "Surface training t=4366, loss=0.038566384464502335\n",
      "Surface training t=4367, loss=0.037334030494093895\n",
      "Surface training t=4368, loss=0.03829311765730381\n",
      "Surface training t=4369, loss=0.03731343895196915\n",
      "Surface training t=4370, loss=0.03883013501763344\n",
      "Surface training t=4371, loss=0.03917723335325718\n",
      "Surface training t=4372, loss=0.0369943231344223\n",
      "Surface training t=4373, loss=0.03988439403474331\n",
      "Surface training t=4374, loss=0.03650044836103916\n",
      "Surface training t=4375, loss=0.03725520521402359\n",
      "Surface training t=4376, loss=0.03845963440835476\n",
      "Surface training t=4377, loss=0.03734812140464783\n",
      "Surface training t=4378, loss=0.04079359211027622\n",
      "Surface training t=4379, loss=0.04165505804121494\n",
      "Surface training t=4380, loss=0.04174207150936127\n",
      "Surface training t=4381, loss=0.04363363981246948\n",
      "Surface training t=4382, loss=0.04454565793275833\n",
      "Surface training t=4383, loss=0.04487820528447628\n",
      "Surface training t=4384, loss=0.0433393195271492\n",
      "Surface training t=4385, loss=0.04196067154407501\n",
      "Surface training t=4386, loss=0.041972870007157326\n",
      "Surface training t=4387, loss=0.042451657354831696\n",
      "Surface training t=4388, loss=0.04440085031092167\n",
      "Surface training t=4389, loss=0.044363586232066154\n",
      "Surface training t=4390, loss=0.04418385401368141\n",
      "Surface training t=4391, loss=0.04108045995235443\n",
      "Surface training t=4392, loss=0.04139803722500801\n",
      "Surface training t=4393, loss=0.04367265850305557\n",
      "Surface training t=4394, loss=0.04457883723080158\n",
      "Surface training t=4395, loss=0.04367859847843647\n",
      "Surface training t=4396, loss=0.04410548694431782\n",
      "Surface training t=4397, loss=0.04164804518222809\n",
      "Surface training t=4398, loss=0.04162403382360935\n",
      "Surface training t=4399, loss=0.04398895986378193\n",
      "Surface training t=4400, loss=0.04403551667928696\n",
      "Surface training t=4401, loss=0.04229535162448883\n",
      "Surface training t=4402, loss=0.04133227653801441\n",
      "Surface training t=4403, loss=0.041025497019290924\n",
      "Surface training t=4404, loss=0.03989541716873646\n",
      "Surface training t=4405, loss=0.04016014188528061\n",
      "Surface training t=4406, loss=0.039440734311938286\n",
      "Surface training t=4407, loss=0.03931877017021179\n",
      "Surface training t=4408, loss=0.038702815771102905\n",
      "Surface training t=4409, loss=0.03824364021420479\n",
      "Surface training t=4410, loss=0.03749421052634716\n",
      "Surface training t=4411, loss=0.03834610991179943\n",
      "Surface training t=4412, loss=0.039783937856554985\n",
      "Surface training t=4413, loss=0.03868342749774456\n",
      "Surface training t=4414, loss=0.03961198776960373\n",
      "Surface training t=4415, loss=0.03915238752961159\n",
      "Surface training t=4416, loss=0.03892786428332329\n",
      "Surface training t=4417, loss=0.03923314996063709\n",
      "Surface training t=4418, loss=0.038511140272021294\n",
      "Surface training t=4419, loss=0.03712788596749306\n",
      "Surface training t=4420, loss=0.039988432079553604\n",
      "Surface training t=4421, loss=0.03809473104774952\n",
      "Surface training t=4422, loss=0.03729618526995182\n",
      "Surface training t=4423, loss=0.03888673335313797\n",
      "Surface training t=4424, loss=0.03896002098917961\n",
      "Surface training t=4425, loss=0.03834282420575619\n",
      "Surface training t=4426, loss=0.037956805899739265\n",
      "Surface training t=4427, loss=0.03862382099032402\n",
      "Surface training t=4428, loss=0.03788276948034763\n",
      "Surface training t=4429, loss=0.036769432947039604\n",
      "Surface training t=4430, loss=0.03768194653093815\n",
      "Surface training t=4431, loss=0.037232210859656334\n",
      "Surface training t=4432, loss=0.038142235949635506\n",
      "Surface training t=4433, loss=0.03866139426827431\n",
      "Surface training t=4434, loss=0.038900939747691154\n",
      "Surface training t=4435, loss=0.03883018344640732\n",
      "Surface training t=4436, loss=0.03817538358271122\n",
      "Surface training t=4437, loss=0.03777134418487549\n",
      "Surface training t=4438, loss=0.03870974853634834\n",
      "Surface training t=4439, loss=0.040462426841259\n",
      "Surface training t=4440, loss=0.037052374333143234\n",
      "Surface training t=4441, loss=0.03825012221932411\n",
      "Surface training t=4442, loss=0.03607115522027016\n",
      "Surface training t=4443, loss=0.03656616248190403\n",
      "Surface training t=4444, loss=0.037186142057180405\n",
      "Surface training t=4445, loss=0.03806842491030693\n",
      "Surface training t=4446, loss=0.03850499354302883\n",
      "Surface training t=4447, loss=0.03908173739910126\n",
      "Surface training t=4448, loss=0.03819833695888519\n",
      "Surface training t=4449, loss=0.03852226585149765\n",
      "Surface training t=4450, loss=0.039775172248482704\n",
      "Surface training t=4451, loss=0.041549479588866234\n",
      "Surface training t=4452, loss=0.040099479258060455\n",
      "Surface training t=4453, loss=0.04314848408102989\n",
      "Surface training t=4454, loss=0.04553517699241638\n",
      "Surface training t=4455, loss=0.043933453038334846\n",
      "Surface training t=4456, loss=0.04244218394160271\n",
      "Surface training t=4457, loss=0.04109262488782406\n",
      "Surface training t=4458, loss=0.04162643477320671\n",
      "Surface training t=4459, loss=0.042691636830568314\n",
      "Surface training t=4460, loss=0.04516822285950184\n",
      "Surface training t=4461, loss=0.04327377863228321\n",
      "Surface training t=4462, loss=0.04121031053364277\n",
      "Surface training t=4463, loss=0.04206857085227966\n",
      "Surface training t=4464, loss=0.04124927893280983\n",
      "Surface training t=4465, loss=0.04238726571202278\n",
      "Surface training t=4466, loss=0.04240529611706734\n",
      "Surface training t=4467, loss=0.04321853257715702\n",
      "Surface training t=4468, loss=0.04149962216615677\n",
      "Surface training t=4469, loss=0.03958006016910076\n",
      "Surface training t=4470, loss=0.04027314484119415\n",
      "Surface training t=4471, loss=0.04138418100774288\n",
      "Surface training t=4472, loss=0.04376126639544964\n",
      "Surface training t=4473, loss=0.04254389926791191\n",
      "Surface training t=4474, loss=0.04160073585808277\n",
      "Surface training t=4475, loss=0.04076194949448109\n",
      "Surface training t=4476, loss=0.040011314675211906\n",
      "Surface training t=4477, loss=0.04065164178609848\n",
      "Surface training t=4478, loss=0.04213070683181286\n",
      "Surface training t=4479, loss=0.04443219676613808\n",
      "Surface training t=4480, loss=0.043656812980771065\n",
      "Surface training t=4481, loss=0.039955951273441315\n",
      "Surface training t=4482, loss=0.04028737172484398\n",
      "Surface training t=4483, loss=0.03965252637863159\n",
      "Surface training t=4484, loss=0.04113364778459072\n",
      "Surface training t=4485, loss=0.04320310987532139\n",
      "Surface training t=4486, loss=0.04155222699046135\n",
      "Surface training t=4487, loss=0.039776887744665146\n",
      "Surface training t=4488, loss=0.04500516504049301\n",
      "Surface training t=4489, loss=0.04203513637185097\n",
      "Surface training t=4490, loss=0.04062958434224129\n",
      "Surface training t=4491, loss=0.03942548483610153\n",
      "Surface training t=4492, loss=0.0397868026047945\n",
      "Surface training t=4493, loss=0.04124046303331852\n",
      "Surface training t=4494, loss=0.04148203693330288\n",
      "Surface training t=4495, loss=0.042098455131053925\n",
      "Surface training t=4496, loss=0.04406360350549221\n",
      "Surface training t=4497, loss=0.04083099216222763\n",
      "Surface training t=4498, loss=0.03926262632012367\n",
      "Surface training t=4499, loss=0.040596840903162956\n",
      "Surface training t=4500, loss=0.04106457531452179\n",
      "Surface training t=4501, loss=0.043623778969049454\n",
      "Surface training t=4502, loss=0.04257740266621113\n",
      "Surface training t=4503, loss=0.04327021539211273\n",
      "Surface training t=4504, loss=0.041298696771264076\n",
      "Surface training t=4505, loss=0.04036306217312813\n",
      "Surface training t=4506, loss=0.04171137697994709\n",
      "Surface training t=4507, loss=0.042643388733267784\n",
      "Surface training t=4508, loss=0.04231939651072025\n",
      "Surface training t=4509, loss=0.04229259118437767\n",
      "Surface training t=4510, loss=0.04033018462359905\n",
      "Surface training t=4511, loss=0.04103076457977295\n",
      "Surface training t=4512, loss=0.0399588942527771\n",
      "Surface training t=4513, loss=0.04019481502473354\n",
      "Surface training t=4514, loss=0.04030755162239075\n",
      "Surface training t=4515, loss=0.04065125249326229\n",
      "Surface training t=4516, loss=0.04009516164660454\n",
      "Surface training t=4517, loss=0.04189158231019974\n",
      "Surface training t=4518, loss=0.04131426848471165\n",
      "Surface training t=4519, loss=0.043031664565205574\n",
      "Surface training t=4520, loss=0.04111872613430023\n",
      "Surface training t=4521, loss=0.041367948055267334\n",
      "Surface training t=4522, loss=0.038994522765278816\n",
      "Surface training t=4523, loss=0.040238216519355774\n",
      "Surface training t=4524, loss=0.0405452735722065\n",
      "Surface training t=4525, loss=0.04065146669745445\n",
      "Surface training t=4526, loss=0.04000689275562763\n",
      "Surface training t=4527, loss=0.04092482104897499\n",
      "Surface training t=4528, loss=0.04029209353029728\n",
      "Surface training t=4529, loss=0.041873978450894356\n",
      "Surface training t=4530, loss=0.040750518441200256\n",
      "Surface training t=4531, loss=0.03906328044831753\n",
      "Surface training t=4532, loss=0.038974134251475334\n",
      "Surface training t=4533, loss=0.039531370624899864\n",
      "Surface training t=4534, loss=0.03836572356522083\n",
      "Surface training t=4535, loss=0.038895079866051674\n",
      "Surface training t=4536, loss=0.039080917835235596\n",
      "Surface training t=4537, loss=0.039012715220451355\n",
      "Surface training t=4538, loss=0.0390192735940218\n",
      "Surface training t=4539, loss=0.0391573142260313\n",
      "Surface training t=4540, loss=0.03816411457955837\n",
      "Surface training t=4541, loss=0.03868957795202732\n",
      "Surface training t=4542, loss=0.03728513978421688\n",
      "Surface training t=4543, loss=0.03846966288983822\n",
      "Surface training t=4544, loss=0.03935553692281246\n",
      "Surface training t=4545, loss=0.03728475421667099\n",
      "Surface training t=4546, loss=0.036451512947678566\n",
      "Surface training t=4547, loss=0.037081293761730194\n",
      "Surface training t=4548, loss=0.03725377656519413\n",
      "Surface training t=4549, loss=0.038136620074510574\n",
      "Surface training t=4550, loss=0.03628980182111263\n",
      "Surface training t=4551, loss=0.03598061203956604\n",
      "Surface training t=4552, loss=0.036679474636912346\n",
      "Surface training t=4553, loss=0.03676388040184975\n",
      "Surface training t=4554, loss=0.036125535145401955\n",
      "Surface training t=4555, loss=0.03746421821415424\n",
      "Surface training t=4556, loss=0.03744032233953476\n",
      "Surface training t=4557, loss=0.037034183740615845\n",
      "Surface training t=4558, loss=0.038331449031829834\n",
      "Surface training t=4559, loss=0.03725193254649639\n",
      "Surface training t=4560, loss=0.03801757097244263\n",
      "Surface training t=4561, loss=0.03710048273205757\n",
      "Surface training t=4562, loss=0.03636076673865318\n",
      "Surface training t=4563, loss=0.036624468863010406\n",
      "Surface training t=4564, loss=0.035284120589494705\n",
      "Surface training t=4565, loss=0.035959210246801376\n",
      "Surface training t=4566, loss=0.03612174093723297\n",
      "Surface training t=4567, loss=0.037566106766462326\n",
      "Surface training t=4568, loss=0.03743803687393665\n",
      "Surface training t=4569, loss=0.037681324407458305\n",
      "Surface training t=4570, loss=0.03756382316350937\n",
      "Surface training t=4571, loss=0.03631437383592129\n",
      "Surface training t=4572, loss=0.03597039729356766\n",
      "Surface training t=4573, loss=0.03755198232829571\n",
      "Surface training t=4574, loss=0.03700018301606178\n",
      "Surface training t=4575, loss=0.03895493783056736\n",
      "Surface training t=4576, loss=0.0365269910544157\n",
      "Surface training t=4577, loss=0.035379862412810326\n",
      "Surface training t=4578, loss=0.03868510574102402\n",
      "Surface training t=4579, loss=0.037327010184526443\n",
      "Surface training t=4580, loss=0.037419190630316734\n",
      "Surface training t=4581, loss=0.03872549720108509\n",
      "Surface training t=4582, loss=0.037650637328624725\n",
      "Surface training t=4583, loss=0.03625084273517132\n",
      "Surface training t=4584, loss=0.03686312958598137\n",
      "Surface training t=4585, loss=0.03626445680856705\n",
      "Surface training t=4586, loss=0.036214280873537064\n",
      "Surface training t=4587, loss=0.039178939536213875\n",
      "Surface training t=4588, loss=0.03821358270943165\n",
      "Surface training t=4589, loss=0.03634731285274029\n",
      "Surface training t=4590, loss=0.037405792623758316\n",
      "Surface training t=4591, loss=0.03678775578737259\n",
      "Surface training t=4592, loss=0.0367828831076622\n",
      "Surface training t=4593, loss=0.036150217056274414\n",
      "Surface training t=4594, loss=0.03549143858253956\n",
      "Surface training t=4595, loss=0.03607475012540817\n",
      "Surface training t=4596, loss=0.03595605492591858\n",
      "Surface training t=4597, loss=0.035823702812194824\n",
      "Surface training t=4598, loss=0.03662640042603016\n",
      "Surface training t=4599, loss=0.036072468385100365\n",
      "Surface training t=4600, loss=0.03610432893037796\n",
      "Surface training t=4601, loss=0.03673475794494152\n",
      "Surface training t=4602, loss=0.037047479301691055\n",
      "Surface training t=4603, loss=0.035592325031757355\n",
      "Surface training t=4604, loss=0.03647361882030964\n",
      "Surface training t=4605, loss=0.037242745980620384\n",
      "Surface training t=4606, loss=0.035534339025616646\n",
      "Surface training t=4607, loss=0.036264434456825256\n",
      "Surface training t=4608, loss=0.03625229001045227\n",
      "Surface training t=4609, loss=0.03485102951526642\n",
      "Surface training t=4610, loss=0.037817930802702904\n",
      "Surface training t=4611, loss=0.036066893488168716\n",
      "Surface training t=4612, loss=0.03606613166630268\n",
      "Surface training t=4613, loss=0.03724538907408714\n",
      "Surface training t=4614, loss=0.036793818697333336\n",
      "Surface training t=4615, loss=0.037713268771767616\n",
      "Surface training t=4616, loss=0.03974984213709831\n",
      "Surface training t=4617, loss=0.03755779005587101\n",
      "Surface training t=4618, loss=0.03705083951354027\n",
      "Surface training t=4619, loss=0.03819656744599342\n",
      "Surface training t=4620, loss=0.03679461404681206\n",
      "Surface training t=4621, loss=0.0352858267724514\n",
      "Surface training t=4622, loss=0.038469644263386726\n",
      "Surface training t=4623, loss=0.03651105798780918\n",
      "Surface training t=4624, loss=0.03518213517963886\n",
      "Surface training t=4625, loss=0.03708738461136818\n",
      "Surface training t=4626, loss=0.036777110770344734\n",
      "Surface training t=4627, loss=0.03634312003850937\n",
      "Surface training t=4628, loss=0.0355303306132555\n",
      "Surface training t=4629, loss=0.037418182939291\n",
      "Surface training t=4630, loss=0.0363201629370451\n",
      "Surface training t=4631, loss=0.034432705491781235\n",
      "Surface training t=4632, loss=0.035037947818636894\n",
      "Surface training t=4633, loss=0.03668171726167202\n",
      "Surface training t=4634, loss=0.03684753552079201\n",
      "Surface training t=4635, loss=0.03690374828875065\n",
      "Surface training t=4636, loss=0.038876933977007866\n",
      "Surface training t=4637, loss=0.03926299139857292\n",
      "Surface training t=4638, loss=0.04153192602097988\n",
      "Surface training t=4639, loss=0.045724593102931976\n",
      "Surface training t=4640, loss=0.041295427829027176\n",
      "Surface training t=4641, loss=0.040385544300079346\n",
      "Surface training t=4642, loss=0.04134259559214115\n",
      "Surface training t=4643, loss=0.04010278731584549\n",
      "Surface training t=4644, loss=0.04198236018419266\n",
      "Surface training t=4645, loss=0.043837323784828186\n",
      "Surface training t=4646, loss=0.043496325612068176\n",
      "Surface training t=4647, loss=0.04071740806102753\n",
      "Surface training t=4648, loss=0.03970940597355366\n",
      "Surface training t=4649, loss=0.039555979892611504\n",
      "Surface training t=4650, loss=0.04127569869160652\n",
      "Surface training t=4651, loss=0.041977159678936005\n",
      "Surface training t=4652, loss=0.03998401574790478\n",
      "Surface training t=4653, loss=0.03944283723831177\n",
      "Surface training t=4654, loss=0.040465643629431725\n",
      "Surface training t=4655, loss=0.04126841574907303\n",
      "Surface training t=4656, loss=0.04255537688732147\n",
      "Surface training t=4657, loss=0.039232393726706505\n",
      "Surface training t=4658, loss=0.03967922180891037\n",
      "Surface training t=4659, loss=0.037839287891983986\n",
      "Surface training t=4660, loss=0.04153914004564285\n",
      "Surface training t=4661, loss=0.041518183425068855\n",
      "Surface training t=4662, loss=0.041315481066703796\n",
      "Surface training t=4663, loss=0.040485257282853127\n",
      "Surface training t=4664, loss=0.038717394694685936\n",
      "Surface training t=4665, loss=0.03965215943753719\n",
      "Surface training t=4666, loss=0.04255206137895584\n",
      "Surface training t=4667, loss=0.041698457673192024\n",
      "Surface training t=4668, loss=0.04193880967795849\n",
      "Surface training t=4669, loss=0.038917044177651405\n",
      "Surface training t=4670, loss=0.03859301470220089\n",
      "Surface training t=4671, loss=0.03928779996931553\n",
      "Surface training t=4672, loss=0.04089181497693062\n",
      "Surface training t=4673, loss=0.0401673037558794\n",
      "Surface training t=4674, loss=0.03991668485105038\n",
      "Surface training t=4675, loss=0.04190590977668762\n",
      "Surface training t=4676, loss=0.039196232333779335\n",
      "Surface training t=4677, loss=0.03828364424407482\n",
      "Surface training t=4678, loss=0.03857534006237984\n",
      "Surface training t=4679, loss=0.039748866111040115\n",
      "Surface training t=4680, loss=0.041495729237794876\n",
      "Surface training t=4681, loss=0.03840750642120838\n",
      "Surface training t=4682, loss=0.04171675443649292\n",
      "Surface training t=4683, loss=0.03695124387741089\n",
      "Surface training t=4684, loss=0.03784884326159954\n",
      "Surface training t=4685, loss=0.03830447234213352\n",
      "Surface training t=4686, loss=0.040242528542876244\n",
      "Surface training t=4687, loss=0.03942405618727207\n",
      "Surface training t=4688, loss=0.04223927669227123\n",
      "Surface training t=4689, loss=0.03933415748178959\n",
      "Surface training t=4690, loss=0.040161989629268646\n",
      "Surface training t=4691, loss=0.03844357840716839\n",
      "Surface training t=4692, loss=0.03803405351936817\n",
      "Surface training t=4693, loss=0.03809279389679432\n",
      "Surface training t=4694, loss=0.038129791617393494\n",
      "Surface training t=4695, loss=0.03789915330708027\n",
      "Surface training t=4696, loss=0.038705287501215935\n",
      "Surface training t=4697, loss=0.03912932984530926\n",
      "Surface training t=4698, loss=0.041063472628593445\n",
      "Surface training t=4699, loss=0.0406335461884737\n",
      "Surface training t=4700, loss=0.04231075756251812\n",
      "Surface training t=4701, loss=0.03848082385957241\n",
      "Surface training t=4702, loss=0.0370192751288414\n",
      "Surface training t=4703, loss=0.03563825786113739\n",
      "Surface training t=4704, loss=0.036805037409067154\n",
      "Surface training t=4705, loss=0.03866121172904968\n",
      "Surface training t=4706, loss=0.03635275363922119\n",
      "Surface training t=4707, loss=0.03702018968760967\n",
      "Surface training t=4708, loss=0.03717261925339699\n",
      "Surface training t=4709, loss=0.03777858801186085\n",
      "Surface training t=4710, loss=0.03594328090548515\n",
      "Surface training t=4711, loss=0.03627137281000614\n",
      "Surface training t=4712, loss=0.03583417274057865\n",
      "Surface training t=4713, loss=0.0356372632086277\n",
      "Surface training t=4714, loss=0.037443067878484726\n",
      "Surface training t=4715, loss=0.034778768196702003\n",
      "Surface training t=4716, loss=0.035422755405306816\n",
      "Surface training t=4717, loss=0.03608795255422592\n",
      "Surface training t=4718, loss=0.03625716455280781\n",
      "Surface training t=4719, loss=0.03638927638530731\n",
      "Surface training t=4720, loss=0.035088591277599335\n",
      "Surface training t=4721, loss=0.03493068553507328\n",
      "Surface training t=4722, loss=0.036225564777851105\n",
      "Surface training t=4723, loss=0.035718731582164764\n",
      "Surface training t=4724, loss=0.035362863913178444\n",
      "Surface training t=4725, loss=0.03585018590092659\n",
      "Surface training t=4726, loss=0.03678027726709843\n",
      "Surface training t=4727, loss=0.035226501524448395\n",
      "Surface training t=4728, loss=0.03540537878870964\n",
      "Surface training t=4729, loss=0.03568336367607117\n",
      "Surface training t=4730, loss=0.035702746361494064\n",
      "Surface training t=4731, loss=0.036895204335451126\n",
      "Surface training t=4732, loss=0.036179713904857635\n",
      "Surface training t=4733, loss=0.03604172542691231\n",
      "Surface training t=4734, loss=0.03641434945166111\n",
      "Surface training t=4735, loss=0.03580360673367977\n",
      "Surface training t=4736, loss=0.03559420630335808\n",
      "Surface training t=4737, loss=0.03572788089513779\n",
      "Surface training t=4738, loss=0.03541494347155094\n",
      "Surface training t=4739, loss=0.037026360630989075\n",
      "Surface training t=4740, loss=0.03437735326588154\n",
      "Surface training t=4741, loss=0.03598808869719505\n",
      "Surface training t=4742, loss=0.035808006301522255\n",
      "Surface training t=4743, loss=0.034968335181474686\n",
      "Surface training t=4744, loss=0.03485533595085144\n",
      "Surface training t=4745, loss=0.03510841727256775\n",
      "Surface training t=4746, loss=0.03475487232208252\n",
      "Surface training t=4747, loss=0.03557015769183636\n",
      "Surface training t=4748, loss=0.03558351658284664\n",
      "Surface training t=4749, loss=0.03516858071088791\n",
      "Surface training t=4750, loss=0.034477606415748596\n",
      "Surface training t=4751, loss=0.03491928055882454\n",
      "Surface training t=4752, loss=0.03606966510415077\n",
      "Surface training t=4753, loss=0.03484789654612541\n",
      "Surface training t=4754, loss=0.034061772748827934\n",
      "Surface training t=4755, loss=0.03649181313812733\n",
      "Surface training t=4756, loss=0.03496458753943443\n",
      "Surface training t=4757, loss=0.034664928913116455\n",
      "Surface training t=4758, loss=0.03570343740284443\n",
      "Surface training t=4759, loss=0.03577795252203941\n",
      "Surface training t=4760, loss=0.036149321123957634\n",
      "Surface training t=4761, loss=0.03635838441550732\n",
      "Surface training t=4762, loss=0.03621852584183216\n",
      "Surface training t=4763, loss=0.035450298339128494\n",
      "Surface training t=4764, loss=0.03576844930648804\n",
      "Surface training t=4765, loss=0.0365576334297657\n",
      "Surface training t=4766, loss=0.0355210080742836\n",
      "Surface training t=4767, loss=0.03623647429049015\n",
      "Surface training t=4768, loss=0.03709072060883045\n",
      "Surface training t=4769, loss=0.039062004536390305\n",
      "Surface training t=4770, loss=0.04364531673491001\n",
      "Surface training t=4771, loss=0.04274950921535492\n",
      "Surface training t=4772, loss=0.03724765032529831\n",
      "Surface training t=4773, loss=0.03824072331190109\n",
      "Surface training t=4774, loss=0.04018785618245602\n",
      "Surface training t=4775, loss=0.03866470791399479\n",
      "Surface training t=4776, loss=0.03953580930829048\n",
      "Surface training t=4777, loss=0.04249555058777332\n",
      "Surface training t=4778, loss=0.04100722260773182\n",
      "Surface training t=4779, loss=0.03932994417846203\n",
      "Surface training t=4780, loss=0.038176463916897774\n",
      "Surface training t=4781, loss=0.03947618044912815\n",
      "Surface training t=4782, loss=0.04005018435418606\n",
      "Surface training t=4783, loss=0.04095974937081337\n",
      "Surface training t=4784, loss=0.04240780510008335\n",
      "Surface training t=4785, loss=0.037035562098026276\n",
      "Surface training t=4786, loss=0.03935663215816021\n",
      "Surface training t=4787, loss=0.03869315981864929\n",
      "Surface training t=4788, loss=0.039620593190193176\n",
      "Surface training t=4789, loss=0.04142617993056774\n",
      "Surface training t=4790, loss=0.04044627584517002\n",
      "Surface training t=4791, loss=0.04012071713805199\n",
      "Surface training t=4792, loss=0.03673533350229263\n",
      "Surface training t=4793, loss=0.037468018010258675\n",
      "Surface training t=4794, loss=0.03880484960973263\n",
      "Surface training t=4795, loss=0.039663996547460556\n",
      "Surface training t=4796, loss=0.04243606701493263\n",
      "Surface training t=4797, loss=0.03889513202011585\n",
      "Surface training t=4798, loss=0.03838709928095341\n",
      "Surface training t=4799, loss=0.040547821670770645\n",
      "Surface training t=4800, loss=0.04069270007312298\n",
      "Surface training t=4801, loss=0.03958769142627716\n",
      "Surface training t=4802, loss=0.03998491354286671\n",
      "Surface training t=4803, loss=0.03717532567679882\n",
      "Surface training t=4804, loss=0.03958992473781109\n",
      "Surface training t=4805, loss=0.03850524500012398\n",
      "Surface training t=4806, loss=0.04126579128205776\n",
      "Surface training t=4807, loss=0.037857141345739365\n",
      "Surface training t=4808, loss=0.040704963728785515\n",
      "Surface training t=4809, loss=0.04033930040895939\n",
      "Surface training t=4810, loss=0.03946250304579735\n",
      "Surface training t=4811, loss=0.03765157237648964\n",
      "Surface training t=4812, loss=0.03717328421771526\n",
      "Surface training t=4813, loss=0.03614755906164646\n",
      "Surface training t=4814, loss=0.03686306066811085\n",
      "Surface training t=4815, loss=0.03596090152859688\n",
      "Surface training t=4816, loss=0.03546125628054142\n",
      "Surface training t=4817, loss=0.03409094177186489\n",
      "Surface training t=4818, loss=0.03281320817768574\n",
      "Surface training t=4819, loss=0.03488738648593426\n",
      "Surface training t=4820, loss=0.03466229699552059\n",
      "Surface training t=4821, loss=0.0350435096770525\n",
      "Surface training t=4822, loss=0.035341596230864525\n",
      "Surface training t=4823, loss=0.03350462391972542\n",
      "Surface training t=4824, loss=0.03518981300294399\n",
      "Surface training t=4825, loss=0.03463156148791313\n",
      "Surface training t=4826, loss=0.035326674580574036\n",
      "Surface training t=4827, loss=0.03472030349075794\n",
      "Surface training t=4828, loss=0.03514574468135834\n",
      "Surface training t=4829, loss=0.0345489326864481\n",
      "Surface training t=4830, loss=0.0344708189368248\n",
      "Surface training t=4831, loss=0.03414134308695793\n",
      "Surface training t=4832, loss=0.03347437642514706\n",
      "Surface training t=4833, loss=0.03364706411957741\n",
      "Surface training t=4834, loss=0.034762950614094734\n",
      "Surface training t=4835, loss=0.035427143797278404\n",
      "Surface training t=4836, loss=0.035183219239115715\n",
      "Surface training t=4837, loss=0.035483311861753464\n",
      "Surface training t=4838, loss=0.03643038310110569\n",
      "Surface training t=4839, loss=0.04098389856517315\n",
      "Surface training t=4840, loss=0.04070649296045303\n",
      "Surface training t=4841, loss=0.03836346976459026\n",
      "Surface training t=4842, loss=0.04058208130300045\n",
      "Surface training t=4843, loss=0.03963332064449787\n",
      "Surface training t=4844, loss=0.03905690088868141\n",
      "Surface training t=4845, loss=0.0384366549551487\n",
      "Surface training t=4846, loss=0.03938135504722595\n",
      "Surface training t=4847, loss=0.04142710752785206\n",
      "Surface training t=4848, loss=0.03646523877978325\n",
      "Surface training t=4849, loss=0.036550816148519516\n",
      "Surface training t=4850, loss=0.03856103867292404\n",
      "Surface training t=4851, loss=0.04090837016701698\n",
      "Surface training t=4852, loss=0.03984244540333748\n",
      "Surface training t=4853, loss=0.03803562372922897\n",
      "Surface training t=4854, loss=0.04035188816487789\n",
      "Surface training t=4855, loss=0.040728574618697166\n",
      "Surface training t=4856, loss=0.03692810796201229\n",
      "Surface training t=4857, loss=0.03715264052152634\n",
      "Surface training t=4858, loss=0.03860356658697128\n",
      "Surface training t=4859, loss=0.039103200659155846\n",
      "Surface training t=4860, loss=0.04142005182802677\n",
      "Surface training t=4861, loss=0.03972972556948662\n",
      "Surface training t=4862, loss=0.03847719728946686\n",
      "Surface training t=4863, loss=0.03831903450191021\n",
      "Surface training t=4864, loss=0.03799887001514435\n",
      "Surface training t=4865, loss=0.04219038970768452\n",
      "Surface training t=4866, loss=0.03737814165651798\n",
      "Surface training t=4867, loss=0.040953682735562325\n",
      "Surface training t=4868, loss=0.03664706274867058\n",
      "Surface training t=4869, loss=0.03827010281383991\n",
      "Surface training t=4870, loss=0.03661921247839928\n",
      "Surface training t=4871, loss=0.03841063566505909\n",
      "Surface training t=4872, loss=0.03692222759127617\n",
      "Surface training t=4873, loss=0.037047769874334335\n",
      "Surface training t=4874, loss=0.03566654026508331\n",
      "Surface training t=4875, loss=0.035767100751399994\n",
      "Surface training t=4876, loss=0.03476323001086712\n",
      "Surface training t=4877, loss=0.035357553511857986\n",
      "Surface training t=4878, loss=0.03487622179090977\n",
      "Surface training t=4879, loss=0.034672658890485764\n",
      "Surface training t=4880, loss=0.035646023228764534\n",
      "Surface training t=4881, loss=0.03484748490154743\n",
      "Surface training t=4882, loss=0.03532695956528187\n",
      "Surface training t=4883, loss=0.034073181450366974\n",
      "Surface training t=4884, loss=0.03319050744175911\n",
      "Surface training t=4885, loss=0.03453538566827774\n",
      "Surface training t=4886, loss=0.03366099111735821\n",
      "Surface training t=4887, loss=0.033685507252812386\n",
      "Surface training t=4888, loss=0.03575143218040466\n",
      "Surface training t=4889, loss=0.03350680321455002\n",
      "Surface training t=4890, loss=0.03397382237017155\n",
      "Surface training t=4891, loss=0.03572480194270611\n",
      "Surface training t=4892, loss=0.0359374713152647\n",
      "Surface training t=4893, loss=0.03468580171465874\n",
      "Surface training t=4894, loss=0.03368849493563175\n",
      "Surface training t=4895, loss=0.034201888367533684\n",
      "Surface training t=4896, loss=0.03497501276433468\n",
      "Surface training t=4897, loss=0.03519391641020775\n",
      "Surface training t=4898, loss=0.03440210968255997\n",
      "Surface training t=4899, loss=0.03394927270710468\n",
      "Surface training t=4900, loss=0.03515092469751835\n",
      "Surface training t=4901, loss=0.0334898978471756\n",
      "Surface training t=4902, loss=0.03567848354578018\n",
      "Surface training t=4903, loss=0.03671648167073727\n",
      "Surface training t=4904, loss=0.03605945222079754\n",
      "Surface training t=4905, loss=0.0366318766027689\n",
      "Surface training t=4906, loss=0.0363172423094511\n",
      "Surface training t=4907, loss=0.0391843318939209\n",
      "Surface training t=4908, loss=0.0408499650657177\n",
      "Surface training t=4909, loss=0.041008083149790764\n",
      "Surface training t=4910, loss=0.038393659517169\n",
      "Surface training t=4911, loss=0.03723835200071335\n",
      "Surface training t=4912, loss=0.035745056346058846\n",
      "Surface training t=4913, loss=0.03741590492427349\n",
      "Surface training t=4914, loss=0.03984323516488075\n",
      "Surface training t=4915, loss=0.039376793429255486\n",
      "Surface training t=4916, loss=0.03845883347094059\n",
      "Surface training t=4917, loss=0.03767670318484306\n",
      "Surface training t=4918, loss=0.03745633363723755\n",
      "Surface training t=4919, loss=0.039457691833376884\n",
      "Surface training t=4920, loss=0.03995761647820473\n",
      "Surface training t=4921, loss=0.03816317580640316\n",
      "Surface training t=4922, loss=0.039249517023563385\n",
      "Surface training t=4923, loss=0.03892667219042778\n",
      "Surface training t=4924, loss=0.03859167546033859\n",
      "Surface training t=4925, loss=0.038924455642700195\n",
      "Surface training t=4926, loss=0.03919521905481815\n",
      "Surface training t=4927, loss=0.03802179731428623\n",
      "Surface training t=4928, loss=0.039665067568421364\n",
      "Surface training t=4929, loss=0.03835161589086056\n",
      "Surface training t=4930, loss=0.038598617538809776\n",
      "Surface training t=4931, loss=0.03833898901939392\n",
      "Surface training t=4932, loss=0.03924626111984253\n",
      "Surface training t=4933, loss=0.03780929557979107\n",
      "Surface training t=4934, loss=0.03882933780550957\n",
      "Surface training t=4935, loss=0.03770565055310726\n",
      "Surface training t=4936, loss=0.039287641644477844\n",
      "Surface training t=4937, loss=0.03907316364347935\n",
      "Surface training t=4938, loss=0.0384213849902153\n",
      "Surface training t=4939, loss=0.03936411626636982\n",
      "Surface training t=4940, loss=0.037911755964159966\n",
      "Surface training t=4941, loss=0.038914259523153305\n",
      "Surface training t=4942, loss=0.037236208096146584\n",
      "Surface training t=4943, loss=0.038156865164637566\n",
      "Surface training t=4944, loss=0.03743268363177776\n",
      "Surface training t=4945, loss=0.038081830367445946\n",
      "Surface training t=4946, loss=0.0369814969599247\n",
      "Surface training t=4947, loss=0.037518348544836044\n",
      "Surface training t=4948, loss=0.037011949345469475\n",
      "Surface training t=4949, loss=0.03765421733260155\n",
      "Surface training t=4950, loss=0.03825423866510391\n",
      "Surface training t=4951, loss=0.03797224722802639\n",
      "Surface training t=4952, loss=0.037427352741360664\n",
      "Surface training t=4953, loss=0.037167174741625786\n",
      "Surface training t=4954, loss=0.03733970411121845\n",
      "Surface training t=4955, loss=0.037653472274541855\n",
      "Surface training t=4956, loss=0.03776941075921059\n",
      "Surface training t=4957, loss=0.037506718188524246\n",
      "Surface training t=4958, loss=0.03865472599864006\n",
      "Surface training t=4959, loss=0.03721866384148598\n",
      "Surface training t=4960, loss=0.03769191354513168\n",
      "Surface training t=4961, loss=0.037818582728505135\n",
      "Surface training t=4962, loss=0.03816965967416763\n",
      "Surface training t=4963, loss=0.03738691098988056\n",
      "Surface training t=4964, loss=0.036923933774232864\n",
      "Surface training t=4965, loss=0.03509919531643391\n",
      "Surface training t=4966, loss=0.03608817793428898\n",
      "Surface training t=4967, loss=0.03632693365216255\n",
      "Surface training t=4968, loss=0.03588932193815708\n",
      "Surface training t=4969, loss=0.03587550111114979\n",
      "Surface training t=4970, loss=0.03706854395568371\n",
      "Surface training t=4971, loss=0.037592191249132156\n",
      "Surface training t=4972, loss=0.037286195904016495\n",
      "Surface training t=4973, loss=0.037837449461221695\n",
      "Surface training t=4974, loss=0.036478063091635704\n",
      "Surface training t=4975, loss=0.036298658698797226\n",
      "Surface training t=4976, loss=0.03836563974618912\n",
      "Surface training t=4977, loss=0.03802801854908466\n",
      "Surface training t=4978, loss=0.03718988224864006\n",
      "Surface training t=4979, loss=0.03726026974618435\n",
      "Surface training t=4980, loss=0.03653867915272713\n",
      "Surface training t=4981, loss=0.03731217794120312\n",
      "Surface training t=4982, loss=0.03741629421710968\n",
      "Surface training t=4983, loss=0.03718266822397709\n",
      "Surface training t=4984, loss=0.0365581214427948\n",
      "Surface training t=4985, loss=0.038090046495199203\n",
      "Surface training t=4986, loss=0.036991046741604805\n",
      "Surface training t=4987, loss=0.037024764344096184\n",
      "Surface training t=4988, loss=0.03601078875362873\n",
      "Surface training t=4989, loss=0.0359576940536499\n",
      "Surface training t=4990, loss=0.035311384126544\n",
      "Surface training t=4991, loss=0.036134956404566765\n",
      "Surface training t=4992, loss=0.0367230549454689\n",
      "Surface training t=4993, loss=0.0359206460416317\n",
      "Surface training t=4994, loss=0.03725893050432205\n",
      "Surface training t=4995, loss=0.03656899556517601\n",
      "Surface training t=4996, loss=0.03799021244049072\n",
      "Surface training t=4997, loss=0.038003114983439445\n",
      "Surface training t=4998, loss=0.03706437721848488\n",
      "Surface training t=4999, loss=0.037770314142107964\n",
      "Surface training t=5000, loss=0.03772684186697006\n",
      "Surface training t=5001, loss=0.036825692281126976\n",
      "Surface training t=5002, loss=0.03677859902381897\n",
      "Surface training t=5003, loss=0.03620922565460205\n",
      "Surface training t=5004, loss=0.03629525750875473\n",
      "Surface training t=5005, loss=0.03724850155413151\n",
      "Surface training t=5006, loss=0.03710956498980522\n",
      "Surface training t=5007, loss=0.03529651090502739\n",
      "Surface training t=5008, loss=0.03725757263600826\n",
      "Surface training t=5009, loss=0.0371185801923275\n",
      "Surface training t=5010, loss=0.03702903725206852\n",
      "Surface training t=5011, loss=0.03618525341153145\n",
      "Surface training t=5012, loss=0.03603347949683666\n",
      "Surface training t=5013, loss=0.03604935668408871\n",
      "Surface training t=5014, loss=0.035984382033348083\n",
      "Surface training t=5015, loss=0.03648757562041283\n",
      "Surface training t=5016, loss=0.035676274448633194\n",
      "Surface training t=5017, loss=0.03729735314846039\n",
      "Surface training t=5018, loss=0.03764121234416962\n",
      "Surface training t=5019, loss=0.03631855174899101\n",
      "Surface training t=5020, loss=0.036031829193234444\n",
      "Surface training t=5021, loss=0.03641402721405029\n",
      "Surface training t=5022, loss=0.03777502104640007\n",
      "Surface training t=5023, loss=0.0364057756960392\n",
      "Surface training t=5024, loss=0.03576955571770668\n",
      "Surface training t=5025, loss=0.03557965159416199\n",
      "Surface training t=5026, loss=0.03539316542446613\n",
      "Surface training t=5027, loss=0.036359965801239014\n",
      "Surface training t=5028, loss=0.035694338381290436\n",
      "Surface training t=5029, loss=0.03569156676530838\n",
      "Surface training t=5030, loss=0.03574595972895622\n",
      "Surface training t=5031, loss=0.037484386935830116\n",
      "Surface training t=5032, loss=0.038246745243668556\n",
      "Surface training t=5033, loss=0.036270614713430405\n",
      "Surface training t=5034, loss=0.03729780577123165\n",
      "Surface training t=5035, loss=0.03683106228709221\n",
      "Surface training t=5036, loss=0.036992425099015236\n",
      "Surface training t=5037, loss=0.0362496804445982\n",
      "Surface training t=5038, loss=0.03660266473889351\n",
      "Surface training t=5039, loss=0.036679770797491074\n",
      "Surface training t=5040, loss=0.036719080060720444\n",
      "Surface training t=5041, loss=0.03659014031291008\n",
      "Surface training t=5042, loss=0.036274295300245285\n",
      "Surface training t=5043, loss=0.03747011534869671\n",
      "Surface training t=5044, loss=0.036913204938173294\n",
      "Surface training t=5045, loss=0.037436600774526596\n",
      "Surface training t=5046, loss=0.037326302379369736\n",
      "Surface training t=5047, loss=0.037694746628403664\n",
      "Surface training t=5048, loss=0.03592211194336414\n",
      "Surface training t=5049, loss=0.03583918698132038\n",
      "Surface training t=5050, loss=0.0358177088201046\n",
      "Surface training t=5051, loss=0.03544575907289982\n",
      "Surface training t=5052, loss=0.03580109961330891\n",
      "Surface training t=5053, loss=0.03749481961131096\n",
      "Surface training t=5054, loss=0.03711097873747349\n",
      "Surface training t=5055, loss=0.03633197024464607\n",
      "Surface training t=5056, loss=0.03680066950619221\n",
      "Surface training t=5057, loss=0.035628121346235275\n",
      "Surface training t=5058, loss=0.0367510337382555\n",
      "Surface training t=5059, loss=0.03500918485224247\n",
      "Surface training t=5060, loss=0.03686656430363655\n",
      "Surface training t=5061, loss=0.036758698523044586\n",
      "Surface training t=5062, loss=0.03568355739116669\n",
      "Surface training t=5063, loss=0.03605300560593605\n",
      "Surface training t=5064, loss=0.03679633140563965\n",
      "Surface training t=5065, loss=0.03574094921350479\n",
      "Surface training t=5066, loss=0.035391081124544144\n",
      "Surface training t=5067, loss=0.03726823441684246\n",
      "Surface training t=5068, loss=0.03588755056262016\n",
      "Surface training t=5069, loss=0.03586830943822861\n",
      "Surface training t=5070, loss=0.03694658540189266\n",
      "Surface training t=5071, loss=0.035758694633841515\n",
      "Surface training t=5072, loss=0.03617408126592636\n",
      "Surface training t=5073, loss=0.036909446120262146\n",
      "Surface training t=5074, loss=0.03582239709794521\n",
      "Surface training t=5075, loss=0.0352479200810194\n",
      "Surface training t=5076, loss=0.03533010557293892\n",
      "Surface training t=5077, loss=0.035859161987900734\n",
      "Surface training t=5078, loss=0.0360154677182436\n",
      "Surface training t=5079, loss=0.036181412637233734\n",
      "Surface training t=5080, loss=0.03632733412086964\n",
      "Surface training t=5081, loss=0.036275068297982216\n",
      "Surface training t=5082, loss=0.034937504678964615\n",
      "Surface training t=5083, loss=0.03517915494740009\n",
      "Surface training t=5084, loss=0.03503706119954586\n",
      "Surface training t=5085, loss=0.035477668046951294\n",
      "Surface training t=5086, loss=0.03668496012687683\n",
      "Surface training t=5087, loss=0.03487760201096535\n",
      "Surface training t=5088, loss=0.035771602764725685\n",
      "Surface training t=5089, loss=0.03639537841081619\n",
      "Surface training t=5090, loss=0.03692198544740677\n",
      "Surface training t=5091, loss=0.038055894896388054\n",
      "Surface training t=5092, loss=0.03721453249454498\n",
      "Surface training t=5093, loss=0.03626426123082638\n",
      "Surface training t=5094, loss=0.03741741552948952\n",
      "Surface training t=5095, loss=0.03422597423195839\n",
      "Surface training t=5096, loss=0.03466852381825447\n",
      "Surface training t=5097, loss=0.03453679382801056\n",
      "Surface training t=5098, loss=0.03409839980304241\n",
      "Surface training t=5099, loss=0.03468647785484791\n",
      "Surface training t=5100, loss=0.03521554544568062\n",
      "Surface training t=5101, loss=0.033861204981803894\n",
      "Surface training t=5102, loss=0.032599346712231636\n",
      "Surface training t=5103, loss=0.032917965203523636\n",
      "Surface training t=5104, loss=0.03295991197228432\n",
      "Surface training t=5105, loss=0.0341960359364748\n",
      "Surface training t=5106, loss=0.032055001705884933\n",
      "Surface training t=5107, loss=0.0334689486771822\n",
      "Surface training t=5108, loss=0.033682530745863914\n",
      "Surface training t=5109, loss=0.03243311680853367\n",
      "Surface training t=5110, loss=0.03374049440026283\n",
      "Surface training t=5111, loss=0.033635588362812996\n",
      "Surface training t=5112, loss=0.032962409779429436\n",
      "Surface training t=5113, loss=0.031778912991285324\n",
      "Surface training t=5114, loss=0.032761335372924805\n",
      "Surface training t=5115, loss=0.032532851211726665\n",
      "Surface training t=5116, loss=0.03406847454607487\n",
      "Surface training t=5117, loss=0.032622115686535835\n",
      "Surface training t=5118, loss=0.03239401150494814\n",
      "Surface training t=5119, loss=0.032313400879502296\n",
      "Surface training t=5120, loss=0.03314480744302273\n",
      "Surface training t=5121, loss=0.03545320779085159\n",
      "Surface training t=5122, loss=0.03325947746634483\n",
      "Surface training t=5123, loss=0.03353287652134895\n",
      "Surface training t=5124, loss=0.034691017121076584\n",
      "Surface training t=5125, loss=0.03315300866961479\n",
      "Surface training t=5126, loss=0.031956328079104424\n",
      "Surface training t=5127, loss=0.032722678035497665\n",
      "Surface training t=5128, loss=0.03272384591400623\n",
      "Surface training t=5129, loss=0.0316872950643301\n",
      "Surface training t=5130, loss=0.033369677141308784\n",
      "Surface training t=5131, loss=0.033141156658530235\n",
      "Surface training t=5132, loss=0.03180974815040827\n",
      "Surface training t=5133, loss=0.03341626562178135\n",
      "Surface training t=5134, loss=0.03466888144612312\n",
      "Surface training t=5135, loss=0.03389781340956688\n",
      "Surface training t=5136, loss=0.0332845002412796\n",
      "Surface training t=5137, loss=0.032924569211900234\n",
      "Surface training t=5138, loss=0.03475845418870449\n",
      "Surface training t=5139, loss=0.03308812156319618\n",
      "Surface training t=5140, loss=0.03222787380218506\n",
      "Surface training t=5141, loss=0.03244726546108723\n",
      "Surface training t=5142, loss=0.032539788633584976\n",
      "Surface training t=5143, loss=0.03303392603993416\n",
      "Surface training t=5144, loss=0.03211768716573715\n",
      "Surface training t=5145, loss=0.03349780850112438\n",
      "Surface training t=5146, loss=0.03347279503941536\n",
      "Surface training t=5147, loss=0.033199381083250046\n",
      "Surface training t=5148, loss=0.0331086739897728\n",
      "Surface training t=5149, loss=0.033545590937137604\n",
      "Surface training t=5150, loss=0.033534662798047066\n",
      "Surface training t=5151, loss=0.03398917242884636\n",
      "Surface training t=5152, loss=0.03464060090482235\n",
      "Surface training t=5153, loss=0.034438012167811394\n",
      "Surface training t=5154, loss=0.03403162211179733\n",
      "Surface training t=5155, loss=0.03344796970486641\n",
      "Surface training t=5156, loss=0.03276699688285589\n",
      "Surface training t=5157, loss=0.0338932853192091\n",
      "Surface training t=5158, loss=0.034336430951952934\n",
      "Surface training t=5159, loss=0.03416827507317066\n",
      "Surface training t=5160, loss=0.03370748460292816\n",
      "Surface training t=5161, loss=0.031953283585608006\n",
      "Surface training t=5162, loss=0.031055587343871593\n",
      "Surface training t=5163, loss=0.03382121026515961\n",
      "Surface training t=5164, loss=0.03130307234823704\n",
      "Surface training t=5165, loss=0.031897494569420815\n",
      "Surface training t=5166, loss=0.031434777192771435\n",
      "Surface training t=5167, loss=0.03234982118010521\n",
      "Surface training t=5168, loss=0.033111581578850746\n",
      "Surface training t=5169, loss=0.03136946726590395\n",
      "Surface training t=5170, loss=0.03216489590704441\n",
      "Surface training t=5171, loss=0.032844485715031624\n",
      "Surface training t=5172, loss=0.032881252467632294\n",
      "Surface training t=5173, loss=0.03515871986746788\n",
      "Surface training t=5174, loss=0.03791167587041855\n",
      "Surface training t=5175, loss=0.03616379201412201\n",
      "Surface training t=5176, loss=0.035322053357958794\n",
      "Surface training t=5177, loss=0.03656505234539509\n",
      "Surface training t=5178, loss=0.03641582280397415\n",
      "Surface training t=5179, loss=0.036910418421030045\n",
      "Surface training t=5180, loss=0.03483940288424492\n",
      "Surface training t=5181, loss=0.033948902040719986\n",
      "Surface training t=5182, loss=0.035309601575136185\n",
      "Surface training t=5183, loss=0.03655104339122772\n",
      "Surface training t=5184, loss=0.03515722416341305\n",
      "Surface training t=5185, loss=0.03477484919130802\n",
      "Surface training t=5186, loss=0.03453259356319904\n",
      "Surface training t=5187, loss=0.03624003194272518\n",
      "Surface training t=5188, loss=0.03656492196023464\n",
      "Surface training t=5189, loss=0.036839164793491364\n",
      "Surface training t=5190, loss=0.03614024072885513\n",
      "Surface training t=5191, loss=0.03517313115298748\n",
      "Surface training t=5192, loss=0.034570420160889626\n",
      "Surface training t=5193, loss=0.03358971327543259\n",
      "Surface training t=5194, loss=0.03494752757251263\n",
      "Surface training t=5195, loss=0.033157963305711746\n",
      "Surface training t=5196, loss=0.03277591057121754\n",
      "Surface training t=5197, loss=0.032922372221946716\n",
      "Surface training t=5198, loss=0.03242228180170059\n",
      "Surface training t=5199, loss=0.03248459845781326\n",
      "Surface training t=5200, loss=0.03316650167107582\n",
      "Surface training t=5201, loss=0.031459356658160686\n",
      "Surface training t=5202, loss=0.03196784108877182\n",
      "Surface training t=5203, loss=0.0319050382822752\n",
      "Surface training t=5204, loss=0.03214564546942711\n",
      "Surface training t=5205, loss=0.03176751360297203\n",
      "Surface training t=5206, loss=0.03186851181089878\n",
      "Surface training t=5207, loss=0.03108977433294058\n",
      "Surface training t=5208, loss=0.03271779045462608\n",
      "Surface training t=5209, loss=0.0319343414157629\n",
      "Surface training t=5210, loss=0.03258390072733164\n",
      "Surface training t=5211, loss=0.03310537710785866\n",
      "Surface training t=5212, loss=0.0320705808699131\n",
      "Surface training t=5213, loss=0.032433269545435905\n",
      "Surface training t=5214, loss=0.03433452360332012\n",
      "Surface training t=5215, loss=0.034456849098205566\n",
      "Surface training t=5216, loss=0.03203059732913971\n",
      "Surface training t=5217, loss=0.03296445496380329\n",
      "Surface training t=5218, loss=0.033868784084916115\n",
      "Surface training t=5219, loss=0.03291722945868969\n",
      "Surface training t=5220, loss=0.0319735212251544\n",
      "Surface training t=5221, loss=0.033607637509703636\n",
      "Surface training t=5222, loss=0.033783270977437496\n",
      "Surface training t=5223, loss=0.03684674762189388\n",
      "Surface training t=5224, loss=0.03505701944231987\n",
      "Surface training t=5225, loss=0.035743676126003265\n",
      "Surface training t=5226, loss=0.038809362798929214\n",
      "Surface training t=5227, loss=0.03733032010495663\n",
      "Surface training t=5228, loss=0.036720944568514824\n",
      "Surface training t=5229, loss=0.03631679527461529\n",
      "Surface training t=5230, loss=0.03737155720591545\n",
      "Surface training t=5231, loss=0.03774643503129482\n",
      "Surface training t=5232, loss=0.034452155232429504\n",
      "Surface training t=5233, loss=0.034266864880919456\n",
      "Surface training t=5234, loss=0.03633825108408928\n",
      "Surface training t=5235, loss=0.0398764256387949\n",
      "Surface training t=5236, loss=0.03776049055159092\n",
      "Surface training t=5237, loss=0.036212315782904625\n",
      "Surface training t=5238, loss=0.034043654799461365\n",
      "Surface training t=5239, loss=0.03544720076024532\n",
      "Surface training t=5240, loss=0.036722179502248764\n",
      "Surface training t=5241, loss=0.040295181795954704\n",
      "Surface training t=5242, loss=0.03421895578503609\n",
      "Surface training t=5243, loss=0.033063365146517754\n",
      "Surface training t=5244, loss=0.0336487740278244\n",
      "Surface training t=5245, loss=0.03426516056060791\n",
      "Surface training t=5246, loss=0.03500819578766823\n",
      "Surface training t=5247, loss=0.03866413049399853\n",
      "Surface training t=5248, loss=0.03868838585913181\n",
      "Surface training t=5249, loss=0.037143006920814514\n",
      "Surface training t=5250, loss=0.0340045802295208\n",
      "Surface training t=5251, loss=0.03504440374672413\n",
      "Surface training t=5252, loss=0.036963922902941704\n",
      "Surface training t=5253, loss=0.039511581882834435\n",
      "Surface training t=5254, loss=0.034046245738863945\n",
      "Surface training t=5255, loss=0.034898849204182625\n",
      "Surface training t=5256, loss=0.035149745643138885\n",
      "Surface training t=5257, loss=0.038253623992204666\n",
      "Surface training t=5258, loss=0.03601657412946224\n",
      "Surface training t=5259, loss=0.037575751543045044\n",
      "Surface training t=5260, loss=0.034354135394096375\n",
      "Surface training t=5261, loss=0.037971777841448784\n",
      "Surface training t=5262, loss=0.03500151261687279\n",
      "Surface training t=5263, loss=0.03528694063425064\n",
      "Surface training t=5264, loss=0.033188882283866405\n",
      "Surface training t=5265, loss=0.03628178499639034\n",
      "Surface training t=5266, loss=0.035490160807967186\n",
      "Surface training t=5267, loss=0.03808535635471344\n",
      "Surface training t=5268, loss=0.03664439916610718\n",
      "Surface training t=5269, loss=0.035296447575092316\n",
      "Surface training t=5270, loss=0.03325776942074299\n",
      "Surface training t=5271, loss=0.03300259727984667\n",
      "Surface training t=5272, loss=0.03357754833996296\n",
      "Surface training t=5273, loss=0.03447607532143593\n",
      "Surface training t=5274, loss=0.033411359414458275\n",
      "Surface training t=5275, loss=0.03471388295292854\n",
      "Surface training t=5276, loss=0.03271137084811926\n",
      "Surface training t=5277, loss=0.03467836603522301\n",
      "Surface training t=5278, loss=0.033997731283307076\n",
      "Surface training t=5279, loss=0.03260514326393604\n",
      "Surface training t=5280, loss=0.031925031915307045\n",
      "Surface training t=5281, loss=0.03162105567753315\n",
      "Surface training t=5282, loss=0.031642137095332146\n",
      "Surface training t=5283, loss=0.03253396600484848\n",
      "Surface training t=5284, loss=0.031337566673755646\n",
      "Surface training t=5285, loss=0.031966155394911766\n",
      "Surface training t=5286, loss=0.03171098791062832\n",
      "Surface training t=5287, loss=0.031009536236524582\n",
      "Surface training t=5288, loss=0.03229314927011728\n",
      "Surface training t=5289, loss=0.03168917074799538\n",
      "Surface training t=5290, loss=0.030899017117917538\n",
      "Surface training t=5291, loss=0.032245186157524586\n",
      "Surface training t=5292, loss=0.03095011506229639\n",
      "Surface training t=5293, loss=0.03287230804562569\n",
      "Surface training t=5294, loss=0.03231082949787378\n",
      "Surface training t=5295, loss=0.03234346583485603\n",
      "Surface training t=5296, loss=0.03156966157257557\n",
      "Surface training t=5297, loss=0.031727624125778675\n",
      "Surface training t=5298, loss=0.031148466281592846\n",
      "Surface training t=5299, loss=0.032631708309054375\n",
      "Surface training t=5300, loss=0.03309893235564232\n",
      "Surface training t=5301, loss=0.032907379791140556\n",
      "Surface training t=5302, loss=0.031634148210287094\n",
      "Surface training t=5303, loss=0.03249802440404892\n",
      "Surface training t=5304, loss=0.031029208563268185\n",
      "Surface training t=5305, loss=0.030791858211159706\n",
      "Surface training t=5306, loss=0.03195127099752426\n",
      "Surface training t=5307, loss=0.03148958645761013\n",
      "Surface training t=5308, loss=0.03216761536896229\n",
      "Surface training t=5309, loss=0.03268337994813919\n",
      "Surface training t=5310, loss=0.030867082998156548\n",
      "Surface training t=5311, loss=0.03269355557858944\n",
      "Surface training t=5312, loss=0.03368937410414219\n",
      "Surface training t=5313, loss=0.03264021500945091\n",
      "Surface training t=5314, loss=0.03134303167462349\n",
      "Surface training t=5315, loss=0.03263095021247864\n",
      "Surface training t=5316, loss=0.03323988150805235\n",
      "Surface training t=5317, loss=0.030246919952332973\n",
      "Surface training t=5318, loss=0.0329438503831625\n",
      "Surface training t=5319, loss=0.031048581935465336\n",
      "Surface training t=5320, loss=0.03133611660450697\n",
      "Surface training t=5321, loss=0.03321945108473301\n",
      "Surface training t=5322, loss=0.03322719223797321\n",
      "Surface training t=5323, loss=0.03318855259567499\n",
      "Surface training t=5324, loss=0.03445318527519703\n",
      "Surface training t=5325, loss=0.03520016558468342\n",
      "Surface training t=5326, loss=0.03777381405234337\n",
      "Surface training t=5327, loss=0.03706934489309788\n",
      "Surface training t=5328, loss=0.03543740138411522\n",
      "Surface training t=5329, loss=0.03319933079183102\n",
      "Surface training t=5330, loss=0.03287072665989399\n",
      "Surface training t=5331, loss=0.03287206403911114\n",
      "Surface training t=5332, loss=0.03449755720794201\n",
      "Surface training t=5333, loss=0.034891700372099876\n",
      "Surface training t=5334, loss=0.03631146810948849\n",
      "Surface training t=5335, loss=0.037484804168343544\n",
      "Surface training t=5336, loss=0.035443440079689026\n",
      "Surface training t=5337, loss=0.03498848155140877\n",
      "Surface training t=5338, loss=0.03559486195445061\n",
      "Surface training t=5339, loss=0.036072706803679466\n",
      "Surface training t=5340, loss=0.037144940346479416\n",
      "Surface training t=5341, loss=0.03595193848013878\n",
      "Surface training t=5342, loss=0.036077624186873436\n",
      "Surface training t=5343, loss=0.035558076575398445\n",
      "Surface training t=5344, loss=0.03577405400574207\n",
      "Surface training t=5345, loss=0.03522128239274025\n",
      "Surface training t=5346, loss=0.03474549576640129\n",
      "Surface training t=5347, loss=0.03462471812963486\n",
      "Surface training t=5348, loss=0.03512082062661648\n",
      "Surface training t=5349, loss=0.03453660570085049\n",
      "Surface training t=5350, loss=0.03512341156601906\n",
      "Surface training t=5351, loss=0.034804942086339\n",
      "Surface training t=5352, loss=0.03583662584424019\n",
      "Surface training t=5353, loss=0.03582572937011719\n",
      "Surface training t=5354, loss=0.03563728556036949\n",
      "Surface training t=5355, loss=0.03498484194278717\n",
      "Surface training t=5356, loss=0.03534193895757198\n",
      "Surface training t=5357, loss=0.034443289041519165\n",
      "Surface training t=5358, loss=0.03526119329035282\n",
      "Surface training t=5359, loss=0.034956397488713264\n",
      "Surface training t=5360, loss=0.03590137138962746\n",
      "Surface training t=5361, loss=0.03477805107831955\n",
      "Surface training t=5362, loss=0.03426003456115723\n",
      "Surface training t=5363, loss=0.03623725846409798\n",
      "Surface training t=5364, loss=0.03415348380804062\n",
      "Surface training t=5365, loss=0.03429337032139301\n",
      "Surface training t=5366, loss=0.03498337231576443\n",
      "Surface training t=5367, loss=0.03555932454764843\n",
      "Surface training t=5368, loss=0.03427059203386307\n",
      "Surface training t=5369, loss=0.03595314174890518\n",
      "Surface training t=5370, loss=0.03551122173666954\n",
      "Surface training t=5371, loss=0.03524866700172424\n",
      "Surface training t=5372, loss=0.03466993197798729\n",
      "Surface training t=5373, loss=0.033921096473932266\n",
      "Surface training t=5374, loss=0.034114547073841095\n",
      "Surface training t=5375, loss=0.03459559381008148\n",
      "Surface training t=5376, loss=0.034670254215598106\n",
      "Surface training t=5377, loss=0.03542759083211422\n",
      "Surface training t=5378, loss=0.034451112151145935\n",
      "Surface training t=5379, loss=0.03342551551759243\n",
      "Surface training t=5380, loss=0.034038349986076355\n",
      "Surface training t=5381, loss=0.03323131054639816\n",
      "Surface training t=5382, loss=0.034434039145708084\n",
      "Surface training t=5383, loss=0.034600889310240746\n",
      "Surface training t=5384, loss=0.035778941586613655\n",
      "Surface training t=5385, loss=0.037001486867666245\n",
      "Surface training t=5386, loss=0.03498850576579571\n",
      "Surface training t=5387, loss=0.034823330119252205\n",
      "Surface training t=5388, loss=0.03454672545194626\n",
      "Surface training t=5389, loss=0.034701382741332054\n",
      "Surface training t=5390, loss=0.034642817452549934\n",
      "Surface training t=5391, loss=0.03425433486700058\n",
      "Surface training t=5392, loss=0.03500157222151756\n",
      "Surface training t=5393, loss=0.034530917182564735\n",
      "Surface training t=5394, loss=0.0339485127478838\n",
      "Surface training t=5395, loss=0.0338046383112669\n",
      "Surface training t=5396, loss=0.03375667706131935\n",
      "Surface training t=5397, loss=0.03451961278915405\n",
      "Surface training t=5398, loss=0.03434792719781399\n",
      "Surface training t=5399, loss=0.03548973426222801\n",
      "Surface training t=5400, loss=0.03437091317027807\n",
      "Surface training t=5401, loss=0.03392924554646015\n",
      "Surface training t=5402, loss=0.03273048624396324\n",
      "Surface training t=5403, loss=0.03361567296087742\n",
      "Surface training t=5404, loss=0.03219832107424736\n",
      "Surface training t=5405, loss=0.03219880908727646\n",
      "Surface training t=5406, loss=0.03290863800793886\n",
      "Surface training t=5407, loss=0.0313237551599741\n",
      "Surface training t=5408, loss=0.0322501128539443\n",
      "Surface training t=5409, loss=0.03057494480162859\n",
      "Surface training t=5410, loss=0.032801708206534386\n",
      "Surface training t=5411, loss=0.031143602915108204\n",
      "Surface training t=5412, loss=0.031100712716579437\n",
      "Surface training t=5413, loss=0.031598517671227455\n",
      "Surface training t=5414, loss=0.03066903632134199\n",
      "Surface training t=5415, loss=0.031000607647001743\n",
      "Surface training t=5416, loss=0.031755764968693256\n",
      "Surface training t=5417, loss=0.030616840347647667\n",
      "Surface training t=5418, loss=0.029872646555304527\n",
      "Surface training t=5419, loss=0.031064840033650398\n",
      "Surface training t=5420, loss=0.029937254264950752\n",
      "Surface training t=5421, loss=0.03064123634248972\n",
      "Surface training t=5422, loss=0.030093220993876457\n",
      "Surface training t=5423, loss=0.03097207471728325\n",
      "Surface training t=5424, loss=0.031081325374543667\n",
      "Surface training t=5425, loss=0.03162411134690046\n",
      "Surface training t=5426, loss=0.030331377871334553\n",
      "Surface training t=5427, loss=0.031057309359312057\n",
      "Surface training t=5428, loss=0.03082286473363638\n",
      "Surface training t=5429, loss=0.031969351693987846\n",
      "Surface training t=5430, loss=0.03240375220775604\n",
      "Surface training t=5431, loss=0.03212310094386339\n",
      "Surface training t=5432, loss=0.030880524776875973\n",
      "Surface training t=5433, loss=0.030511475168168545\n",
      "Surface training t=5434, loss=0.02885456569492817\n",
      "Surface training t=5435, loss=0.030018508434295654\n",
      "Surface training t=5436, loss=0.0302119180560112\n",
      "Surface training t=5437, loss=0.030269531533122063\n",
      "Surface training t=5438, loss=0.031429107300937176\n",
      "Surface training t=5439, loss=0.02920449525117874\n",
      "Surface training t=5440, loss=0.029078182764351368\n",
      "Surface training t=5441, loss=0.03032167349010706\n",
      "Surface training t=5442, loss=0.029423938132822514\n",
      "Surface training t=5443, loss=0.02935013920068741\n",
      "Surface training t=5444, loss=0.02946640457957983\n",
      "Surface training t=5445, loss=0.03021339327096939\n",
      "Surface training t=5446, loss=0.029856115579605103\n",
      "Surface training t=5447, loss=0.029817226342856884\n",
      "Surface training t=5448, loss=0.029161053709685802\n",
      "Surface training t=5449, loss=0.030332937836647034\n",
      "Surface training t=5450, loss=0.030432788655161858\n",
      "Surface training t=5451, loss=0.031429700553417206\n",
      "Surface training t=5452, loss=0.029294373467564583\n",
      "Surface training t=5453, loss=0.031145518645644188\n",
      "Surface training t=5454, loss=0.030410884879529476\n",
      "Surface training t=5455, loss=0.031045401468873024\n",
      "Surface training t=5456, loss=0.03201964870095253\n",
      "Surface training t=5457, loss=0.03059143666177988\n",
      "Surface training t=5458, loss=0.031017963774502277\n",
      "Surface training t=5459, loss=0.031380439177155495\n",
      "Surface training t=5460, loss=0.029193077236413956\n",
      "Surface training t=5461, loss=0.030114556662738323\n",
      "Surface training t=5462, loss=0.031023451127111912\n",
      "Surface training t=5463, loss=0.031254423782229424\n",
      "Surface training t=5464, loss=0.03062781784683466\n",
      "Surface training t=5465, loss=0.029718654230237007\n",
      "Surface training t=5466, loss=0.030960526317358017\n",
      "Surface training t=5467, loss=0.03131341189146042\n",
      "Surface training t=5468, loss=0.030828112736344337\n",
      "Surface training t=5469, loss=0.030327211134135723\n",
      "Surface training t=5470, loss=0.03030259720981121\n",
      "Surface training t=5471, loss=0.030036463402211666\n",
      "Surface training t=5472, loss=0.028974613174796104\n",
      "Surface training t=5473, loss=0.029555813409388065\n",
      "Surface training t=5474, loss=0.02974994946271181\n",
      "Surface training t=5475, loss=0.0319483857601881\n",
      "Surface training t=5476, loss=0.02863688115030527\n",
      "Surface training t=5477, loss=0.03006726037710905\n",
      "Surface training t=5478, loss=0.030167813412845135\n",
      "Surface training t=5479, loss=0.030833213590085506\n",
      "Surface training t=5480, loss=0.030856944620609283\n",
      "Surface training t=5481, loss=0.029871714301407337\n",
      "Surface training t=5482, loss=0.03064161166548729\n",
      "Surface training t=5483, loss=0.029938556253910065\n",
      "Surface training t=5484, loss=0.028909536078572273\n",
      "Surface training t=5485, loss=0.0294432220980525\n",
      "Surface training t=5486, loss=0.029533114284276962\n",
      "Surface training t=5487, loss=0.03026492800563574\n",
      "Surface training t=5488, loss=0.030048096552491188\n",
      "Surface training t=5489, loss=0.030283969826996326\n",
      "Surface training t=5490, loss=0.02960030548274517\n",
      "Surface training t=5491, loss=0.031025776639580727\n",
      "Surface training t=5492, loss=0.031393690034747124\n",
      "Surface training t=5493, loss=0.030705741606652737\n",
      "Surface training t=5494, loss=0.029519994743168354\n",
      "Surface training t=5495, loss=0.02978176809847355\n",
      "Surface training t=5496, loss=0.030353830195963383\n",
      "Surface training t=5497, loss=0.02999659813940525\n",
      "Surface training t=5498, loss=0.029687246307730675\n",
      "Surface training t=5499, loss=0.031833916902542114\n",
      "Surface training t=5500, loss=0.030643603764474392\n",
      "Surface training t=5501, loss=0.02941833157092333\n",
      "Surface training t=5502, loss=0.02856940682977438\n",
      "Surface training t=5503, loss=0.02798045612871647\n",
      "Surface training t=5504, loss=0.030648810788989067\n",
      "Surface training t=5505, loss=0.02985647227615118\n",
      "Surface training t=5506, loss=0.029914206825196743\n",
      "Surface training t=5507, loss=0.029160658828914165\n",
      "Surface training t=5508, loss=0.029856816865503788\n",
      "Surface training t=5509, loss=0.03100709244608879\n",
      "Surface training t=5510, loss=0.030745514668524265\n",
      "Surface training t=5511, loss=0.030512847937643528\n",
      "Surface training t=5512, loss=0.032547177746891975\n",
      "Surface training t=5513, loss=0.03466690331697464\n",
      "Surface training t=5514, loss=0.04090816341340542\n",
      "Surface training t=5515, loss=0.03404528088867664\n",
      "Surface training t=5516, loss=0.033551376312971115\n",
      "Surface training t=5517, loss=0.03318275325000286\n",
      "Surface training t=5518, loss=0.03809395059943199\n",
      "Surface training t=5519, loss=0.03085271641612053\n",
      "Surface training t=5520, loss=0.036318548023700714\n",
      "Surface training t=5521, loss=0.03207320533692837\n",
      "Surface training t=5522, loss=0.03597197122871876\n",
      "Surface training t=5523, loss=0.03525680676102638\n",
      "Surface training t=5524, loss=0.03559061884880066\n",
      "Surface training t=5525, loss=0.036458589136600494\n",
      "Surface training t=5526, loss=0.03348411247134209\n",
      "Surface training t=5527, loss=0.034080637618899345\n",
      "Surface training t=5528, loss=0.03565859608352184\n",
      "Surface training t=5529, loss=0.03678324446082115\n",
      "Surface training t=5530, loss=0.0335818063467741\n",
      "Surface training t=5531, loss=0.03243808262050152\n",
      "Surface training t=5532, loss=0.03265742212533951\n",
      "Surface training t=5533, loss=0.03372762072831392\n",
      "Surface training t=5534, loss=0.036779530346393585\n",
      "Surface training t=5535, loss=0.03467686101794243\n",
      "Surface training t=5536, loss=0.03430003114044666\n",
      "Surface training t=5537, loss=0.031864835880696774\n",
      "Surface training t=5538, loss=0.03557223454117775\n",
      "Surface training t=5539, loss=0.03321582451462746\n",
      "Surface training t=5540, loss=0.034051062539219856\n",
      "Surface training t=5541, loss=0.03444749489426613\n",
      "Surface training t=5542, loss=0.03408201411366463\n",
      "Surface training t=5543, loss=0.03516673855483532\n",
      "Surface training t=5544, loss=0.03323850594460964\n",
      "Surface training t=5545, loss=0.03213739674538374\n",
      "Surface training t=5546, loss=0.03346744738519192\n",
      "Surface training t=5547, loss=0.03463401272892952\n",
      "Surface training t=5548, loss=0.036804065108299255\n",
      "Surface training t=5549, loss=0.03261725232005119\n",
      "Surface training t=5550, loss=0.03280658274888992\n",
      "Surface training t=5551, loss=0.032416900619864464\n",
      "Surface training t=5552, loss=0.035005468875169754\n",
      "Surface training t=5553, loss=0.035862816497683525\n",
      "Surface training t=5554, loss=0.03425799123942852\n",
      "Surface training t=5555, loss=0.03378703072667122\n",
      "Surface training t=5556, loss=0.03438808023929596\n",
      "Surface training t=5557, loss=0.034204140305519104\n",
      "Surface training t=5558, loss=0.03370929695665836\n",
      "Surface training t=5559, loss=0.03358287923038006\n",
      "Surface training t=5560, loss=0.0339710246771574\n",
      "Surface training t=5561, loss=0.03290853649377823\n",
      "Surface training t=5562, loss=0.03580213710665703\n",
      "Surface training t=5563, loss=0.03406493179500103\n",
      "Surface training t=5564, loss=0.035282667726278305\n",
      "Surface training t=5565, loss=0.03170965705066919\n",
      "Surface training t=5566, loss=0.03240029513835907\n",
      "Surface training t=5567, loss=0.03134857676923275\n",
      "Surface training t=5568, loss=0.03231031075119972\n",
      "Surface training t=5569, loss=0.03297056723386049\n",
      "Surface training t=5570, loss=0.03633556328713894\n",
      "Surface training t=5571, loss=0.0332575598731637\n",
      "Surface training t=5572, loss=0.032189346849918365\n",
      "Surface training t=5573, loss=0.03197421506047249\n",
      "Surface training t=5574, loss=0.032918842509388924\n",
      "Surface training t=5575, loss=0.03405708260834217\n",
      "Surface training t=5576, loss=0.037451233714818954\n",
      "Surface training t=5577, loss=0.03262116760015488\n",
      "Surface training t=5578, loss=0.03222522884607315\n",
      "Surface training t=5579, loss=0.03384651057422161\n",
      "Surface training t=5580, loss=0.037327058613300323\n",
      "Surface training t=5581, loss=0.03233460895717144\n",
      "Surface training t=5582, loss=0.03148710075765848\n",
      "Surface training t=5583, loss=0.03111308440566063\n",
      "Surface training t=5584, loss=0.030350465327501297\n",
      "Surface training t=5585, loss=0.03180418163537979\n",
      "Surface training t=5586, loss=0.031031872145831585\n",
      "Surface training t=5587, loss=0.029390991665422916\n",
      "Surface training t=5588, loss=0.02874130755662918\n",
      "Surface training t=5589, loss=0.02885338757187128\n",
      "Surface training t=5590, loss=0.029077552258968353\n",
      "Surface training t=5591, loss=0.028813323937356472\n",
      "Surface training t=5592, loss=0.028628569096326828\n",
      "Surface training t=5593, loss=0.028921416960656643\n",
      "Surface training t=5594, loss=0.028389963321387768\n",
      "Surface training t=5595, loss=0.02938075177371502\n",
      "Surface training t=5596, loss=0.02845565229654312\n",
      "Surface training t=5597, loss=0.029063817113637924\n",
      "Surface training t=5598, loss=0.029122392646968365\n",
      "Surface training t=5599, loss=0.029666009359061718\n",
      "Surface training t=5600, loss=0.028353331610560417\n",
      "Surface training t=5601, loss=0.028596175834536552\n",
      "Surface training t=5602, loss=0.028146976605057716\n",
      "Surface training t=5603, loss=0.027063586749136448\n",
      "Surface training t=5604, loss=0.028011144138872623\n",
      "Surface training t=5605, loss=0.029176569543778896\n",
      "Surface training t=5606, loss=0.028397098183631897\n",
      "Surface training t=5607, loss=0.028585193678736687\n",
      "Surface training t=5608, loss=0.029764460399746895\n",
      "Surface training t=5609, loss=0.02843210007995367\n",
      "Surface training t=5610, loss=0.029098324477672577\n",
      "Surface training t=5611, loss=0.027624044567346573\n",
      "Surface training t=5612, loss=0.02856653742492199\n",
      "Surface training t=5613, loss=0.02799862250685692\n",
      "Surface training t=5614, loss=0.028528898023068905\n",
      "Surface training t=5615, loss=0.02898152358829975\n",
      "Surface training t=5616, loss=0.028620492666959763\n",
      "Surface training t=5617, loss=0.028887984342873096\n",
      "Surface training t=5618, loss=0.02839698176831007\n",
      "Surface training t=5619, loss=0.02903115004301071\n",
      "Surface training t=5620, loss=0.028799875639379025\n",
      "Surface training t=5621, loss=0.02891472354531288\n",
      "Surface training t=5622, loss=0.02903972566127777\n",
      "Surface training t=5623, loss=0.028099331073462963\n",
      "Surface training t=5624, loss=0.02929149568080902\n",
      "Surface training t=5625, loss=0.029950905591249466\n",
      "Surface training t=5626, loss=0.02949093095958233\n",
      "Surface training t=5627, loss=0.029365116730332375\n",
      "Surface training t=5628, loss=0.02953351940959692\n",
      "Surface training t=5629, loss=0.030281033366918564\n",
      "Surface training t=5630, loss=0.02872330229729414\n",
      "Surface training t=5631, loss=0.028118550777435303\n",
      "Surface training t=5632, loss=0.028774228878319263\n",
      "Surface training t=5633, loss=0.029658441431820393\n",
      "Surface training t=5634, loss=0.028796368278563023\n",
      "Surface training t=5635, loss=0.03019966371357441\n",
      "Surface training t=5636, loss=0.02834082767367363\n",
      "Surface training t=5637, loss=0.027379640378057957\n",
      "Surface training t=5638, loss=0.028089702129364014\n",
      "Surface training t=5639, loss=0.028570623137056828\n",
      "Surface training t=5640, loss=0.02816880401223898\n",
      "Surface training t=5641, loss=0.02846774458885193\n",
      "Surface training t=5642, loss=0.030633004382252693\n",
      "Surface training t=5643, loss=0.028228883631527424\n",
      "Surface training t=5644, loss=0.02971574291586876\n",
      "Surface training t=5645, loss=0.029017881490290165\n",
      "Surface training t=5646, loss=0.029124801978468895\n",
      "Surface training t=5647, loss=0.02784646302461624\n",
      "Surface training t=5648, loss=0.028293579816818237\n",
      "Surface training t=5649, loss=0.02906668931245804\n",
      "Surface training t=5650, loss=0.02844574023038149\n",
      "Surface training t=5651, loss=0.027761968784034252\n",
      "Surface training t=5652, loss=0.02900111209601164\n",
      "Surface training t=5653, loss=0.02833474986255169\n",
      "Surface training t=5654, loss=0.02961023896932602\n",
      "Surface training t=5655, loss=0.02983978670090437\n",
      "Surface training t=5656, loss=0.029011120088398457\n",
      "Surface training t=5657, loss=0.030671048909425735\n",
      "Surface training t=5658, loss=0.028473814949393272\n",
      "Surface training t=5659, loss=0.02962113358080387\n",
      "Surface training t=5660, loss=0.028689011000096798\n",
      "Surface training t=5661, loss=0.028911590576171875\n",
      "Surface training t=5662, loss=0.029140440747141838\n",
      "Surface training t=5663, loss=0.029405923560261726\n",
      "Surface training t=5664, loss=0.028249764814972878\n",
      "Surface training t=5665, loss=0.028048194013535976\n",
      "Surface training t=5666, loss=0.028015922755002975\n",
      "Surface training t=5667, loss=0.02878667414188385\n",
      "Surface training t=5668, loss=0.02867626305669546\n",
      "Surface training t=5669, loss=0.028309505432844162\n",
      "Surface training t=5670, loss=0.02928096242249012\n",
      "Surface training t=5671, loss=0.02891680784523487\n",
      "Surface training t=5672, loss=0.0297262417152524\n",
      "Surface training t=5673, loss=0.02995650190860033\n",
      "Surface training t=5674, loss=0.027679228223860264\n",
      "Surface training t=5675, loss=0.0281425341963768\n",
      "Surface training t=5676, loss=0.029915890656411648\n",
      "Surface training t=5677, loss=0.029009982012212276\n",
      "Surface training t=5678, loss=0.028128444217145443\n",
      "Surface training t=5679, loss=0.02875945344567299\n",
      "Surface training t=5680, loss=0.028648072853684425\n",
      "Surface training t=5681, loss=0.02875049877911806\n",
      "Surface training t=5682, loss=0.027841034345328808\n",
      "Surface training t=5683, loss=0.02957984246313572\n",
      "Surface training t=5684, loss=0.02979138307273388\n",
      "Surface training t=5685, loss=0.02754528820514679\n",
      "Surface training t=5686, loss=0.029090060852468014\n",
      "Surface training t=5687, loss=0.028603811748325825\n",
      "Surface training t=5688, loss=0.026959622278809547\n",
      "Surface training t=5689, loss=0.02817402221262455\n",
      "Surface training t=5690, loss=0.02816220559179783\n",
      "Surface training t=5691, loss=0.028008791618049145\n",
      "Surface training t=5692, loss=0.03031998686492443\n",
      "Surface training t=5693, loss=0.029680160805583\n",
      "Surface training t=5694, loss=0.030396890826523304\n",
      "Surface training t=5695, loss=0.03177543729543686\n",
      "Surface training t=5696, loss=0.03402589447796345\n",
      "Surface training t=5697, loss=0.03359615057706833\n",
      "Surface training t=5698, loss=0.033072478137910366\n",
      "Surface training t=5699, loss=0.033277736976742744\n",
      "Surface training t=5700, loss=0.03214479982852936\n",
      "Surface training t=5701, loss=0.03200940415263176\n",
      "Surface training t=5702, loss=0.03169238194823265\n",
      "Surface training t=5703, loss=0.03283030726015568\n",
      "Surface training t=5704, loss=0.03214068338274956\n",
      "Surface training t=5705, loss=0.03171075880527496\n",
      "Surface training t=5706, loss=0.03319086506962776\n",
      "Surface training t=5707, loss=0.03584601730108261\n",
      "Surface training t=5708, loss=0.03782364912331104\n",
      "Surface training t=5709, loss=0.031339939683675766\n",
      "Surface training t=5710, loss=0.029776102863252163\n",
      "Surface training t=5711, loss=0.03192588686943054\n",
      "Surface training t=5712, loss=0.03790942393243313\n",
      "Surface training t=5713, loss=0.03338766656816006\n",
      "Surface training t=5714, loss=0.03260054998099804\n",
      "Surface training t=5715, loss=0.03128251060843468\n",
      "Surface training t=5716, loss=0.032831355929374695\n",
      "Surface training t=5717, loss=0.03697650134563446\n",
      "Surface training t=5718, loss=0.03262604400515556\n",
      "Surface training t=5719, loss=0.03417298197746277\n",
      "Surface training t=5720, loss=0.029933704994618893\n",
      "Surface training t=5721, loss=0.030642748810350895\n",
      "Surface training t=5722, loss=0.03267524018883705\n",
      "Surface training t=5723, loss=0.03558631055057049\n",
      "Surface training t=5724, loss=0.03765105456113815\n",
      "Surface training t=5725, loss=0.03123169858008623\n",
      "Surface training t=5726, loss=0.03298123553395271\n",
      "Surface training t=5727, loss=0.030738450586795807\n",
      "Surface training t=5728, loss=0.03328135050833225\n",
      "Surface training t=5729, loss=0.03418741747736931\n",
      "Surface training t=5730, loss=0.03226267732679844\n",
      "Surface training t=5731, loss=0.03553155995905399\n",
      "Surface training t=5732, loss=0.030396685004234314\n",
      "Surface training t=5733, loss=0.029881549067795277\n",
      "Surface training t=5734, loss=0.03228219039738178\n",
      "Surface training t=5735, loss=0.036516277119517326\n",
      "Surface training t=5736, loss=0.03443767596036196\n",
      "Surface training t=5737, loss=0.03100617416203022\n",
      "Surface training t=5738, loss=0.033405959606170654\n",
      "Surface training t=5739, loss=0.03388391621410847\n",
      "Surface training t=5740, loss=0.030698850750923157\n",
      "Surface training t=5741, loss=0.031416311860084534\n",
      "Surface training t=5742, loss=0.03414802625775337\n",
      "Surface training t=5743, loss=0.03248150460422039\n",
      "Surface training t=5744, loss=0.03145873174071312\n",
      "Surface training t=5745, loss=0.03268796019256115\n",
      "Surface training t=5746, loss=0.030702853575348854\n",
      "Surface training t=5747, loss=0.03481400944292545\n",
      "Surface training t=5748, loss=0.033853184431791306\n",
      "Surface training t=5749, loss=0.03330788016319275\n",
      "Surface training t=5750, loss=0.030289608985185623\n",
      "Surface training t=5751, loss=0.03180529549717903\n",
      "Surface training t=5752, loss=0.03428982384502888\n",
      "Surface training t=5753, loss=0.03403091803193092\n",
      "Surface training t=5754, loss=0.034107498824596405\n",
      "Surface training t=5755, loss=0.030509687960147858\n",
      "Surface training t=5756, loss=0.03074994869530201\n",
      "Surface training t=5757, loss=0.03521396964788437\n",
      "Surface training t=5758, loss=0.032196810469031334\n",
      "Surface training t=5759, loss=0.034721989184617996\n",
      "Surface training t=5760, loss=0.03078547865152359\n",
      "Surface training t=5761, loss=0.0320607740432024\n",
      "Surface training t=5762, loss=0.034123362973332405\n",
      "Surface training t=5763, loss=0.03304683044552803\n",
      "Surface training t=5764, loss=0.03150675445795059\n",
      "Surface training t=5765, loss=0.033805496990680695\n",
      "Surface training t=5766, loss=0.03203036729246378\n",
      "Surface training t=5767, loss=0.03159801475703716\n",
      "Surface training t=5768, loss=0.031567047350108624\n",
      "Surface training t=5769, loss=0.03113013692200184\n",
      "Surface training t=5770, loss=0.0331057645380497\n",
      "Surface training t=5771, loss=0.03452824428677559\n",
      "Surface training t=5772, loss=0.03196978196501732\n",
      "Surface training t=5773, loss=0.031174220144748688\n",
      "Surface training t=5774, loss=0.03054423350840807\n",
      "Surface training t=5775, loss=0.03277517203241587\n",
      "Surface training t=5776, loss=0.03143890295177698\n",
      "Surface training t=5777, loss=0.032946811988949776\n",
      "Surface training t=5778, loss=0.032087260857224464\n",
      "Surface training t=5779, loss=0.033503491431474686\n",
      "Surface training t=5780, loss=0.03221309557557106\n",
      "Surface training t=5781, loss=0.030259559862315655\n",
      "Surface training t=5782, loss=0.03265023045241833\n",
      "Surface training t=5783, loss=0.033032964915037155\n",
      "Surface training t=5784, loss=0.0323637630790472\n",
      "Surface training t=5785, loss=0.032264918088912964\n",
      "Surface training t=5786, loss=0.03076135367155075\n",
      "Surface training t=5787, loss=0.03264095075428486\n",
      "Surface training t=5788, loss=0.03230743855237961\n",
      "Surface training t=5789, loss=0.03336584381759167\n",
      "Surface training t=5790, loss=0.03382771089673042\n",
      "Surface training t=5791, loss=0.031621319241821766\n",
      "Surface training t=5792, loss=0.031510455533862114\n",
      "Surface training t=5793, loss=0.030389519408345222\n",
      "Surface training t=5794, loss=0.03205007314682007\n",
      "Surface training t=5795, loss=0.033679174259305\n",
      "Surface training t=5796, loss=0.030985938385128975\n",
      "Surface training t=5797, loss=0.031203068792819977\n",
      "Surface training t=5798, loss=0.031095311976969242\n",
      "Surface training t=5799, loss=0.03289583791047335\n",
      "Surface training t=5800, loss=0.033939603716135025\n",
      "Surface training t=5801, loss=0.03214247524738312\n",
      "Surface training t=5802, loss=0.02936652395874262\n",
      "Surface training t=5803, loss=0.02873540949076414\n",
      "Surface training t=5804, loss=0.029866044409573078\n",
      "Surface training t=5805, loss=0.030688108876347542\n",
      "Surface training t=5806, loss=0.03243216034024954\n",
      "Surface training t=5807, loss=0.03395812585949898\n",
      "Surface training t=5808, loss=0.031592980958521366\n",
      "Surface training t=5809, loss=0.03006109967827797\n",
      "Surface training t=5810, loss=0.02895297110080719\n",
      "Surface training t=5811, loss=0.03094707801938057\n",
      "Surface training t=5812, loss=0.029839596711099148\n",
      "Surface training t=5813, loss=0.031713676638901234\n",
      "Surface training t=5814, loss=0.031691231764853\n",
      "Surface training t=5815, loss=0.03339312970638275\n",
      "Surface training t=5816, loss=0.03264186717569828\n",
      "Surface training t=5817, loss=0.030706669203937054\n",
      "Surface training t=5818, loss=0.030052000656723976\n",
      "Surface training t=5819, loss=0.02917622961103916\n",
      "Surface training t=5820, loss=0.029480907134711742\n",
      "Surface training t=5821, loss=0.02939765900373459\n",
      "Surface training t=5822, loss=0.02819105051457882\n",
      "Surface training t=5823, loss=0.027698000892996788\n",
      "Surface training t=5824, loss=0.02771494910120964\n",
      "Surface training t=5825, loss=0.02960959915071726\n",
      "Surface training t=5826, loss=0.029170014895498753\n",
      "Surface training t=5827, loss=0.027039943262934685\n",
      "Surface training t=5828, loss=0.02790637593716383\n",
      "Surface training t=5829, loss=0.028309489600360394\n",
      "Surface training t=5830, loss=0.026503325439989567\n",
      "Surface training t=5831, loss=0.027686484158039093\n",
      "Surface training t=5832, loss=0.027271782979369164\n",
      "Surface training t=5833, loss=0.028550203889608383\n",
      "Surface training t=5834, loss=0.0273221917450428\n",
      "Surface training t=5835, loss=0.02720469329506159\n",
      "Surface training t=5836, loss=0.027151805348694324\n",
      "Surface training t=5837, loss=0.02710562665015459\n",
      "Surface training t=5838, loss=0.027507285587489605\n",
      "Surface training t=5839, loss=0.028054901398718357\n",
      "Surface training t=5840, loss=0.02646766323596239\n",
      "Surface training t=5841, loss=0.027284769341349602\n",
      "Surface training t=5842, loss=0.027295446023344994\n",
      "Surface training t=5843, loss=0.025963895954191685\n",
      "Surface training t=5844, loss=0.02682187221944332\n",
      "Surface training t=5845, loss=0.025928284972906113\n",
      "Surface training t=5846, loss=0.02655656449496746\n",
      "Surface training t=5847, loss=0.026953432708978653\n",
      "Surface training t=5848, loss=0.026608828455209732\n",
      "Surface training t=5849, loss=0.02709519863128662\n",
      "Surface training t=5850, loss=0.026106039993464947\n",
      "Surface training t=5851, loss=0.02734449692070484\n",
      "Surface training t=5852, loss=0.026364044286310673\n",
      "Surface training t=5853, loss=0.02765697054564953\n",
      "Surface training t=5854, loss=0.028394320979714394\n",
      "Surface training t=5855, loss=0.028463371098041534\n",
      "Surface training t=5856, loss=0.02701505273580551\n",
      "Surface training t=5857, loss=0.028800738975405693\n",
      "Surface training t=5858, loss=0.02883818279951811\n",
      "Surface training t=5859, loss=0.02939292322844267\n",
      "Surface training t=5860, loss=0.02887233067303896\n",
      "Surface training t=5861, loss=0.027844946831464767\n",
      "Surface training t=5862, loss=0.028183535672724247\n",
      "Surface training t=5863, loss=0.0285126194357872\n",
      "Surface training t=5864, loss=0.03007448185235262\n",
      "Surface training t=5865, loss=0.03117997944355011\n",
      "Surface training t=5866, loss=0.031743478029966354\n",
      "Surface training t=5867, loss=0.0333410557359457\n",
      "Surface training t=5868, loss=0.030800572596490383\n",
      "Surface training t=5869, loss=0.03031493816524744\n",
      "Surface training t=5870, loss=0.031847063452005386\n",
      "Surface training t=5871, loss=0.03463002108037472\n",
      "Surface training t=5872, loss=0.03240662906318903\n",
      "Surface training t=5873, loss=0.03131367638707161\n",
      "Surface training t=5874, loss=0.03171355929225683\n",
      "Surface training t=5875, loss=0.03061429038643837\n",
      "Surface training t=5876, loss=0.030034520663321018\n",
      "Surface training t=5877, loss=0.030692442320287228\n",
      "Surface training t=5878, loss=0.03101375699043274\n",
      "Surface training t=5879, loss=0.030167927034199238\n",
      "Surface training t=5880, loss=0.02829750534147024\n",
      "Surface training t=5881, loss=0.02847625222057104\n",
      "Surface training t=5882, loss=0.029344103299081326\n",
      "Surface training t=5883, loss=0.029613202437758446\n",
      "Surface training t=5884, loss=0.02962790336459875\n",
      "Surface training t=5885, loss=0.029317200183868408\n",
      "Surface training t=5886, loss=0.029518062248826027\n",
      "Surface training t=5887, loss=0.028564576990902424\n",
      "Surface training t=5888, loss=0.028393241576850414\n",
      "Surface training t=5889, loss=0.028018460609018803\n",
      "Surface training t=5890, loss=0.02790316380560398\n",
      "Surface training t=5891, loss=0.028661951422691345\n",
      "Surface training t=5892, loss=0.027598711661994457\n",
      "Surface training t=5893, loss=0.02705511264503002\n",
      "Surface training t=5894, loss=0.026229928247630596\n",
      "Surface training t=5895, loss=0.025888649746775627\n",
      "Surface training t=5896, loss=0.027284960262477398\n",
      "Surface training t=5897, loss=0.02672561164945364\n",
      "Surface training t=5898, loss=0.02605200558900833\n",
      "Surface training t=5899, loss=0.025941066443920135\n",
      "Surface training t=5900, loss=0.026872788555920124\n",
      "Surface training t=5901, loss=0.02662890125066042\n",
      "Surface training t=5902, loss=0.026794048957526684\n",
      "Surface training t=5903, loss=0.02667921781539917\n",
      "Surface training t=5904, loss=0.027574121952056885\n",
      "Surface training t=5905, loss=0.026962018571794033\n",
      "Surface training t=5906, loss=0.025705702602863312\n",
      "Surface training t=5907, loss=0.0258245300501585\n",
      "Surface training t=5908, loss=0.02734152041375637\n",
      "Surface training t=5909, loss=0.0271591953933239\n",
      "Surface training t=5910, loss=0.028748931363224983\n",
      "Surface training t=5911, loss=0.026627875864505768\n",
      "Surface training t=5912, loss=0.028492498211562634\n",
      "Surface training t=5913, loss=0.029404720291495323\n",
      "Surface training t=5914, loss=0.029964493587613106\n",
      "Surface training t=5915, loss=0.03018041979521513\n",
      "Surface training t=5916, loss=0.032258166000247\n",
      "Surface training t=5917, loss=0.030740978196263313\n",
      "Surface training t=5918, loss=0.029914085753262043\n",
      "Surface training t=5919, loss=0.03067757934331894\n",
      "Surface training t=5920, loss=0.03640059195458889\n",
      "Surface training t=5921, loss=0.028927816078066826\n",
      "Surface training t=5922, loss=0.03089889045804739\n",
      "Surface training t=5923, loss=0.02777519728988409\n",
      "Surface training t=5924, loss=0.031422861851751804\n",
      "Surface training t=5925, loss=0.032801304012537\n",
      "Surface training t=5926, loss=0.032310811802744865\n",
      "Surface training t=5927, loss=0.03368503041565418\n",
      "Surface training t=5928, loss=0.029797430150210857\n",
      "Surface training t=5929, loss=0.031476921401917934\n",
      "Surface training t=5930, loss=0.02972393576055765\n",
      "Surface training t=5931, loss=0.02956361137330532\n",
      "Surface training t=5932, loss=0.03298414498567581\n",
      "Surface training t=5933, loss=0.029700598679482937\n",
      "Surface training t=5934, loss=0.03435148298740387\n",
      "Surface training t=5935, loss=0.028739208355545998\n",
      "Surface training t=5936, loss=0.029975670389831066\n",
      "Surface training t=5937, loss=0.03013954684138298\n",
      "Surface training t=5938, loss=0.031227371655404568\n",
      "Surface training t=5939, loss=0.03274003602564335\n",
      "Surface training t=5940, loss=0.031087078154087067\n",
      "Surface training t=5941, loss=0.03141194488853216\n",
      "Surface training t=5942, loss=0.028597489930689335\n",
      "Surface training t=5943, loss=0.030624156817793846\n",
      "Surface training t=5944, loss=0.03245753422379494\n",
      "Surface training t=5945, loss=0.030473182909190655\n",
      "Surface training t=5946, loss=0.03090044390410185\n",
      "Surface training t=5947, loss=0.030831663869321346\n",
      "Surface training t=5948, loss=0.030031700618565083\n",
      "Surface training t=5949, loss=0.029518771916627884\n",
      "Surface training t=5950, loss=0.029454774223268032\n",
      "Surface training t=5951, loss=0.029282542876899242\n",
      "Surface training t=5952, loss=0.03014260996133089\n",
      "Surface training t=5953, loss=0.03335604816675186\n",
      "Surface training t=5954, loss=0.03361911326646805\n",
      "Surface training t=5955, loss=0.02847306802868843\n",
      "Surface training t=5956, loss=0.02837031614035368\n",
      "Surface training t=5957, loss=0.029677055776119232\n",
      "Surface training t=5958, loss=0.031906941905617714\n",
      "Surface training t=5959, loss=0.031885101459920406\n",
      "Surface training t=5960, loss=0.031876953318715096\n",
      "Surface training t=5961, loss=0.03243306465446949\n",
      "Surface training t=5962, loss=0.028590021654963493\n",
      "Surface training t=5963, loss=0.028094734996557236\n",
      "Surface training t=5964, loss=0.030856330879032612\n",
      "Surface training t=5965, loss=0.03240280505269766\n",
      "Surface training t=5966, loss=0.03139477875083685\n",
      "Surface training t=5967, loss=0.029847783036530018\n",
      "Surface training t=5968, loss=0.028682831674814224\n",
      "Surface training t=5969, loss=0.030963975004851818\n",
      "Surface training t=5970, loss=0.0332530215382576\n",
      "Surface training t=5971, loss=0.031748589128255844\n",
      "Surface training t=5972, loss=0.029953666031360626\n",
      "Surface training t=5973, loss=0.028041431680321693\n",
      "Surface training t=5974, loss=0.028826302848756313\n",
      "Surface training t=5975, loss=0.031038704328238964\n",
      "Surface training t=5976, loss=0.03364336118102074\n",
      "Surface training t=5977, loss=0.029278206638991833\n",
      "Surface training t=5978, loss=0.031277854926884174\n",
      "Surface training t=5979, loss=0.02800770942121744\n",
      "Surface training t=5980, loss=0.030852111987769604\n",
      "Surface training t=5981, loss=0.030835259705781937\n",
      "Surface training t=5982, loss=0.03183858469128609\n",
      "Surface training t=5983, loss=0.029924612492322922\n",
      "Surface training t=5984, loss=0.029287093318998814\n",
      "Surface training t=5985, loss=0.029773708432912827\n",
      "Surface training t=5986, loss=0.030859965831041336\n",
      "Surface training t=5987, loss=0.030643398873507977\n",
      "Surface training t=5988, loss=0.030615809373557568\n",
      "Surface training t=5989, loss=0.029708183370530605\n",
      "Surface training t=5990, loss=0.029332990758121014\n",
      "Surface training t=5991, loss=0.029428178444504738\n",
      "Surface training t=5992, loss=0.029517080634832382\n",
      "Surface training t=5993, loss=0.03067485522478819\n",
      "Surface training t=5994, loss=0.0321105532348156\n",
      "Surface training t=5995, loss=0.028580603189766407\n",
      "Surface training t=5996, loss=0.028508862480521202\n",
      "Surface training t=5997, loss=0.03033966477960348\n",
      "Surface training t=5998, loss=0.033121801912784576\n",
      "Surface training t=5999, loss=0.029184285551309586\n",
      "Surface training t=6000, loss=0.030029023997485638\n",
      "Surface training t=6001, loss=0.028580402955412865\n",
      "Surface training t=6002, loss=0.031801387667655945\n",
      "Surface training t=6003, loss=0.02968535479158163\n",
      "Surface training t=6004, loss=0.030175224877893925\n",
      "Surface training t=6005, loss=0.029088623821735382\n",
      "Surface training t=6006, loss=0.03070985246449709\n",
      "Surface training t=6007, loss=0.029620754532516003\n",
      "Surface training t=6008, loss=0.029237770475447178\n",
      "Surface training t=6009, loss=0.032007452100515366\n",
      "Surface training t=6010, loss=0.029927050694823265\n",
      "Surface training t=6011, loss=0.028639343567192554\n",
      "Surface training t=6012, loss=0.02932781632989645\n",
      "Surface training t=6013, loss=0.030001895502209663\n",
      "Surface training t=6014, loss=0.031292323023080826\n",
      "Surface training t=6015, loss=0.03127374313771725\n",
      "Surface training t=6016, loss=0.030496732331812382\n",
      "Surface training t=6017, loss=0.028279013000428677\n",
      "Surface training t=6018, loss=0.02971456479281187\n",
      "Surface training t=6019, loss=0.028574478812515736\n",
      "Surface training t=6020, loss=0.029154479503631592\n",
      "Surface training t=6021, loss=0.02887664455920458\n",
      "Surface training t=6022, loss=0.030601168051362038\n",
      "Surface training t=6023, loss=0.03070354461669922\n",
      "Surface training t=6024, loss=0.029816082678735256\n",
      "Surface training t=6025, loss=0.02984596975147724\n",
      "Surface training t=6026, loss=0.029380270279943943\n",
      "Surface training t=6027, loss=0.030163840390741825\n",
      "Surface training t=6028, loss=0.029886096715927124\n",
      "Surface training t=6029, loss=0.027976051904261112\n",
      "Surface training t=6030, loss=0.030902359634637833\n",
      "Surface training t=6031, loss=0.028345514088869095\n",
      "Surface training t=6032, loss=0.03149918466806412\n",
      "Surface training t=6033, loss=0.028535871766507626\n",
      "Surface training t=6034, loss=0.029239919036626816\n",
      "Surface training t=6035, loss=0.028856322169303894\n",
      "Surface training t=6036, loss=0.029175211675465107\n",
      "Surface training t=6037, loss=0.02946796454489231\n",
      "Surface training t=6038, loss=0.028869297355413437\n",
      "Surface training t=6039, loss=0.030859637074172497\n",
      "Surface training t=6040, loss=0.029353843070566654\n",
      "Surface training t=6041, loss=0.028432265855371952\n",
      "Surface training t=6042, loss=0.02818957157433033\n",
      "Surface training t=6043, loss=0.029806029051542282\n",
      "Surface training t=6044, loss=0.03147491440176964\n",
      "Surface training t=6045, loss=0.030397328548133373\n",
      "Surface training t=6046, loss=0.029382754117250443\n",
      "Surface training t=6047, loss=0.028113889507949352\n",
      "Surface training t=6048, loss=0.028457697480916977\n",
      "Surface training t=6049, loss=0.029223961755633354\n",
      "Surface training t=6050, loss=0.02939668484032154\n",
      "Surface training t=6051, loss=0.028692745603621006\n",
      "Surface training t=6052, loss=0.028358077630400658\n",
      "Surface training t=6053, loss=0.028522723354399204\n",
      "Surface training t=6054, loss=0.029869625344872475\n",
      "Surface training t=6055, loss=0.03050694242119789\n",
      "Surface training t=6056, loss=0.029882872477173805\n",
      "Surface training t=6057, loss=0.027265377342700958\n",
      "Surface training t=6058, loss=0.02804913278669119\n",
      "Surface training t=6059, loss=0.03046957217156887\n",
      "Surface training t=6060, loss=0.03324451670050621\n",
      "Surface training t=6061, loss=0.028329317457973957\n",
      "Surface training t=6062, loss=0.029459289275109768\n",
      "Surface training t=6063, loss=0.02823398169130087\n",
      "Surface training t=6064, loss=0.02745735924690962\n",
      "Surface training t=6065, loss=0.02696624957025051\n",
      "Surface training t=6066, loss=0.02725483849644661\n",
      "Surface training t=6067, loss=0.027161136269569397\n",
      "Surface training t=6068, loss=0.03026246279478073\n",
      "Surface training t=6069, loss=0.03298269957304001\n",
      "Surface training t=6070, loss=0.03001276310533285\n",
      "Surface training t=6071, loss=0.027881581336259842\n",
      "Surface training t=6072, loss=0.02758992649614811\n",
      "Surface training t=6073, loss=0.02840429823845625\n",
      "Surface training t=6074, loss=0.031436050310730934\n",
      "Surface training t=6075, loss=0.029845775105059147\n",
      "Surface training t=6076, loss=0.03004596009850502\n",
      "Surface training t=6077, loss=0.026989937759935856\n",
      "Surface training t=6078, loss=0.027841518633067608\n",
      "Surface training t=6079, loss=0.027677482925355434\n",
      "Surface training t=6080, loss=0.030706663616001606\n",
      "Surface training t=6081, loss=0.03164177946746349\n",
      "Surface training t=6082, loss=0.028205711394548416\n",
      "Surface training t=6083, loss=0.02903007809072733\n",
      "Surface training t=6084, loss=0.02822146564722061\n",
      "Surface training t=6085, loss=0.02830121386796236\n",
      "Surface training t=6086, loss=0.029804189689457417\n",
      "Surface training t=6087, loss=0.02931620553135872\n",
      "Surface training t=6088, loss=0.030356635339558125\n",
      "Surface training t=6089, loss=0.028047019615769386\n",
      "Surface training t=6090, loss=0.028023977763950825\n",
      "Surface training t=6091, loss=0.03007107973098755\n",
      "Surface training t=6092, loss=0.030557841062545776\n",
      "Surface training t=6093, loss=0.0282998438924551\n",
      "Surface training t=6094, loss=0.028177802450954914\n",
      "Surface training t=6095, loss=0.029222802259027958\n",
      "Surface training t=6096, loss=0.029738619923591614\n",
      "Surface training t=6097, loss=0.029827644117176533\n",
      "Surface training t=6098, loss=0.028869363479316235\n",
      "Surface training t=6099, loss=0.028979578986763954\n",
      "Surface training t=6100, loss=0.029460645280778408\n",
      "Surface training t=6101, loss=0.027717262506484985\n",
      "Surface training t=6102, loss=0.028137569315731525\n",
      "Surface training t=6103, loss=0.02729873824864626\n",
      "Surface training t=6104, loss=0.029016255401074886\n",
      "Surface training t=6105, loss=0.02959220390766859\n",
      "Surface training t=6106, loss=0.03085051104426384\n",
      "Surface training t=6107, loss=0.027949243783950806\n",
      "Surface training t=6108, loss=0.026522076688706875\n",
      "Surface training t=6109, loss=0.028191798366606236\n",
      "Surface training t=6110, loss=0.02938660327345133\n",
      "Surface training t=6111, loss=0.03062517661601305\n",
      "Surface training t=6112, loss=0.027975193224847317\n",
      "Surface training t=6113, loss=0.026712297461926937\n",
      "Surface training t=6114, loss=0.028314677067101002\n",
      "Surface training t=6115, loss=0.02742013707756996\n",
      "Surface training t=6116, loss=0.02947379369288683\n",
      "Surface training t=6117, loss=0.028694230131804943\n",
      "Surface training t=6118, loss=0.027351018972694874\n",
      "Surface training t=6119, loss=0.028079153038561344\n",
      "Surface training t=6120, loss=0.028063293546438217\n",
      "Surface training t=6121, loss=0.031342050060629845\n",
      "Surface training t=6122, loss=0.029427647590637207\n",
      "Surface training t=6123, loss=0.02828090451657772\n",
      "Surface training t=6124, loss=0.02889304980635643\n",
      "Surface training t=6125, loss=0.02883277926594019\n",
      "Surface training t=6126, loss=0.030158260837197304\n",
      "Surface training t=6127, loss=0.027685160748660564\n",
      "Surface training t=6128, loss=0.029189450666308403\n",
      "Surface training t=6129, loss=0.0288023529574275\n",
      "Surface training t=6130, loss=0.029788607731461525\n",
      "Surface training t=6131, loss=0.027890987694263458\n",
      "Surface training t=6132, loss=0.028336104936897755\n",
      "Surface training t=6133, loss=0.02793939784169197\n",
      "Surface training t=6134, loss=0.028881982900202274\n",
      "Surface training t=6135, loss=0.028498560190200806\n",
      "Surface training t=6136, loss=0.02962834108620882\n",
      "Surface training t=6137, loss=0.02982748206704855\n",
      "Surface training t=6138, loss=0.028750049881637096\n",
      "Surface training t=6139, loss=0.029196761548519135\n",
      "Surface training t=6140, loss=0.028272694908082485\n",
      "Surface training t=6141, loss=0.027387427166104317\n",
      "Surface training t=6142, loss=0.028199994936585426\n",
      "Surface training t=6143, loss=0.029922448098659515\n",
      "Surface training t=6144, loss=0.030862148851156235\n",
      "Surface training t=6145, loss=0.028233269229531288\n",
      "Surface training t=6146, loss=0.026628868654370308\n",
      "Surface training t=6147, loss=0.026476205326616764\n",
      "Surface training t=6148, loss=0.027606099843978882\n",
      "Surface training t=6149, loss=0.028696974739432335\n",
      "Surface training t=6150, loss=0.0294429250061512\n",
      "Surface training t=6151, loss=0.027552414685487747\n",
      "Surface training t=6152, loss=0.028307421132922173\n",
      "Surface training t=6153, loss=0.030526219867169857\n",
      "Surface training t=6154, loss=0.027823850512504578\n",
      "Surface training t=6155, loss=0.025774777866899967\n",
      "Surface training t=6156, loss=0.02530045248568058\n",
      "Surface training t=6157, loss=0.02545149903744459\n",
      "Surface training t=6158, loss=0.02564152702689171\n",
      "Surface training t=6159, loss=0.02506042644381523\n",
      "Surface training t=6160, loss=0.02485793363302946\n",
      "Surface training t=6161, loss=0.024479436688125134\n",
      "Surface training t=6162, loss=0.024402672424912453\n",
      "Surface training t=6163, loss=0.02461692877113819\n",
      "Surface training t=6164, loss=0.025686434470117092\n",
      "Surface training t=6165, loss=0.024764420464634895\n",
      "Surface training t=6166, loss=0.024103522300720215\n",
      "Surface training t=6167, loss=0.02484150417149067\n",
      "Surface training t=6168, loss=0.024793959222733974\n",
      "Surface training t=6169, loss=0.02429231908172369\n",
      "Surface training t=6170, loss=0.024221884086728096\n",
      "Surface training t=6171, loss=0.024517311714589596\n",
      "Surface training t=6172, loss=0.02305766474455595\n",
      "Surface training t=6173, loss=0.024034286849200726\n",
      "Surface training t=6174, loss=0.02418164350092411\n",
      "Surface training t=6175, loss=0.02468752395361662\n",
      "Surface training t=6176, loss=0.02418718207627535\n",
      "Surface training t=6177, loss=0.023951307870447636\n",
      "Surface training t=6178, loss=0.023585048504173756\n",
      "Surface training t=6179, loss=0.024188918992877007\n",
      "Surface training t=6180, loss=0.024245102889835835\n",
      "Surface training t=6181, loss=0.024328299798071384\n",
      "Surface training t=6182, loss=0.023888195864856243\n",
      "Surface training t=6183, loss=0.024751094169914722\n",
      "Surface training t=6184, loss=0.023835444822907448\n",
      "Surface training t=6185, loss=0.02428622357547283\n",
      "Surface training t=6186, loss=0.024248016998171806\n",
      "Surface training t=6187, loss=0.023988154716789722\n",
      "Surface training t=6188, loss=0.02413427084684372\n",
      "Surface training t=6189, loss=0.023990574292838573\n",
      "Surface training t=6190, loss=0.023838769644498825\n",
      "Surface training t=6191, loss=0.024988023564219475\n",
      "Surface training t=6192, loss=0.024033340625464916\n",
      "Surface training t=6193, loss=0.024217871949076653\n",
      "Surface training t=6194, loss=0.025302348658442497\n",
      "Surface training t=6195, loss=0.023647576570510864\n",
      "Surface training t=6196, loss=0.02370390295982361\n",
      "Surface training t=6197, loss=0.023826424032449722\n",
      "Surface training t=6198, loss=0.024708195589482784\n",
      "Surface training t=6199, loss=0.02407477516680956\n",
      "Surface training t=6200, loss=0.024387436918914318\n",
      "Surface training t=6201, loss=0.02455138321965933\n",
      "Surface training t=6202, loss=0.02463157568126917\n",
      "Surface training t=6203, loss=0.024455959908664227\n",
      "Surface training t=6204, loss=0.02414698153734207\n",
      "Surface training t=6205, loss=0.023498302325606346\n",
      "Surface training t=6206, loss=0.024574209935963154\n",
      "Surface training t=6207, loss=0.023928229697048664\n",
      "Surface training t=6208, loss=0.024318424053490162\n",
      "Surface training t=6209, loss=0.024311608634889126\n",
      "Surface training t=6210, loss=0.02311508823186159\n",
      "Surface training t=6211, loss=0.02316232491284609\n",
      "Surface training t=6212, loss=0.023362820968031883\n",
      "Surface training t=6213, loss=0.02466374821960926\n",
      "Surface training t=6214, loss=0.0237039215862751\n",
      "Surface training t=6215, loss=0.02319151908159256\n",
      "Surface training t=6216, loss=0.023681473918259144\n",
      "Surface training t=6217, loss=0.023261492140591145\n",
      "Surface training t=6218, loss=0.024478882551193237\n",
      "Surface training t=6219, loss=0.0230304142460227\n",
      "Surface training t=6220, loss=0.024016279727220535\n",
      "Surface training t=6221, loss=0.023967642337083817\n",
      "Surface training t=6222, loss=0.02417598944157362\n",
      "Surface training t=6223, loss=0.023734405636787415\n",
      "Surface training t=6224, loss=0.025801902636885643\n",
      "Surface training t=6225, loss=0.02414530422538519\n",
      "Surface training t=6226, loss=0.02392776496708393\n",
      "Surface training t=6227, loss=0.02298618108034134\n",
      "Surface training t=6228, loss=0.023056240752339363\n",
      "Surface training t=6229, loss=0.024074099957942963\n",
      "Surface training t=6230, loss=0.024237602949142456\n",
      "Surface training t=6231, loss=0.02333164121955633\n",
      "Surface training t=6232, loss=0.025443208403885365\n",
      "Surface training t=6233, loss=0.02393357641994953\n",
      "Surface training t=6234, loss=0.023857123218476772\n",
      "Surface training t=6235, loss=0.02576814591884613\n",
      "Surface training t=6236, loss=0.0250624381005764\n",
      "Surface training t=6237, loss=0.02401208784431219\n",
      "Surface training t=6238, loss=0.025660786777734756\n",
      "Surface training t=6239, loss=0.024467330425977707\n",
      "Surface training t=6240, loss=0.02361898124217987\n",
      "Surface training t=6241, loss=0.022615314461290836\n",
      "Surface training t=6242, loss=0.024102787487208843\n",
      "Surface training t=6243, loss=0.02369238156825304\n",
      "Surface training t=6244, loss=0.023105865344405174\n",
      "Surface training t=6245, loss=0.023426675237715244\n",
      "Surface training t=6246, loss=0.022996211424469948\n",
      "Surface training t=6247, loss=0.024079736322164536\n",
      "Surface training t=6248, loss=0.02328538056463003\n",
      "Surface training t=6249, loss=0.023679407313466072\n",
      "Surface training t=6250, loss=0.02289534080773592\n",
      "Surface training t=6251, loss=0.023181295953691006\n",
      "Surface training t=6252, loss=0.023577211424708366\n",
      "Surface training t=6253, loss=0.0245792455971241\n",
      "Surface training t=6254, loss=0.023523349314928055\n",
      "Surface training t=6255, loss=0.024192131124436855\n",
      "Surface training t=6256, loss=0.02325137611478567\n",
      "Surface training t=6257, loss=0.024295425973832607\n",
      "Surface training t=6258, loss=0.024151496589183807\n",
      "Surface training t=6259, loss=0.02386206015944481\n",
      "Surface training t=6260, loss=0.024902645498514175\n",
      "Surface training t=6261, loss=0.02306420262902975\n",
      "Surface training t=6262, loss=0.02419799193739891\n",
      "Surface training t=6263, loss=0.023109018802642822\n",
      "Surface training t=6264, loss=0.022983329370617867\n",
      "Surface training t=6265, loss=0.024144728668034077\n",
      "Surface training t=6266, loss=0.02278032898902893\n",
      "Surface training t=6267, loss=0.024947297759354115\n",
      "Surface training t=6268, loss=0.02346932888031006\n",
      "Surface training t=6269, loss=0.022561081685125828\n",
      "Surface training t=6270, loss=0.022835785523056984\n",
      "Surface training t=6271, loss=0.023274929262697697\n",
      "Surface training t=6272, loss=0.024167013354599476\n",
      "Surface training t=6273, loss=0.02370382472872734\n",
      "Surface training t=6274, loss=0.0242550540715456\n",
      "Surface training t=6275, loss=0.0227461289614439\n",
      "Surface training t=6276, loss=0.023229452781379223\n",
      "Surface training t=6277, loss=0.022830591537058353\n",
      "Surface training t=6278, loss=0.024921512231230736\n",
      "Surface training t=6279, loss=0.023137345910072327\n",
      "Surface training t=6280, loss=0.024319651536643505\n",
      "Surface training t=6281, loss=0.02308858372271061\n",
      "Surface training t=6282, loss=0.023413985036313534\n",
      "Surface training t=6283, loss=0.024154256097972393\n",
      "Surface training t=6284, loss=0.02292003110051155\n",
      "Surface training t=6285, loss=0.022891136817634106\n",
      "Surface training t=6286, loss=0.023199033923447132\n",
      "Surface training t=6287, loss=0.02385676372796297\n",
      "Surface training t=6288, loss=0.02307862415909767\n",
      "Surface training t=6289, loss=0.022504357621073723\n",
      "Surface training t=6290, loss=0.02305847778916359\n",
      "Surface training t=6291, loss=0.0230150381103158\n",
      "Surface training t=6292, loss=0.023991689085960388\n",
      "Surface training t=6293, loss=0.02420806512236595\n",
      "Surface training t=6294, loss=0.023657286539673805\n",
      "Surface training t=6295, loss=0.022859309799969196\n",
      "Surface training t=6296, loss=0.023691795766353607\n",
      "Surface training t=6297, loss=0.022755037061870098\n",
      "Surface training t=6298, loss=0.0226192157715559\n",
      "Surface training t=6299, loss=0.022492271848022938\n",
      "Surface training t=6300, loss=0.02241644822061062\n",
      "Surface training t=6301, loss=0.023245060816407204\n",
      "Surface training t=6302, loss=0.02296402771025896\n",
      "Surface training t=6303, loss=0.02321552950888872\n",
      "Surface training t=6304, loss=0.02388318721204996\n",
      "Surface training t=6305, loss=0.02187012042850256\n",
      "Surface training t=6306, loss=0.024739842861890793\n",
      "Surface training t=6307, loss=0.022812589071691036\n",
      "Surface training t=6308, loss=0.023698276840150356\n",
      "Surface training t=6309, loss=0.023304548114538193\n",
      "Surface training t=6310, loss=0.02283846214413643\n",
      "Surface training t=6311, loss=0.02268571685999632\n",
      "Surface training t=6312, loss=0.022511390037834644\n",
      "Surface training t=6313, loss=0.02490370161831379\n",
      "Surface training t=6314, loss=0.023012218065559864\n",
      "Surface training t=6315, loss=0.02301137149333954\n",
      "Surface training t=6316, loss=0.022444876842200756\n",
      "Surface training t=6317, loss=0.023435247130692005\n",
      "Surface training t=6318, loss=0.023691066540777683\n",
      "Surface training t=6319, loss=0.023391243070364\n",
      "Surface training t=6320, loss=0.021946378983557224\n",
      "Surface training t=6321, loss=0.022174110636115074\n",
      "Surface training t=6322, loss=0.022823411971330643\n",
      "Surface training t=6323, loss=0.024316338822245598\n",
      "Surface training t=6324, loss=0.023074902594089508\n",
      "Surface training t=6325, loss=0.02266949787735939\n",
      "Surface training t=6326, loss=0.02281432691961527\n",
      "Surface training t=6327, loss=0.022639309987425804\n",
      "Surface training t=6328, loss=0.023195753805339336\n",
      "Surface training t=6329, loss=0.022282530553638935\n",
      "Surface training t=6330, loss=0.02319820411503315\n",
      "Surface training t=6331, loss=0.02283992152661085\n",
      "Surface training t=6332, loss=0.023167816922068596\n",
      "Surface training t=6333, loss=0.023248462937772274\n",
      "Surface training t=6334, loss=0.023017222993075848\n",
      "Surface training t=6335, loss=0.02246137522161007\n",
      "Surface training t=6336, loss=0.022606820799410343\n",
      "Surface training t=6337, loss=0.02385206986218691\n",
      "Surface training t=6338, loss=0.02197658270597458\n",
      "Surface training t=6339, loss=0.022334414534270763\n",
      "Surface training t=6340, loss=0.0230632945895195\n",
      "Surface training t=6341, loss=0.02332390286028385\n",
      "Surface training t=6342, loss=0.022386285476386547\n",
      "Surface training t=6343, loss=0.022660527378320694\n",
      "Surface training t=6344, loss=0.021960516460239887\n",
      "Surface training t=6345, loss=0.022767623886466026\n",
      "Surface training t=6346, loss=0.021667412482202053\n",
      "Surface training t=6347, loss=0.022086214274168015\n",
      "Surface training t=6348, loss=0.02300012018531561\n",
      "Surface training t=6349, loss=0.02366687823086977\n",
      "Surface training t=6350, loss=0.02362652774900198\n",
      "Surface training t=6351, loss=0.023081768304109573\n",
      "Surface training t=6352, loss=0.022603550925850868\n",
      "Surface training t=6353, loss=0.022766675800085068\n",
      "Surface training t=6354, loss=0.022153597325086594\n",
      "Surface training t=6355, loss=0.023402336053550243\n",
      "Surface training t=6356, loss=0.022403428331017494\n",
      "Surface training t=6357, loss=0.02190870139747858\n",
      "Surface training t=6358, loss=0.021756209433078766\n",
      "Surface training t=6359, loss=0.022748523391783237\n",
      "Surface training t=6360, loss=0.023527873679995537\n",
      "Surface training t=6361, loss=0.022173565812408924\n",
      "Surface training t=6362, loss=0.021698263473808765\n",
      "Surface training t=6363, loss=0.022725844755768776\n",
      "Surface training t=6364, loss=0.023978312499821186\n",
      "Surface training t=6365, loss=0.02231760136783123\n",
      "Surface training t=6366, loss=0.021690852008759975\n",
      "Surface training t=6367, loss=0.02276839129626751\n",
      "Surface training t=6368, loss=0.023258797824382782\n",
      "Surface training t=6369, loss=0.022303503938019276\n",
      "Surface training t=6370, loss=0.0218517342582345\n",
      "Surface training t=6371, loss=0.02376667410135269\n",
      "Surface training t=6372, loss=0.022494186647236347\n",
      "Surface training t=6373, loss=0.02266023028641939\n",
      "Surface training t=6374, loss=0.022409120574593544\n",
      "Surface training t=6375, loss=0.022313330322504044\n",
      "Surface training t=6376, loss=0.023974372074007988\n",
      "Surface training t=6377, loss=0.023278284817934036\n",
      "Surface training t=6378, loss=0.0231989286839962\n",
      "Surface training t=6379, loss=0.023176065646111965\n",
      "Surface training t=6380, loss=0.02213227655738592\n",
      "Surface training t=6381, loss=0.023716233670711517\n",
      "Surface training t=6382, loss=0.023197578266263008\n",
      "Surface training t=6383, loss=0.02206444274634123\n",
      "Surface training t=6384, loss=0.022997500374913216\n",
      "Surface training t=6385, loss=0.02229681797325611\n",
      "Surface training t=6386, loss=0.023047151044011116\n",
      "Surface training t=6387, loss=0.02194239478558302\n",
      "Surface training t=6388, loss=0.022495622746646404\n",
      "Surface training t=6389, loss=0.02250849548727274\n",
      "Surface training t=6390, loss=0.022459842264652252\n",
      "Surface training t=6391, loss=0.024000472389161587\n",
      "Surface training t=6392, loss=0.02172117866575718\n",
      "Surface training t=6393, loss=0.023246086202561855\n",
      "Surface training t=6394, loss=0.022011825814843178\n",
      "Surface training t=6395, loss=0.024922242388129234\n",
      "Surface training t=6396, loss=0.023866587318480015\n",
      "Surface training t=6397, loss=0.021762092597782612\n",
      "Surface training t=6398, loss=0.0240994431078434\n",
      "Surface training t=6399, loss=0.02192080393433571\n",
      "Surface training t=6400, loss=0.024677975103259087\n",
      "Surface training t=6401, loss=0.026354359462857246\n",
      "Surface training t=6402, loss=0.03522629290819168\n",
      "Surface training t=6403, loss=0.027465486899018288\n",
      "Surface training t=6404, loss=0.026465061120688915\n",
      "Surface training t=6405, loss=0.02685739193111658\n",
      "Surface training t=6406, loss=0.0319001292809844\n",
      "Surface training t=6407, loss=0.0278912503272295\n",
      "Surface training t=6408, loss=0.028402449563145638\n",
      "Surface training t=6409, loss=0.028643276542425156\n",
      "Surface training t=6410, loss=0.026496010832488537\n",
      "Surface training t=6411, loss=0.024479934014379978\n",
      "Surface training t=6412, loss=0.02546989545226097\n",
      "Surface training t=6413, loss=0.02538813930004835\n",
      "Surface training t=6414, loss=0.02642082143574953\n",
      "Surface training t=6415, loss=0.02697451040148735\n",
      "Surface training t=6416, loss=0.0270413001999259\n",
      "Surface training t=6417, loss=0.024846794083714485\n",
      "Surface training t=6418, loss=0.026279112324118614\n",
      "Surface training t=6419, loss=0.02654630597680807\n",
      "Surface training t=6420, loss=0.029214278794825077\n",
      "Surface training t=6421, loss=0.026183851063251495\n",
      "Surface training t=6422, loss=0.026260484009981155\n",
      "Surface training t=6423, loss=0.026226451620459557\n",
      "Surface training t=6424, loss=0.025531779043376446\n",
      "Surface training t=6425, loss=0.02613536175340414\n",
      "Surface training t=6426, loss=0.026484699919819832\n",
      "Surface training t=6427, loss=0.026225846260786057\n",
      "Surface training t=6428, loss=0.025171207264065742\n",
      "Surface training t=6429, loss=0.023522031493484974\n",
      "Surface training t=6430, loss=0.024574922397732735\n",
      "Surface training t=6431, loss=0.02631452400237322\n",
      "Surface training t=6432, loss=0.02566644921898842\n",
      "Surface training t=6433, loss=0.026480799540877342\n",
      "Surface training t=6434, loss=0.02689033094793558\n",
      "Surface training t=6435, loss=0.0272572860121727\n",
      "Surface training t=6436, loss=0.0263491403311491\n",
      "Surface training t=6437, loss=0.02685258351266384\n",
      "Surface training t=6438, loss=0.027986708097159863\n",
      "Surface training t=6439, loss=0.030307642184197903\n",
      "Surface training t=6440, loss=0.027900800108909607\n",
      "Surface training t=6441, loss=0.026763152331113815\n",
      "Surface training t=6442, loss=0.02687179669737816\n",
      "Surface training t=6443, loss=0.02670963667333126\n",
      "Surface training t=6444, loss=0.02755855955183506\n",
      "Surface training t=6445, loss=0.02690277062356472\n",
      "Surface training t=6446, loss=0.027141079306602478\n",
      "Surface training t=6447, loss=0.026462215930223465\n",
      "Surface training t=6448, loss=0.027429262176156044\n",
      "Surface training t=6449, loss=0.028156007640063763\n",
      "Surface training t=6450, loss=0.02648060955107212\n",
      "Surface training t=6451, loss=0.025987008586525917\n",
      "Surface training t=6452, loss=0.02763949427753687\n",
      "Surface training t=6453, loss=0.029142256826162338\n",
      "Surface training t=6454, loss=0.026822705753147602\n",
      "Surface training t=6455, loss=0.026385044679045677\n",
      "Surface training t=6456, loss=0.025871621444821358\n",
      "Surface training t=6457, loss=0.02692215610295534\n",
      "Surface training t=6458, loss=0.028647536411881447\n",
      "Surface training t=6459, loss=0.02684742584824562\n",
      "Surface training t=6460, loss=0.025963745079934597\n",
      "Surface training t=6461, loss=0.025625085458159447\n",
      "Surface training t=6462, loss=0.027990571223199368\n",
      "Surface training t=6463, loss=0.027513292618095875\n",
      "Surface training t=6464, loss=0.026383508928120136\n",
      "Surface training t=6465, loss=0.02620952483266592\n",
      "Surface training t=6466, loss=0.02842492889612913\n",
      "Surface training t=6467, loss=0.028419923968613148\n",
      "Surface training t=6468, loss=0.026773138903081417\n",
      "Surface training t=6469, loss=0.02576816827058792\n",
      "Surface training t=6470, loss=0.02624160796403885\n",
      "Surface training t=6471, loss=0.026326609775424004\n",
      "Surface training t=6472, loss=0.02661802899092436\n",
      "Surface training t=6473, loss=0.026213008910417557\n",
      "Surface training t=6474, loss=0.025839079171419144\n",
      "Surface training t=6475, loss=0.025471581146121025\n",
      "Surface training t=6476, loss=0.02663994114845991\n",
      "Surface training t=6477, loss=0.027476315386593342\n",
      "Surface training t=6478, loss=0.027489508502185345\n",
      "Surface training t=6479, loss=0.027365462854504585\n",
      "Surface training t=6480, loss=0.027030158787965775\n",
      "Surface training t=6481, loss=0.02619156986474991\n",
      "Surface training t=6482, loss=0.02603335492312908\n",
      "Surface training t=6483, loss=0.027371516451239586\n",
      "Surface training t=6484, loss=0.02727161254733801\n",
      "Surface training t=6485, loss=0.026187529787421227\n",
      "Surface training t=6486, loss=0.025941578671336174\n",
      "Surface training t=6487, loss=0.025110214948654175\n",
      "Surface training t=6488, loss=0.026428251527249813\n",
      "Surface training t=6489, loss=0.026936364360153675\n",
      "Surface training t=6490, loss=0.02725336980074644\n",
      "Surface training t=6491, loss=0.026169930584728718\n",
      "Surface training t=6492, loss=0.02507870551198721\n",
      "Surface training t=6493, loss=0.025119722820818424\n",
      "Surface training t=6494, loss=0.02538810856640339\n",
      "Surface training t=6495, loss=0.027847494930028915\n",
      "Surface training t=6496, loss=0.02694024797528982\n",
      "Surface training t=6497, loss=0.025691131129860878\n",
      "Surface training t=6498, loss=0.025084027089178562\n",
      "Surface training t=6499, loss=0.024247691966593266\n",
      "Surface training t=6500, loss=0.024817723780870438\n",
      "Surface training t=6501, loss=0.026267792098224163\n",
      "Surface training t=6502, loss=0.02715958096086979\n",
      "Surface training t=6503, loss=0.027489221654832363\n",
      "Surface training t=6504, loss=0.02765265852212906\n",
      "Surface training t=6505, loss=0.026641025207936764\n",
      "Surface training t=6506, loss=0.023739230819046497\n",
      "Surface training t=6507, loss=0.02380555309355259\n",
      "Surface training t=6508, loss=0.023060566745698452\n",
      "Surface training t=6509, loss=0.022232571616768837\n",
      "Surface training t=6510, loss=0.021511228755116463\n",
      "Surface training t=6511, loss=0.022464695386588573\n",
      "Surface training t=6512, loss=0.022006883285939693\n",
      "Surface training t=6513, loss=0.020975316874682903\n",
      "Surface training t=6514, loss=0.021896442398428917\n",
      "Surface training t=6515, loss=0.020828166976571083\n",
      "Surface training t=6516, loss=0.021358019672334194\n",
      "Surface training t=6517, loss=0.020213122479617596\n",
      "Surface training t=6518, loss=0.022233925759792328\n",
      "Surface training t=6519, loss=0.020697073079645634\n",
      "Surface training t=6520, loss=0.021343005821108818\n",
      "Surface training t=6521, loss=0.021192245185375214\n",
      "Surface training t=6522, loss=0.020484781824052334\n",
      "Surface training t=6523, loss=0.02001702506095171\n",
      "Surface training t=6524, loss=0.020229479297995567\n",
      "Surface training t=6525, loss=0.02093327883630991\n",
      "Surface training t=6526, loss=0.021811755374073982\n",
      "Surface training t=6527, loss=0.021155805326998234\n",
      "Surface training t=6528, loss=0.020785481669008732\n",
      "Surface training t=6529, loss=0.02114804834127426\n",
      "Surface training t=6530, loss=0.0216622706502676\n",
      "Surface training t=6531, loss=0.020485945977270603\n",
      "Surface training t=6532, loss=0.020327682606875896\n",
      "Surface training t=6533, loss=0.020939191803336143\n",
      "Surface training t=6534, loss=0.019909262657165527\n",
      "Surface training t=6535, loss=0.020179102197289467\n",
      "Surface training t=6536, loss=0.020544867031276226\n",
      "Surface training t=6537, loss=0.021649304777383804\n",
      "Surface training t=6538, loss=0.021149517968297005\n",
      "Surface training t=6539, loss=0.019740532152354717\n",
      "Surface training t=6540, loss=0.02075745351612568\n",
      "Surface training t=6541, loss=0.02134392037987709\n",
      "Surface training t=6542, loss=0.021138903684914112\n",
      "Surface training t=6543, loss=0.020289254374802113\n",
      "Surface training t=6544, loss=0.021400708705186844\n",
      "Surface training t=6545, loss=0.020861989818513393\n",
      "Surface training t=6546, loss=0.02060950268059969\n",
      "Surface training t=6547, loss=0.020425240509212017\n",
      "Surface training t=6548, loss=0.022030151449143887\n",
      "Surface training t=6549, loss=0.021087713539600372\n",
      "Surface training t=6550, loss=0.021891111508011818\n",
      "Surface training t=6551, loss=0.02256675995886326\n",
      "Surface training t=6552, loss=0.021918758749961853\n",
      "Surface training t=6553, loss=0.021188613958656788\n",
      "Surface training t=6554, loss=0.022738801315426826\n",
      "Surface training t=6555, loss=0.02290576882660389\n",
      "Surface training t=6556, loss=0.0222054710611701\n",
      "Surface training t=6557, loss=0.021515144035220146\n",
      "Surface training t=6558, loss=0.021122360602021217\n",
      "Surface training t=6559, loss=0.02139359526336193\n",
      "Surface training t=6560, loss=0.020710602402687073\n",
      "Surface training t=6561, loss=0.02070445753633976\n",
      "Surface training t=6562, loss=0.021147056482732296\n",
      "Surface training t=6563, loss=0.019839907996356487\n",
      "Surface training t=6564, loss=0.022511438466608524\n",
      "Surface training t=6565, loss=0.02085041254758835\n",
      "Surface training t=6566, loss=0.022008350118994713\n",
      "Surface training t=6567, loss=0.02283104509115219\n",
      "Surface training t=6568, loss=0.02156782988458872\n",
      "Surface training t=6569, loss=0.022625909186899662\n",
      "Surface training t=6570, loss=0.027321811765432358\n",
      "Surface training t=6571, loss=0.030012043192982674\n",
      "Surface training t=6572, loss=0.02608511596918106\n",
      "Surface training t=6573, loss=0.02566288411617279\n",
      "Surface training t=6574, loss=0.024559258483350277\n",
      "Surface training t=6575, loss=0.03005028422921896\n",
      "Surface training t=6576, loss=0.027122458443045616\n",
      "Surface training t=6577, loss=0.022156398743391037\n",
      "Surface training t=6578, loss=0.02203762996941805\n",
      "Surface training t=6579, loss=0.022624614648520947\n",
      "Surface training t=6580, loss=0.02196471020579338\n",
      "Surface training t=6581, loss=0.02252406533807516\n",
      "Surface training t=6582, loss=0.021100160665810108\n",
      "Surface training t=6583, loss=0.020261014811694622\n",
      "Surface training t=6584, loss=0.021863662637770176\n",
      "Surface training t=6585, loss=0.020166780799627304\n",
      "Surface training t=6586, loss=0.02195274829864502\n",
      "Surface training t=6587, loss=0.020821298472583294\n",
      "Surface training t=6588, loss=0.02027859538793564\n",
      "Surface training t=6589, loss=0.01977243833243847\n",
      "Surface training t=6590, loss=0.02152455598115921\n",
      "Surface training t=6591, loss=0.019208606332540512\n",
      "Surface training t=6592, loss=0.0207222243770957\n",
      "Surface training t=6593, loss=0.020835519768297672\n",
      "Surface training t=6594, loss=0.019729698076844215\n",
      "Surface training t=6595, loss=0.020091465674340725\n",
      "Surface training t=6596, loss=0.02128733228892088\n",
      "Surface training t=6597, loss=0.020214873366057873\n",
      "Surface training t=6598, loss=0.021315100602805614\n",
      "Surface training t=6599, loss=0.020514694042503834\n",
      "Surface training t=6600, loss=0.01949358358979225\n",
      "Surface training t=6601, loss=0.02131316252052784\n",
      "Surface training t=6602, loss=0.019343375228345394\n",
      "Surface training t=6603, loss=0.02058669552206993\n",
      "Surface training t=6604, loss=0.019440933130681515\n",
      "Surface training t=6605, loss=0.019662081263959408\n",
      "Surface training t=6606, loss=0.020847421139478683\n",
      "Surface training t=6607, loss=0.02080301009118557\n",
      "Surface training t=6608, loss=0.01953990012407303\n",
      "Surface training t=6609, loss=0.019805368036031723\n",
      "Surface training t=6610, loss=0.019346012733876705\n",
      "Surface training t=6611, loss=0.02140578255057335\n",
      "Surface training t=6612, loss=0.019248604774475098\n",
      "Surface training t=6613, loss=0.021182185038924217\n",
      "Surface training t=6614, loss=0.020275662653148174\n",
      "Surface training t=6615, loss=0.019276361912488937\n",
      "Surface training t=6616, loss=0.020267533138394356\n",
      "Surface training t=6617, loss=0.020049638114869595\n",
      "Surface training t=6618, loss=0.019192797131836414\n",
      "Surface training t=6619, loss=0.019129402935504913\n",
      "Surface training t=6620, loss=0.01953200437128544\n",
      "Surface training t=6621, loss=0.020681070163846016\n",
      "Surface training t=6622, loss=0.020455804653465748\n",
      "Surface training t=6623, loss=0.02052134182304144\n",
      "Surface training t=6624, loss=0.01938539743423462\n",
      "Surface training t=6625, loss=0.019615010358393192\n",
      "Surface training t=6626, loss=0.020156405866146088\n",
      "Surface training t=6627, loss=0.0204375172033906\n",
      "Surface training t=6628, loss=0.020618805661797523\n",
      "Surface training t=6629, loss=0.019509056583046913\n",
      "Surface training t=6630, loss=0.021802124567329884\n",
      "Surface training t=6631, loss=0.019909203983843327\n",
      "Surface training t=6632, loss=0.021185382269322872\n",
      "Surface training t=6633, loss=0.019876104779541492\n",
      "Surface training t=6634, loss=0.01913728006184101\n",
      "Surface training t=6635, loss=0.020756598562002182\n",
      "Surface training t=6636, loss=0.020197647623717785\n",
      "Surface training t=6637, loss=0.02085715625435114\n",
      "Surface training t=6638, loss=0.02037912141531706\n",
      "Surface training t=6639, loss=0.019648874178528786\n",
      "Surface training t=6640, loss=0.019095047377049923\n",
      "Surface training t=6641, loss=0.020539622753858566\n",
      "Surface training t=6642, loss=0.02072791662067175\n",
      "Surface training t=6643, loss=0.019542095251381397\n",
      "Surface training t=6644, loss=0.01925254613161087\n",
      "Surface training t=6645, loss=0.019006837159395218\n",
      "Surface training t=6646, loss=0.01991442311555147\n",
      "Surface training t=6647, loss=0.020108812488615513\n",
      "Surface training t=6648, loss=0.01938561536371708\n",
      "Surface training t=6649, loss=0.020182425156235695\n",
      "Surface training t=6650, loss=0.020491440780460835\n",
      "Surface training t=6651, loss=0.019669010303914547\n",
      "Surface training t=6652, loss=0.018522790633141994\n",
      "Surface training t=6653, loss=0.020072796382009983\n",
      "Surface training t=6654, loss=0.020134481601417065\n",
      "Surface training t=6655, loss=0.01950974389910698\n",
      "Surface training t=6656, loss=0.01950748823583126\n",
      "Surface training t=6657, loss=0.01973521150648594\n",
      "Surface training t=6658, loss=0.018999524414539337\n",
      "Surface training t=6659, loss=0.01946101151406765\n",
      "Surface training t=6660, loss=0.019557195715606213\n",
      "Surface training t=6661, loss=0.021642714738845825\n",
      "Surface training t=6662, loss=0.0190719086676836\n",
      "Surface training t=6663, loss=0.020764803513884544\n",
      "Surface training t=6664, loss=0.018864340148866177\n",
      "Surface training t=6665, loss=0.01863247435539961\n",
      "Surface training t=6666, loss=0.018981165252625942\n",
      "Surface training t=6667, loss=0.020724568516016006\n",
      "Surface training t=6668, loss=0.019125194288790226\n",
      "Surface training t=6669, loss=0.018895813263952732\n",
      "Surface training t=6670, loss=0.018677450716495514\n",
      "Surface training t=6671, loss=0.019445940852165222\n",
      "Surface training t=6672, loss=0.02071232069283724\n",
      "Surface training t=6673, loss=0.018782854080200195\n",
      "Surface training t=6674, loss=0.01905023120343685\n",
      "Surface training t=6675, loss=0.021061561070382595\n",
      "Surface training t=6676, loss=0.019368779845535755\n",
      "Surface training t=6677, loss=0.020509157329797745\n",
      "Surface training t=6678, loss=0.021110261790454388\n",
      "Surface training t=6679, loss=0.019087305292487144\n",
      "Surface training t=6680, loss=0.018561389297246933\n",
      "Surface training t=6681, loss=0.018315068446099758\n",
      "Surface training t=6682, loss=0.01897631213068962\n",
      "Surface training t=6683, loss=0.0189585592597723\n",
      "Surface training t=6684, loss=0.020800710655748844\n",
      "Surface training t=6685, loss=0.020024392753839493\n",
      "Surface training t=6686, loss=0.019404781982302666\n",
      "Surface training t=6687, loss=0.018152758479118347\n",
      "Surface training t=6688, loss=0.019099020399153233\n",
      "Surface training t=6689, loss=0.020272615365684032\n",
      "Surface training t=6690, loss=0.01865316741168499\n",
      "Surface training t=6691, loss=0.019062058068811893\n",
      "Surface training t=6692, loss=0.020453752018511295\n",
      "Surface training t=6693, loss=0.01877880934625864\n",
      "Surface training t=6694, loss=0.021677973680198193\n",
      "Surface training t=6695, loss=0.019583839923143387\n",
      "Surface training t=6696, loss=0.02125411108136177\n",
      "Surface training t=6697, loss=0.025326761417090893\n",
      "Surface training t=6698, loss=0.02892181370407343\n",
      "Surface training t=6699, loss=0.023709950037300587\n",
      "Surface training t=6700, loss=0.02244689129292965\n",
      "Surface training t=6701, loss=0.022984692826867104\n",
      "Surface training t=6702, loss=0.022357072681188583\n",
      "Surface training t=6703, loss=0.02197554986923933\n",
      "Surface training t=6704, loss=0.023283186368644238\n",
      "Surface training t=6705, loss=0.022489351220428944\n",
      "Surface training t=6706, loss=0.02359197661280632\n",
      "Surface training t=6707, loss=0.027749604545533657\n",
      "Surface training t=6708, loss=0.023539384827017784\n",
      "Surface training t=6709, loss=0.023981008678674698\n",
      "Surface training t=6710, loss=0.02162201516330242\n",
      "Surface training t=6711, loss=0.023121804930269718\n",
      "Surface training t=6712, loss=0.023425373248755932\n",
      "Surface training t=6713, loss=0.023080291226506233\n",
      "Surface training t=6714, loss=0.022263016551733017\n",
      "Surface training t=6715, loss=0.02391176950186491\n",
      "Surface training t=6716, loss=0.02580256760120392\n",
      "Surface training t=6717, loss=0.026871993206441402\n",
      "Surface training t=6718, loss=0.02414601668715477\n",
      "Surface training t=6719, loss=0.02237403392791748\n",
      "Surface training t=6720, loss=0.02402978390455246\n",
      "Surface training t=6721, loss=0.02717960812151432\n",
      "Surface training t=6722, loss=0.022101634182035923\n",
      "Surface training t=6723, loss=0.021438511088490486\n",
      "Surface training t=6724, loss=0.021521584130823612\n",
      "Surface training t=6725, loss=0.021890435367822647\n",
      "Surface training t=6726, loss=0.02107661683112383\n",
      "Surface training t=6727, loss=0.020136477425694466\n",
      "Surface training t=6728, loss=0.02039879932999611\n",
      "Surface training t=6729, loss=0.020069419406354427\n",
      "Surface training t=6730, loss=0.01981135830283165\n",
      "Surface training t=6731, loss=0.020372482016682625\n",
      "Surface training t=6732, loss=0.020573649555444717\n",
      "Surface training t=6733, loss=0.018412690609693527\n",
      "Surface training t=6734, loss=0.019401279278099537\n",
      "Surface training t=6735, loss=0.01885454449802637\n",
      "Surface training t=6736, loss=0.019349560141563416\n",
      "Surface training t=6737, loss=0.01895387191325426\n",
      "Surface training t=6738, loss=0.01889063324779272\n",
      "Surface training t=6739, loss=0.01995582040399313\n",
      "Surface training t=6740, loss=0.018516672775149345\n",
      "Surface training t=6741, loss=0.019653551280498505\n",
      "Surface training t=6742, loss=0.019917819648981094\n",
      "Surface training t=6743, loss=0.01879039593040943\n",
      "Surface training t=6744, loss=0.01794528402388096\n",
      "Surface training t=6745, loss=0.01937742717564106\n",
      "Surface training t=6746, loss=0.01922117080539465\n",
      "Surface training t=6747, loss=0.019815226085484028\n",
      "Surface training t=6748, loss=0.018896696157753468\n",
      "Surface training t=6749, loss=0.01934107020497322\n",
      "Surface training t=6750, loss=0.01823711022734642\n",
      "Surface training t=6751, loss=0.017574580386281013\n",
      "Surface training t=6752, loss=0.01788349449634552\n",
      "Surface training t=6753, loss=0.01988830789923668\n",
      "Surface training t=6754, loss=0.01841162797063589\n",
      "Surface training t=6755, loss=0.02081249188631773\n",
      "Surface training t=6756, loss=0.01871130522340536\n",
      "Surface training t=6757, loss=0.020971032790839672\n",
      "Surface training t=6758, loss=0.021888568066060543\n",
      "Surface training t=6759, loss=0.027865015901625156\n",
      "Surface training t=6760, loss=0.029339059256017208\n",
      "Surface training t=6761, loss=0.021192094311118126\n",
      "Surface training t=6762, loss=0.026606328785419464\n",
      "Surface training t=6763, loss=0.021430274471640587\n",
      "Surface training t=6764, loss=0.025650084018707275\n",
      "Surface training t=6765, loss=0.026264422573149204\n",
      "Surface training t=6766, loss=0.025515754707157612\n",
      "Surface training t=6767, loss=0.022511759772896767\n",
      "Surface training t=6768, loss=0.026678544469177723\n",
      "Surface training t=6769, loss=0.024859807454049587\n",
      "Surface training t=6770, loss=0.024377355352044106\n",
      "Surface training t=6771, loss=0.022055678069591522\n",
      "Surface training t=6772, loss=0.026594518683850765\n",
      "Surface training t=6773, loss=0.026266394183039665\n",
      "Surface training t=6774, loss=0.023106617853045464\n",
      "Surface training t=6775, loss=0.0226114634424448\n",
      "Surface training t=6776, loss=0.02512917947024107\n",
      "Surface training t=6777, loss=0.025904910638928413\n",
      "Surface training t=6778, loss=0.02364359889179468\n",
      "Surface training t=6779, loss=0.021622655913233757\n",
      "Surface training t=6780, loss=0.02289731241762638\n",
      "Surface training t=6781, loss=0.02896667830646038\n",
      "Surface training t=6782, loss=0.020735817961394787\n",
      "Surface training t=6783, loss=0.02210184559226036\n",
      "Surface training t=6784, loss=0.026452401652932167\n",
      "Surface training t=6785, loss=0.02622721530497074\n",
      "Surface training t=6786, loss=0.024166379123926163\n",
      "Surface training t=6787, loss=0.021252007223665714\n",
      "Surface training t=6788, loss=0.024143616668879986\n",
      "Surface training t=6789, loss=0.02688128687441349\n",
      "Surface training t=6790, loss=0.023763333447277546\n",
      "Surface training t=6791, loss=0.022337570786476135\n",
      "Surface training t=6792, loss=0.02171006239950657\n",
      "Surface training t=6793, loss=0.02481141034513712\n",
      "Surface training t=6794, loss=0.027347013354301453\n",
      "Surface training t=6795, loss=0.02285574469715357\n",
      "Surface training t=6796, loss=0.02205276768654585\n",
      "Surface training t=6797, loss=0.02232562843710184\n",
      "Surface training t=6798, loss=0.02835070714354515\n",
      "Surface training t=6799, loss=0.022461576387286186\n",
      "Surface training t=6800, loss=0.022502663545310497\n",
      "Surface training t=6801, loss=0.021225284785032272\n",
      "Surface training t=6802, loss=0.022315824404358864\n",
      "Surface training t=6803, loss=0.026835689321160316\n",
      "Surface training t=6804, loss=0.023148322477936745\n",
      "Surface training t=6805, loss=0.026072262786328793\n",
      "Surface training t=6806, loss=0.02007542923092842\n",
      "Surface training t=6807, loss=0.0239566620439291\n",
      "Surface training t=6808, loss=0.02595032099634409\n",
      "Surface training t=6809, loss=0.024655191227793694\n",
      "Surface training t=6810, loss=0.021891933865845203\n",
      "Surface training t=6811, loss=0.01997369434684515\n",
      "Surface training t=6812, loss=0.019939327612519264\n",
      "Surface training t=6813, loss=0.02237298432737589\n",
      "Surface training t=6814, loss=0.02676850836724043\n",
      "Surface training t=6815, loss=0.02723169419914484\n",
      "Surface training t=6816, loss=0.01979792583733797\n",
      "Surface training t=6817, loss=0.018802323378622532\n",
      "Surface training t=6818, loss=0.020094302482903004\n",
      "Surface training t=6819, loss=0.02042850386351347\n",
      "Surface training t=6820, loss=0.019813718274235725\n",
      "Surface training t=6821, loss=0.018964221701025963\n",
      "Surface training t=6822, loss=0.018661617301404476\n",
      "Surface training t=6823, loss=0.019923709332942963\n",
      "Surface training t=6824, loss=0.018077214248478413\n",
      "Surface training t=6825, loss=0.018448992632329464\n",
      "Surface training t=6826, loss=0.01947327982634306\n",
      "Surface training t=6827, loss=0.018006864935159683\n",
      "Surface training t=6828, loss=0.020365260541439056\n",
      "Surface training t=6829, loss=0.022564592771232128\n",
      "Surface training t=6830, loss=0.03082429524511099\n",
      "Surface training t=6831, loss=0.021528130397200584\n",
      "Surface training t=6832, loss=0.01899491250514984\n",
      "Surface training t=6833, loss=0.019582543522119522\n",
      "Surface training t=6834, loss=0.02523885015398264\n",
      "Surface training t=6835, loss=0.03040403500199318\n",
      "Surface training t=6836, loss=0.021154610440135002\n",
      "Surface training t=6837, loss=0.01829350646585226\n",
      "Surface training t=6838, loss=0.019084066152572632\n",
      "Surface training t=6839, loss=0.019219611771404743\n",
      "Surface training t=6840, loss=0.018970043398439884\n",
      "Surface training t=6841, loss=0.017783465795218945\n",
      "Surface training t=6842, loss=0.01873026229441166\n",
      "Surface training t=6843, loss=0.020072555169463158\n",
      "Surface training t=6844, loss=0.01904401835054159\n",
      "Surface training t=6845, loss=0.017649934627115726\n",
      "Surface training t=6846, loss=0.019068912602961063\n",
      "Surface training t=6847, loss=0.019515514373779297\n",
      "Surface training t=6848, loss=0.017537491396069527\n",
      "Surface training t=6849, loss=0.01949276588857174\n",
      "Surface training t=6850, loss=0.019265751354396343\n",
      "Surface training t=6851, loss=0.017674126662313938\n",
      "Surface training t=6852, loss=0.01904005091637373\n",
      "Surface training t=6853, loss=0.018899145536124706\n",
      "Surface training t=6854, loss=0.020232997834682465\n",
      "Surface training t=6855, loss=0.02109290286898613\n",
      "Surface training t=6856, loss=0.02473260834813118\n",
      "Surface training t=6857, loss=0.024586944840848446\n",
      "Surface training t=6858, loss=0.02269380260258913\n",
      "Surface training t=6859, loss=0.022571628913283348\n",
      "Surface training t=6860, loss=0.02423660922795534\n",
      "Surface training t=6861, loss=0.022753093391656876\n",
      "Surface training t=6862, loss=0.022037997841835022\n",
      "Surface training t=6863, loss=0.023258326575160027\n",
      "Surface training t=6864, loss=0.02510622050613165\n",
      "Surface training t=6865, loss=0.02377892378717661\n",
      "Surface training t=6866, loss=0.021850974299013615\n",
      "Surface training t=6867, loss=0.022600923664867878\n",
      "Surface training t=6868, loss=0.023010460659861565\n",
      "Surface training t=6869, loss=0.02308779302984476\n",
      "Surface training t=6870, loss=0.02261424995958805\n",
      "Surface training t=6871, loss=0.02249622903764248\n",
      "Surface training t=6872, loss=0.021800155751407146\n",
      "Surface training t=6873, loss=0.02346201427280903\n",
      "Surface training t=6874, loss=0.02374923601746559\n",
      "Surface training t=6875, loss=0.023018484003841877\n",
      "Surface training t=6876, loss=0.021874288097023964\n",
      "Surface training t=6877, loss=0.022509682923555374\n",
      "Surface training t=6878, loss=0.021874748170375824\n",
      "Surface training t=6879, loss=0.022820555604994297\n",
      "Surface training t=6880, loss=0.02295772824436426\n",
      "Surface training t=6881, loss=0.021810325793921947\n",
      "Surface training t=6882, loss=0.022156293503940105\n",
      "Surface training t=6883, loss=0.023314871825277805\n",
      "Surface training t=6884, loss=0.023858106695115566\n",
      "Surface training t=6885, loss=0.023141507990658283\n",
      "Surface training t=6886, loss=0.021327133290469646\n",
      "Surface training t=6887, loss=0.021465076133608818\n",
      "Surface training t=6888, loss=0.02252472471445799\n",
      "Surface training t=6889, loss=0.023251404985785484\n",
      "Surface training t=6890, loss=0.02223727200180292\n",
      "Surface training t=6891, loss=0.02158172708004713\n",
      "Surface training t=6892, loss=0.021391370333731174\n",
      "Surface training t=6893, loss=0.022959326393902302\n",
      "Surface training t=6894, loss=0.022757848724722862\n",
      "Surface training t=6895, loss=0.02279887069016695\n",
      "Surface training t=6896, loss=0.021509535610675812\n",
      "Surface training t=6897, loss=0.02081096451729536\n",
      "Surface training t=6898, loss=0.021224933676421642\n",
      "Surface training t=6899, loss=0.022389525547623634\n",
      "Surface training t=6900, loss=0.023360135965049267\n",
      "Surface training t=6901, loss=0.02201737929135561\n",
      "Surface training t=6902, loss=0.02111080102622509\n",
      "Surface training t=6903, loss=0.0211480176076293\n",
      "Surface training t=6904, loss=0.02214754931628704\n",
      "Surface training t=6905, loss=0.023018425330519676\n",
      "Surface training t=6906, loss=0.02219250798225403\n",
      "Surface training t=6907, loss=0.021999217569828033\n",
      "Surface training t=6908, loss=0.021413130685687065\n",
      "Surface training t=6909, loss=0.02190783154219389\n",
      "Surface training t=6910, loss=0.022648132406175137\n",
      "Surface training t=6911, loss=0.02237726654857397\n",
      "Surface training t=6912, loss=0.022316403687000275\n",
      "Surface training t=6913, loss=0.0208126874640584\n",
      "Surface training t=6914, loss=0.0218819472938776\n",
      "Surface training t=6915, loss=0.02180516254156828\n",
      "Surface training t=6916, loss=0.02211493905633688\n",
      "Surface training t=6917, loss=0.021632029674947262\n",
      "Surface training t=6918, loss=0.02140024583786726\n",
      "Surface training t=6919, loss=0.02099863812327385\n",
      "Surface training t=6920, loss=0.021473579108715057\n",
      "Surface training t=6921, loss=0.02253630943596363\n",
      "Surface training t=6922, loss=0.021659037098288536\n",
      "Surface training t=6923, loss=0.02165115252137184\n",
      "Surface training t=6924, loss=0.021806427277624607\n",
      "Surface training t=6925, loss=0.021714198403060436\n",
      "Surface training t=6926, loss=0.021258198656141758\n",
      "Surface training t=6927, loss=0.02104945946484804\n",
      "Surface training t=6928, loss=0.02083788439631462\n",
      "Surface training t=6929, loss=0.0222359299659729\n",
      "Surface training t=6930, loss=0.02248737495392561\n",
      "Surface training t=6931, loss=0.021769603714346886\n",
      "Surface training t=6932, loss=0.021225598640739918\n",
      "Surface training t=6933, loss=0.02150322124361992\n",
      "Surface training t=6934, loss=0.02199255581945181\n",
      "Surface training t=6935, loss=0.021839482709765434\n",
      "Surface training t=6936, loss=0.021256258711218834\n",
      "Surface training t=6937, loss=0.021754493936896324\n",
      "Surface training t=6938, loss=0.02158278413116932\n",
      "Surface training t=6939, loss=0.021362805739045143\n",
      "Surface training t=6940, loss=0.021118550561368465\n",
      "Surface training t=6941, loss=0.02176277618855238\n",
      "Surface training t=6942, loss=0.022371141240000725\n",
      "Surface training t=6943, loss=0.021453681401908398\n",
      "Surface training t=6944, loss=0.020971421152353287\n",
      "Surface training t=6945, loss=0.021809695288538933\n",
      "Surface training t=6946, loss=0.021513251587748528\n",
      "Surface training t=6947, loss=0.02167411334812641\n",
      "Surface training t=6948, loss=0.02075296174734831\n",
      "Surface training t=6949, loss=0.021131670102477074\n",
      "Surface training t=6950, loss=0.021275575272738934\n",
      "Surface training t=6951, loss=0.022137809544801712\n",
      "Surface training t=6952, loss=0.021945573389530182\n",
      "Surface training t=6953, loss=0.020763122476637363\n",
      "Surface training t=6954, loss=0.020469961687922478\n",
      "Surface training t=6955, loss=0.02124704048037529\n",
      "Surface training t=6956, loss=0.02166764158755541\n",
      "Surface training t=6957, loss=0.020174091681838036\n",
      "Surface training t=6958, loss=0.021268687210977077\n",
      "Surface training t=6959, loss=0.02190584223717451\n",
      "Surface training t=6960, loss=0.021867303177714348\n",
      "Surface training t=6961, loss=0.021040008403360844\n",
      "Surface training t=6962, loss=0.02025987207889557\n",
      "Surface training t=6963, loss=0.021868153475224972\n",
      "Surface training t=6964, loss=0.021751221269369125\n",
      "Surface training t=6965, loss=0.02062498964369297\n",
      "Surface training t=6966, loss=0.019453274086117744\n",
      "Surface training t=6967, loss=0.018989678472280502\n",
      "Surface training t=6968, loss=0.019747118465602398\n",
      "Surface training t=6969, loss=0.022214800119400024\n",
      "Surface training t=6970, loss=0.02205309458076954\n",
      "Surface training t=6971, loss=0.020673321560025215\n",
      "Surface training t=6972, loss=0.019356122240424156\n",
      "Surface training t=6973, loss=0.019187304191291332\n",
      "Surface training t=6974, loss=0.019417673349380493\n",
      "Surface training t=6975, loss=0.020633389241993427\n",
      "Surface training t=6976, loss=0.022064858116209507\n",
      "Surface training t=6977, loss=0.02153563406318426\n",
      "Surface training t=6978, loss=0.020907294936478138\n",
      "Surface training t=6979, loss=0.020304868929088116\n",
      "Surface training t=6980, loss=0.02101000491529703\n",
      "Surface training t=6981, loss=0.021781569346785545\n",
      "Surface training t=6982, loss=0.02110315952450037\n",
      "Surface training t=6983, loss=0.01994937565177679\n",
      "Surface training t=6984, loss=0.020221959799528122\n",
      "Surface training t=6985, loss=0.021511406637728214\n",
      "Surface training t=6986, loss=0.02174527198076248\n",
      "Surface training t=6987, loss=0.021238934248685837\n",
      "Surface training t=6988, loss=0.02104184217751026\n",
      "Surface training t=6989, loss=0.01978827640414238\n",
      "Surface training t=6990, loss=0.0211721146479249\n",
      "Surface training t=6991, loss=0.021954817697405815\n",
      "Surface training t=6992, loss=0.021131208166480064\n",
      "Surface training t=6993, loss=0.020329199731349945\n",
      "Surface training t=6994, loss=0.020361360162496567\n",
      "Surface training t=6995, loss=0.020706861279904842\n",
      "Surface training t=6996, loss=0.02081588003784418\n",
      "Surface training t=6997, loss=0.020846444182097912\n",
      "Surface training t=6998, loss=0.02081087790429592\n",
      "Surface training t=6999, loss=0.021383740939199924\n",
      "Surface training t=7000, loss=0.020866861566901207\n",
      "Surface training t=7001, loss=0.01942853443324566\n",
      "Surface training t=7002, loss=0.01884247548878193\n",
      "Surface training t=7003, loss=0.019290140829980373\n",
      "Surface training t=7004, loss=0.020488467998802662\n",
      "Surface training t=7005, loss=0.021846221759915352\n",
      "Surface training t=7006, loss=0.021907160058617592\n",
      "Surface training t=7007, loss=0.020460158586502075\n",
      "Surface training t=7008, loss=0.020017809234559536\n",
      "Surface training t=7009, loss=0.020512161776423454\n",
      "Surface training t=7010, loss=0.020666906610131264\n",
      "Surface training t=7011, loss=0.02041821926832199\n",
      "Surface training t=7012, loss=0.02055521495640278\n",
      "Surface training t=7013, loss=0.0206338781863451\n",
      "Surface training t=7014, loss=0.021086805500090122\n",
      "Surface training t=7015, loss=0.020894735120236874\n",
      "Surface training t=7016, loss=0.020131764002144337\n",
      "Surface training t=7017, loss=0.020368261262774467\n",
      "Surface training t=7018, loss=0.020120179280638695\n",
      "Surface training t=7019, loss=0.021113449707627296\n",
      "Surface training t=7020, loss=0.020543704740703106\n",
      "Surface training t=7021, loss=0.019382724538445473\n",
      "Surface training t=7022, loss=0.019667389802634716\n",
      "Surface training t=7023, loss=0.020534551702439785\n",
      "Surface training t=7024, loss=0.02165998425334692\n",
      "Surface training t=7025, loss=0.020418564788997173\n",
      "Surface training t=7026, loss=0.019856606610119343\n",
      "Surface training t=7027, loss=0.019787452183663845\n",
      "Surface training t=7028, loss=0.01988216955214739\n",
      "Surface training t=7029, loss=0.020692680031061172\n",
      "Surface training t=7030, loss=0.02076958306133747\n",
      "Surface training t=7031, loss=0.020601564086973667\n",
      "Surface training t=7032, loss=0.02077329531311989\n",
      "Surface training t=7033, loss=0.02011275105178356\n",
      "Surface training t=7034, loss=0.020154399797320366\n",
      "Surface training t=7035, loss=0.02064661029726267\n",
      "Surface training t=7036, loss=0.020721635781228542\n",
      "Surface training t=7037, loss=0.0197317311540246\n",
      "Surface training t=7038, loss=0.020426331087946892\n",
      "Surface training t=7039, loss=0.020411420613527298\n",
      "Surface training t=7040, loss=0.020316422916948795\n",
      "Surface training t=7041, loss=0.020810511894524097\n",
      "Surface training t=7042, loss=0.021013404242694378\n",
      "Surface training t=7043, loss=0.020519543439149857\n",
      "Surface training t=7044, loss=0.01989154051989317\n",
      "Surface training t=7045, loss=0.01976048294454813\n",
      "Surface training t=7046, loss=0.019399208948016167\n",
      "Surface training t=7047, loss=0.020175070501863956\n",
      "Surface training t=7048, loss=0.0207827752456069\n",
      "Surface training t=7049, loss=0.020958682522177696\n",
      "Surface training t=7050, loss=0.019556107930839062\n",
      "Surface training t=7051, loss=0.01931101083755493\n",
      "Surface training t=7052, loss=0.01997737307101488\n",
      "Surface training t=7053, loss=0.02053034771233797\n",
      "Surface training t=7054, loss=0.02082656044512987\n",
      "Surface training t=7055, loss=0.019800199195742607\n",
      "Surface training t=7056, loss=0.01952360663563013\n",
      "Surface training t=7057, loss=0.019098035991191864\n",
      "Surface training t=7058, loss=0.019571589305996895\n",
      "Surface training t=7059, loss=0.020098752342164516\n",
      "Surface training t=7060, loss=0.02071288786828518\n",
      "Surface training t=7061, loss=0.019569640047848225\n",
      "Surface training t=7062, loss=0.019764713011682034\n",
      "Surface training t=7063, loss=0.019724703393876553\n",
      "Surface training t=7064, loss=0.020537259988486767\n",
      "Surface training t=7065, loss=0.02045088540762663\n",
      "Surface training t=7066, loss=0.020486637018620968\n",
      "Surface training t=7067, loss=0.0193175682798028\n",
      "Surface training t=7068, loss=0.019640822894871235\n",
      "Surface training t=7069, loss=0.019569963216781616\n",
      "Surface training t=7070, loss=0.020221066661179066\n",
      "Surface training t=7071, loss=0.02024433482438326\n",
      "Surface training t=7072, loss=0.02048721630126238\n",
      "Surface training t=7073, loss=0.02009203378111124\n",
      "Surface training t=7074, loss=0.01910161506384611\n",
      "Surface training t=7075, loss=0.019443565979599953\n",
      "Surface training t=7076, loss=0.020292777568101883\n",
      "Surface training t=7077, loss=0.020331047475337982\n",
      "Surface training t=7078, loss=0.020155715756118298\n",
      "Surface training t=7079, loss=0.019371723756194115\n",
      "Surface training t=7080, loss=0.01969135459512472\n",
      "Surface training t=7081, loss=0.020355873741209507\n",
      "Surface training t=7082, loss=0.020141144283115864\n",
      "Surface training t=7083, loss=0.018322473391890526\n",
      "Surface training t=7084, loss=0.01821582019329071\n",
      "Surface training t=7085, loss=0.019630176946520805\n",
      "Surface training t=7086, loss=0.01975714322179556\n",
      "Surface training t=7087, loss=0.020718044601380825\n",
      "Surface training t=7088, loss=0.019917266443371773\n",
      "Surface training t=7089, loss=0.01830633543431759\n",
      "Surface training t=7090, loss=0.017694477923214436\n",
      "Surface training t=7091, loss=0.017502934206277132\n",
      "Surface training t=7092, loss=0.017789960373193026\n",
      "Surface training t=7093, loss=0.01912217028439045\n",
      "Surface training t=7094, loss=0.020803223364055157\n",
      "Surface training t=7095, loss=0.020870842039585114\n",
      "Surface training t=7096, loss=0.0193331278860569\n",
      "Surface training t=7097, loss=0.01805287040770054\n",
      "Surface training t=7098, loss=0.019360008649528027\n",
      "Surface training t=7099, loss=0.019655226729810238\n",
      "Surface training t=7100, loss=0.019831905141472816\n",
      "Surface training t=7101, loss=0.01945040374994278\n",
      "Surface training t=7102, loss=0.019441823475062847\n",
      "Surface training t=7103, loss=0.01929428707808256\n",
      "Surface training t=7104, loss=0.01979764737188816\n",
      "Surface training t=7105, loss=0.018728655762970448\n",
      "Surface training t=7106, loss=0.01780372206121683\n",
      "Surface training t=7107, loss=0.01822901051491499\n",
      "Surface training t=7108, loss=0.018029430881142616\n",
      "Surface training t=7109, loss=0.018128267489373684\n",
      "Surface training t=7110, loss=0.0181972598657012\n",
      "Surface training t=7111, loss=0.019673064351081848\n",
      "Surface training t=7112, loss=0.01963127963244915\n",
      "Surface training t=7113, loss=0.01890275999903679\n",
      "Surface training t=7114, loss=0.017872543074190617\n",
      "Surface training t=7115, loss=0.018303179182112217\n",
      "Surface training t=7116, loss=0.019024700857698917\n",
      "Surface training t=7117, loss=0.019965609535574913\n",
      "Surface training t=7118, loss=0.019974570721387863\n",
      "Surface training t=7119, loss=0.01971804816275835\n",
      "Surface training t=7120, loss=0.019060836173593998\n",
      "Surface training t=7121, loss=0.019146202132105827\n",
      "Surface training t=7122, loss=0.019579876214265823\n",
      "Surface training t=7123, loss=0.019864975474774837\n",
      "Surface training t=7124, loss=0.01948780845850706\n",
      "Surface training t=7125, loss=0.019853951409459114\n",
      "Surface training t=7126, loss=0.01936699729412794\n",
      "Surface training t=7127, loss=0.018888873979449272\n",
      "Surface training t=7128, loss=0.019049203023314476\n",
      "Surface training t=7129, loss=0.01998546626418829\n",
      "Surface training t=7130, loss=0.019986683502793312\n",
      "Surface training t=7131, loss=0.019902920350432396\n",
      "Surface training t=7132, loss=0.01888291072100401\n",
      "Surface training t=7133, loss=0.01853410992771387\n",
      "Surface training t=7134, loss=0.018020802177488804\n",
      "Surface training t=7135, loss=0.0182876018807292\n",
      "Surface training t=7136, loss=0.01846808847039938\n",
      "Surface training t=7137, loss=0.019940094090998173\n",
      "Surface training t=7138, loss=0.01993284747004509\n",
      "Surface training t=7139, loss=0.01866579707711935\n",
      "Surface training t=7140, loss=0.018408593721687794\n",
      "Surface training t=7141, loss=0.018283942714333534\n",
      "Surface training t=7142, loss=0.01855276059359312\n",
      "Surface training t=7143, loss=0.01980625931173563\n",
      "Surface training t=7144, loss=0.01926894299685955\n",
      "Surface training t=7145, loss=0.01885643880814314\n",
      "Surface training t=7146, loss=0.018275720067322254\n",
      "Surface training t=7147, loss=0.017523927614092827\n",
      "Surface training t=7148, loss=0.017699740827083588\n",
      "Surface training t=7149, loss=0.017844160087406635\n",
      "Surface training t=7150, loss=0.018542221747338772\n",
      "Surface training t=7151, loss=0.0200746962800622\n",
      "Surface training t=7152, loss=0.02021496742963791\n",
      "Surface training t=7153, loss=0.019151072949171066\n",
      "Surface training t=7154, loss=0.01839146390557289\n",
      "Surface training t=7155, loss=0.01920301839709282\n",
      "Surface training t=7156, loss=0.01951647363603115\n",
      "Surface training t=7157, loss=0.01940926630049944\n",
      "Surface training t=7158, loss=0.019653654657304287\n",
      "Surface training t=7159, loss=0.018928619101643562\n",
      "Surface training t=7160, loss=0.01777469925582409\n",
      "Surface training t=7161, loss=0.018475797958672047\n",
      "Surface training t=7162, loss=0.019357883371412754\n",
      "Surface training t=7163, loss=0.0199395464733243\n",
      "Surface training t=7164, loss=0.01941054966300726\n",
      "Surface training t=7165, loss=0.018674847669899464\n",
      "Surface training t=7166, loss=0.018171539530158043\n",
      "Surface training t=7167, loss=0.01898298319429159\n",
      "Surface training t=7168, loss=0.019382724538445473\n",
      "Surface training t=7169, loss=0.018500924110412598\n",
      "Surface training t=7170, loss=0.018348184414207935\n",
      "Surface training t=7171, loss=0.01847042515873909\n",
      "Surface training t=7172, loss=0.019244452007114887\n",
      "Surface training t=7173, loss=0.019340811297297478\n",
      "Surface training t=7174, loss=0.019018609076738358\n",
      "Surface training t=7175, loss=0.01882022526115179\n",
      "Surface training t=7176, loss=0.01867357362061739\n",
      "Surface training t=7177, loss=0.01875464618206024\n",
      "Surface training t=7178, loss=0.018775838427245617\n",
      "Surface training t=7179, loss=0.0189737631008029\n",
      "Surface training t=7180, loss=0.018731030635535717\n",
      "Surface training t=7181, loss=0.0186481736600399\n",
      "Surface training t=7182, loss=0.018452088348567486\n",
      "Surface training t=7183, loss=0.0190608911216259\n",
      "Surface training t=7184, loss=0.01951561588793993\n",
      "Surface training t=7185, loss=0.018866288475692272\n",
      "Surface training t=7186, loss=0.019288658164441586\n",
      "Surface training t=7187, loss=0.018800307996571064\n",
      "Surface training t=7188, loss=0.018480231054127216\n",
      "Surface training t=7189, loss=0.01831396296620369\n",
      "Surface training t=7190, loss=0.019215386360883713\n",
      "Surface training t=7191, loss=0.01928588841110468\n",
      "Surface training t=7192, loss=0.018527128733694553\n",
      "Surface training t=7193, loss=0.017763851210474968\n",
      "Surface training t=7194, loss=0.017725428566336632\n",
      "Surface training t=7195, loss=0.01762996055185795\n",
      "Surface training t=7196, loss=0.018495752476155758\n",
      "Surface training t=7197, loss=0.01888218056410551\n",
      "Surface training t=7198, loss=0.019092249684035778\n",
      "Surface training t=7199, loss=0.018321897834539413\n",
      "Surface training t=7200, loss=0.017676638439297676\n",
      "Surface training t=7201, loss=0.018263940699398518\n",
      "Surface training t=7202, loss=0.018435015343129635\n",
      "Surface training t=7203, loss=0.019166449084877968\n",
      "Surface training t=7204, loss=0.01907446701079607\n",
      "Surface training t=7205, loss=0.018389451317489147\n",
      "Surface training t=7206, loss=0.017535516060888767\n",
      "Surface training t=7207, loss=0.017694399692118168\n",
      "Surface training t=7208, loss=0.018526045605540276\n",
      "Surface training t=7209, loss=0.019545717164874077\n",
      "Surface training t=7210, loss=0.019968868233263493\n",
      "Surface training t=7211, loss=0.018620532006025314\n",
      "Surface training t=7212, loss=0.017474647611379623\n",
      "Surface training t=7213, loss=0.01808787416666746\n",
      "Surface training t=7214, loss=0.018385225906968117\n",
      "Surface training t=7215, loss=0.019178600050508976\n",
      "Surface training t=7216, loss=0.019187938421964645\n",
      "Surface training t=7217, loss=0.017753737047314644\n",
      "Surface training t=7218, loss=0.018019181676208973\n",
      "Surface training t=7219, loss=0.018019762821495533\n",
      "Surface training t=7220, loss=0.018660531379282475\n",
      "Surface training t=7221, loss=0.018961614929139614\n",
      "Surface training t=7222, loss=0.018972445279359818\n",
      "Surface training t=7223, loss=0.018167191185057163\n",
      "Surface training t=7224, loss=0.018053722567856312\n",
      "Surface training t=7225, loss=0.01787738688290119\n",
      "Surface training t=7226, loss=0.019515457563102245\n",
      "Surface training t=7227, loss=0.018631359562277794\n",
      "Surface training t=7228, loss=0.01822063233703375\n",
      "Surface training t=7229, loss=0.017401047982275486\n",
      "Surface training t=7230, loss=0.017569039948284626\n",
      "Surface training t=7231, loss=0.017776542343199253\n",
      "Surface training t=7232, loss=0.018667394295334816\n",
      "Surface training t=7233, loss=0.01876692660152912\n",
      "Surface training t=7234, loss=0.018418993800878525\n",
      "Surface training t=7235, loss=0.017943426966667175\n",
      "Surface training t=7236, loss=0.01747569814324379\n",
      "Surface training t=7237, loss=0.017705046571791172\n",
      "Surface training t=7238, loss=0.01914969552308321\n",
      "Surface training t=7239, loss=0.01823138166218996\n",
      "Surface training t=7240, loss=0.01842417288571596\n",
      "Surface training t=7241, loss=0.01815558597445488\n",
      "Surface training t=7242, loss=0.017551232129335403\n",
      "Surface training t=7243, loss=0.01761019043624401\n",
      "Surface training t=7244, loss=0.018095712177455425\n",
      "Surface training t=7245, loss=0.01929818745702505\n",
      "Surface training t=7246, loss=0.019012280739843845\n",
      "Surface training t=7247, loss=0.017719680909067392\n",
      "Surface training t=7248, loss=0.016605224460363388\n",
      "Surface training t=7249, loss=0.01707697194069624\n",
      "Surface training t=7250, loss=0.016688122414052486\n",
      "Surface training t=7251, loss=0.01611904986202717\n",
      "Surface training t=7252, loss=0.015849157236516476\n",
      "Surface training t=7253, loss=0.015506939496845007\n",
      "Surface training t=7254, loss=0.015903964173048735\n",
      "Surface training t=7255, loss=0.015049747191369534\n",
      "Surface training t=7256, loss=0.014832635410130024\n",
      "Surface training t=7257, loss=0.014944585505872965\n",
      "Surface training t=7258, loss=0.015265328343957663\n",
      "Surface training t=7259, loss=0.015052678994834423\n",
      "Surface training t=7260, loss=0.014647522009909153\n",
      "Surface training t=7261, loss=0.013982960488647223\n",
      "Surface training t=7262, loss=0.014807944186031818\n",
      "Surface training t=7263, loss=0.01469838758930564\n",
      "Surface training t=7264, loss=0.01490509370341897\n",
      "Surface training t=7265, loss=0.014450897928327322\n",
      "Surface training t=7266, loss=0.014294141437858343\n",
      "Surface training t=7267, loss=0.014423838816583157\n",
      "Surface training t=7268, loss=0.014457484241575003\n",
      "Surface training t=7269, loss=0.014134620781987906\n",
      "Surface training t=7270, loss=0.014416549820452929\n",
      "Surface training t=7271, loss=0.013779031578451395\n",
      "Surface training t=7272, loss=0.01363383000716567\n",
      "Surface training t=7273, loss=0.013476321008056402\n",
      "Surface training t=7274, loss=0.01387154869735241\n",
      "Surface training t=7275, loss=0.013814016245305538\n",
      "Surface training t=7276, loss=0.01371703390032053\n",
      "Surface training t=7277, loss=0.013756446074694395\n",
      "Surface training t=7278, loss=0.014294661115854979\n",
      "Surface training t=7279, loss=0.013804064132273197\n",
      "Surface training t=7280, loss=0.013969925232231617\n",
      "Surface training t=7281, loss=0.014038406778126955\n",
      "Surface training t=7282, loss=0.013822174165397882\n",
      "Surface training t=7283, loss=0.013448389247059822\n",
      "Surface training t=7284, loss=0.013755808118730783\n",
      "Surface training t=7285, loss=0.013461651280522346\n",
      "Surface training t=7286, loss=0.013686297461390495\n",
      "Surface training t=7287, loss=0.013531220611184835\n",
      "Surface training t=7288, loss=0.013837552163749933\n",
      "Surface training t=7289, loss=0.01318049943074584\n",
      "Surface training t=7290, loss=0.013517165556550026\n",
      "Surface training t=7291, loss=0.013341021724045277\n",
      "Surface training t=7292, loss=0.013679598458111286\n",
      "Surface training t=7293, loss=0.013189767021685839\n",
      "Surface training t=7294, loss=0.013443965464830399\n",
      "Surface training t=7295, loss=0.013075775001198053\n",
      "Surface training t=7296, loss=0.01340846810489893\n",
      "Surface training t=7297, loss=0.013244583271443844\n",
      "Surface training t=7298, loss=0.013145854230970144\n",
      "Surface training t=7299, loss=0.012941112276166677\n",
      "Surface training t=7300, loss=0.013827930204570293\n",
      "Surface training t=7301, loss=0.013389528263360262\n",
      "Surface training t=7302, loss=0.013792327139526606\n",
      "Surface training t=7303, loss=0.013455668464303017\n",
      "Surface training t=7304, loss=0.013754784595221281\n",
      "Surface training t=7305, loss=0.013340888544917107\n",
      "Surface training t=7306, loss=0.013369842432439327\n",
      "Surface training t=7307, loss=0.014136901590973139\n",
      "Surface training t=7308, loss=0.013199839740991592\n",
      "Surface training t=7309, loss=0.013160565868020058\n",
      "Surface training t=7310, loss=0.013237276580184698\n",
      "Surface training t=7311, loss=0.012727122753858566\n",
      "Surface training t=7312, loss=0.013739075977355242\n",
      "Surface training t=7313, loss=0.013184601441025734\n",
      "Surface training t=7314, loss=0.013301839586347342\n",
      "Surface training t=7315, loss=0.014013451989740133\n",
      "Surface training t=7316, loss=0.013289627153426409\n",
      "Surface training t=7317, loss=0.013181193731725216\n",
      "Surface training t=7318, loss=0.013378053903579712\n",
      "Surface training t=7319, loss=0.013101046439260244\n",
      "Surface training t=7320, loss=0.01279950374737382\n",
      "Surface training t=7321, loss=0.012961212079972029\n",
      "Surface training t=7322, loss=0.012807396240532398\n",
      "Surface training t=7323, loss=0.014606703538447618\n",
      "Surface training t=7324, loss=0.013707814738154411\n",
      "Surface training t=7325, loss=0.013625344261527061\n",
      "Surface training t=7326, loss=0.014268646948039532\n",
      "Surface training t=7327, loss=0.014764695428311825\n",
      "Surface training t=7328, loss=0.013266093097627163\n",
      "Surface training t=7329, loss=0.014536203350871801\n",
      "Surface training t=7330, loss=0.014009943697601557\n",
      "Surface training t=7331, loss=0.013347072526812553\n",
      "Surface training t=7332, loss=0.014203906524926424\n",
      "Surface training t=7333, loss=0.01411156915128231\n",
      "Surface training t=7334, loss=0.013011194299906492\n",
      "Surface training t=7335, loss=0.013452850747853518\n",
      "Surface training t=7336, loss=0.01350498292595148\n",
      "Surface training t=7337, loss=0.012888055294752121\n",
      "Surface training t=7338, loss=0.01367030618712306\n",
      "Surface training t=7339, loss=0.012937044259160757\n",
      "Surface training t=7340, loss=0.012818770948797464\n",
      "Surface training t=7341, loss=0.01409363467246294\n",
      "Surface training t=7342, loss=0.013752345461398363\n",
      "Surface training t=7343, loss=0.01336565101519227\n",
      "Surface training t=7344, loss=0.013244119007140398\n",
      "Surface training t=7345, loss=0.012911329977214336\n",
      "Surface training t=7346, loss=0.01296894159168005\n",
      "Surface training t=7347, loss=0.013155447784811258\n",
      "Surface training t=7348, loss=0.014143184758722782\n",
      "Surface training t=7349, loss=0.013550183735787868\n",
      "Surface training t=7350, loss=0.013038834556937218\n",
      "Surface training t=7351, loss=0.012697302270680666\n",
      "Surface training t=7352, loss=0.012846659403294325\n",
      "Surface training t=7353, loss=0.013869949150830507\n",
      "Surface training t=7354, loss=0.01418763305991888\n",
      "Surface training t=7355, loss=0.013043243903666735\n",
      "Surface training t=7356, loss=0.013798514381051064\n",
      "Surface training t=7357, loss=0.01380169065669179\n",
      "Surface training t=7358, loss=0.01318864431232214\n",
      "Surface training t=7359, loss=0.014123564586043358\n",
      "Surface training t=7360, loss=0.013323554769158363\n",
      "Surface training t=7361, loss=0.01268090633675456\n",
      "Surface training t=7362, loss=0.013801227323710918\n",
      "Surface training t=7363, loss=0.013365422375500202\n",
      "Surface training t=7364, loss=0.012881023343652487\n",
      "Surface training t=7365, loss=0.012773062102496624\n",
      "Surface training t=7366, loss=0.012472538743168116\n",
      "Surface training t=7367, loss=0.013021812308579683\n",
      "Surface training t=7368, loss=0.014231581706553698\n",
      "Surface training t=7369, loss=0.01373985642567277\n",
      "Surface training t=7370, loss=0.01356568094342947\n",
      "Surface training t=7371, loss=0.012765242718160152\n",
      "Surface training t=7372, loss=0.012763865757733583\n",
      "Surface training t=7373, loss=0.013139183167368174\n",
      "Surface training t=7374, loss=0.013299398124217987\n",
      "Surface training t=7375, loss=0.013505036942660809\n",
      "Surface training t=7376, loss=0.014218147844076157\n",
      "Surface training t=7377, loss=0.012856861110776663\n",
      "Surface training t=7378, loss=0.01486827852204442\n",
      "Surface training t=7379, loss=0.013051268178969622\n",
      "Surface training t=7380, loss=0.01401838194578886\n",
      "Surface training t=7381, loss=0.012924358248710632\n",
      "Surface training t=7382, loss=0.014679410494863987\n",
      "Surface training t=7383, loss=0.01246170699596405\n",
      "Surface training t=7384, loss=0.014303151983767748\n",
      "Surface training t=7385, loss=0.01259042089805007\n",
      "Surface training t=7386, loss=0.014454683288931847\n",
      "Surface training t=7387, loss=0.012835463043302298\n",
      "Surface training t=7388, loss=0.014570246916264296\n",
      "Surface training t=7389, loss=0.012739664874970913\n",
      "Surface training t=7390, loss=0.012039682362228632\n",
      "Surface training t=7391, loss=0.012574480846524239\n",
      "Surface training t=7392, loss=0.014292947016656399\n",
      "Surface training t=7393, loss=0.012371188029646873\n",
      "Surface training t=7394, loss=0.014674187172204256\n",
      "Surface training t=7395, loss=0.012838718015700579\n",
      "Surface training t=7396, loss=0.014153703581541777\n",
      "Surface training t=7397, loss=0.01307212607935071\n",
      "Surface training t=7398, loss=0.014133539982140064\n",
      "Surface training t=7399, loss=0.01384497107937932\n",
      "Surface training t=7400, loss=0.014486872591078281\n",
      "Surface training t=7401, loss=0.013587667606770992\n",
      "Surface training t=7402, loss=0.015309433452785015\n",
      "Surface training t=7403, loss=0.013993686996400356\n",
      "Surface training t=7404, loss=0.014743771404027939\n",
      "Surface training t=7405, loss=0.012939751613885164\n",
      "Surface training t=7406, loss=0.01524674566462636\n",
      "Surface training t=7407, loss=0.0137513461522758\n",
      "Surface training t=7408, loss=0.014180372003465891\n",
      "Surface training t=7409, loss=0.014635861851274967\n",
      "Surface training t=7410, loss=0.01441479567438364\n",
      "Surface training t=7411, loss=0.013399860821664333\n",
      "Surface training t=7412, loss=0.013085890095680952\n",
      "Surface training t=7413, loss=0.014148717746138573\n",
      "Surface training t=7414, loss=0.01373606314882636\n",
      "Surface training t=7415, loss=0.012350646313279867\n",
      "Surface training t=7416, loss=0.012484136503189802\n",
      "Surface training t=7417, loss=0.014372424222528934\n",
      "Surface training t=7418, loss=0.012677815742790699\n",
      "Surface training t=7419, loss=0.014596809633076191\n",
      "Surface training t=7420, loss=0.012188621796667576\n",
      "Surface training t=7421, loss=0.014447578694671392\n",
      "Surface training t=7422, loss=0.01245954493060708\n",
      "Surface training t=7423, loss=0.012541128788143396\n",
      "Surface training t=7424, loss=0.013857085723429918\n",
      "Surface training t=7425, loss=0.01214913884177804\n",
      "Surface training t=7426, loss=0.012606094591319561\n",
      "Surface training t=7427, loss=0.013844783417880535\n",
      "Surface training t=7428, loss=0.012478123884648085\n",
      "Surface training t=7429, loss=0.01209218055009842\n",
      "Surface training t=7430, loss=0.012322972062975168\n",
      "Surface training t=7431, loss=0.013175814878195524\n",
      "Surface training t=7432, loss=0.01423445250838995\n",
      "Surface training t=7433, loss=0.012643707916140556\n",
      "Surface training t=7434, loss=0.013518874533474445\n",
      "Surface training t=7435, loss=0.013598227873444557\n",
      "Surface training t=7436, loss=0.012490191031247377\n",
      "Surface training t=7437, loss=0.014156734570860863\n",
      "Surface training t=7438, loss=0.012736643198877573\n",
      "Surface training t=7439, loss=0.013330042362213135\n",
      "Surface training t=7440, loss=0.01364364568144083\n",
      "Surface training t=7441, loss=0.012731446884572506\n",
      "Surface training t=7442, loss=0.013895692769438028\n",
      "Surface training t=7443, loss=0.01344777038320899\n",
      "Surface training t=7444, loss=0.013350297696888447\n",
      "Surface training t=7445, loss=0.013076757546514273\n",
      "Surface training t=7446, loss=0.01271463604643941\n",
      "Surface training t=7447, loss=0.013825447764247656\n",
      "Surface training t=7448, loss=0.012327129486948252\n",
      "Surface training t=7449, loss=0.013330687303096056\n",
      "Surface training t=7450, loss=0.013876245357096195\n",
      "Surface training t=7451, loss=0.012214298360049725\n",
      "Surface training t=7452, loss=0.014468065928667784\n",
      "Surface training t=7453, loss=0.013524957466870546\n",
      "Surface training t=7454, loss=0.013800546526908875\n",
      "Surface training t=7455, loss=0.013031770009547472\n",
      "Surface training t=7456, loss=0.013785014394670725\n",
      "Surface training t=7457, loss=0.013768535573035479\n",
      "Surface training t=7458, loss=0.013414880726486444\n",
      "Surface training t=7459, loss=0.012575020082294941\n",
      "Surface training t=7460, loss=0.014186279848217964\n",
      "Surface training t=7461, loss=0.013121624942868948\n",
      "Surface training t=7462, loss=0.013390048407018185\n",
      "Surface training t=7463, loss=0.012299576308578253\n",
      "Surface training t=7464, loss=0.013803218957036734\n",
      "Surface training t=7465, loss=0.012088605668395758\n",
      "Surface training t=7466, loss=0.011745346710085869\n",
      "Surface training t=7467, loss=0.012338890228420496\n",
      "Surface training t=7468, loss=0.01385915232822299\n",
      "Surface training t=7469, loss=0.012856431305408478\n",
      "Surface training t=7470, loss=0.01326425326988101\n",
      "Surface training t=7471, loss=0.013029616326093674\n",
      "Surface training t=7472, loss=0.013208078686147928\n",
      "Surface training t=7473, loss=0.013103151228278875\n",
      "Surface training t=7474, loss=0.01318184332922101\n",
      "Surface training t=7475, loss=0.013493807520717382\n",
      "Surface training t=7476, loss=0.012831957545131445\n",
      "Surface training t=7477, loss=0.013824777211993933\n",
      "Surface training t=7478, loss=0.012280382215976715\n",
      "Surface training t=7479, loss=0.011953527573496103\n",
      "Surface training t=7480, loss=0.014452768955379725\n",
      "Surface training t=7481, loss=0.012244596611708403\n",
      "Surface training t=7482, loss=0.013371147215366364\n",
      "Surface training t=7483, loss=0.013758081011474133\n",
      "Surface training t=7484, loss=0.012670611962676048\n",
      "Surface training t=7485, loss=0.013285364955663681\n",
      "Surface training t=7486, loss=0.012519051786512136\n",
      "Surface training t=7487, loss=0.013113298453390598\n",
      "Surface training t=7488, loss=0.013432102743536234\n",
      "Surface training t=7489, loss=0.013088359031826258\n",
      "Surface training t=7490, loss=0.012529767584055662\n",
      "Surface training t=7491, loss=0.01325833098962903\n",
      "Surface training t=7492, loss=0.012936623767018318\n",
      "Surface training t=7493, loss=0.013151348102837801\n",
      "Surface training t=7494, loss=0.013104871846735477\n",
      "Surface training t=7495, loss=0.012102729640901089\n",
      "Surface training t=7496, loss=0.013343601953238249\n",
      "Surface training t=7497, loss=0.012003886979073286\n",
      "Surface training t=7498, loss=0.01332848472520709\n",
      "Surface training t=7499, loss=0.012569435872137547\n",
      "Surface training t=7500, loss=0.013280054554343224\n",
      "Surface training t=7501, loss=0.012014188803732395\n",
      "Surface training t=7502, loss=0.012575068045407534\n",
      "Surface training t=7503, loss=0.012541110161691904\n",
      "Surface training t=7504, loss=0.013440634589642286\n",
      "Surface training t=7505, loss=0.012872171588242054\n",
      "Surface training t=7506, loss=0.011491407174617052\n",
      "Surface training t=7507, loss=0.013103301171213388\n",
      "Surface training t=7508, loss=0.013487427495419979\n",
      "Surface training t=7509, loss=0.01225993549451232\n",
      "Surface training t=7510, loss=0.013416938483715057\n",
      "Surface training t=7511, loss=0.013390999287366867\n",
      "Surface training t=7512, loss=0.012106215115636587\n",
      "Surface training t=7513, loss=0.013788378797471523\n",
      "Surface training t=7514, loss=0.012583157513290644\n",
      "Surface training t=7515, loss=0.013403464574366808\n",
      "Surface training t=7516, loss=0.012760672252625227\n",
      "Surface training t=7517, loss=0.013865714892745018\n",
      "Surface training t=7518, loss=0.011750064324587584\n",
      "Surface training t=7519, loss=0.013046310283243656\n",
      "Surface training t=7520, loss=0.013004414737224579\n",
      "Surface training t=7521, loss=0.012401498388499022\n",
      "Surface training t=7522, loss=0.014117504935711622\n",
      "Surface training t=7523, loss=0.01244724402204156\n",
      "Surface training t=7524, loss=0.013917709235101938\n",
      "Surface training t=7525, loss=0.012574359774589539\n",
      "Surface training t=7526, loss=0.011890992522239685\n",
      "Surface training t=7527, loss=0.013678806368261576\n",
      "Surface training t=7528, loss=0.012404958251863718\n",
      "Surface training t=7529, loss=0.014319002628326416\n",
      "Surface training t=7530, loss=0.015905688516795635\n",
      "Surface training t=7531, loss=0.016446247696876526\n",
      "Surface training t=7532, loss=0.015176074579358101\n",
      "Surface training t=7533, loss=0.011924677062779665\n",
      "Surface training t=7534, loss=0.014029952697455883\n",
      "Surface training t=7535, loss=0.014600762631744146\n",
      "Surface training t=7536, loss=0.015843562316149473\n",
      "Surface training t=7537, loss=0.01741656195372343\n",
      "Surface training t=7538, loss=0.016138209961354733\n",
      "Surface training t=7539, loss=0.015752580948174\n",
      "Surface training t=7540, loss=0.013838368467986584\n",
      "Surface training t=7541, loss=0.013621611054986715\n",
      "Surface training t=7542, loss=0.013754596933722496\n",
      "Surface training t=7543, loss=0.013771099038422108\n",
      "Surface training t=7544, loss=0.015632919501513243\n",
      "Surface training t=7545, loss=0.017625177279114723\n",
      "Surface training t=7546, loss=0.016763598658144474\n",
      "Surface training t=7547, loss=0.015655490569770336\n",
      "Surface training t=7548, loss=0.013170498888939619\n",
      "Surface training t=7549, loss=0.01309923641383648\n",
      "Surface training t=7550, loss=0.013852899894118309\n",
      "Surface training t=7551, loss=0.013693285640329123\n",
      "Surface training t=7552, loss=0.014534892048686743\n",
      "Surface training t=7553, loss=0.013741404749453068\n",
      "Surface training t=7554, loss=0.014059372246265411\n",
      "Surface training t=7555, loss=0.015529876574873924\n",
      "Surface training t=7556, loss=0.018305244855582714\n",
      "Surface training t=7557, loss=0.02279040403664112\n",
      "Surface training t=7558, loss=0.01853236136958003\n",
      "Surface training t=7559, loss=0.016633386258035898\n",
      "Surface training t=7560, loss=0.015016351360827684\n",
      "Surface training t=7561, loss=0.014618419110774994\n",
      "Surface training t=7562, loss=0.012172571383416653\n",
      "Surface training t=7563, loss=0.013554856646806002\n",
      "Surface training t=7564, loss=0.012216012459248304\n",
      "Surface training t=7565, loss=0.013548447750508785\n",
      "Surface training t=7566, loss=0.011864061001688242\n",
      "Surface training t=7567, loss=0.011552944779396057\n",
      "Surface training t=7568, loss=0.014180455356836319\n",
      "Surface training t=7569, loss=0.015415186993777752\n",
      "Surface training t=7570, loss=0.016449652146548033\n",
      "Surface training t=7571, loss=0.020522665232419968\n",
      "Surface training t=7572, loss=0.020781084895133972\n",
      "Surface training t=7573, loss=0.019142156466841698\n",
      "Surface training t=7574, loss=0.01965058036148548\n",
      "Surface training t=7575, loss=0.018171191215515137\n",
      "Surface training t=7576, loss=0.01869562454521656\n",
      "Surface training t=7577, loss=0.019939632155001163\n",
      "Surface training t=7578, loss=0.02164923120290041\n",
      "Surface training t=7579, loss=0.020481466315686703\n",
      "Surface training t=7580, loss=0.013671480119228363\n",
      "Surface training t=7581, loss=0.015287961810827255\n",
      "Surface training t=7582, loss=0.01956118270754814\n",
      "Surface training t=7583, loss=0.024526456370949745\n",
      "Surface training t=7584, loss=0.01661130879074335\n",
      "Surface training t=7585, loss=0.01676071062684059\n",
      "Surface training t=7586, loss=0.016640848945826292\n",
      "Surface training t=7587, loss=0.023950103670358658\n",
      "Surface training t=7588, loss=0.017230658791959286\n",
      "Surface training t=7589, loss=0.018366485834121704\n",
      "Surface training t=7590, loss=0.015027559362351894\n",
      "Surface training t=7591, loss=0.0199412414804101\n",
      "Surface training t=7592, loss=0.019803176634013653\n",
      "Surface training t=7593, loss=0.019847890362143517\n",
      "Surface training t=7594, loss=0.016360124573111534\n",
      "Surface training t=7595, loss=0.017951207235455513\n",
      "Surface training t=7596, loss=0.020713986828923225\n",
      "Surface training t=7597, loss=0.016660944558680058\n",
      "Surface training t=7598, loss=0.017486470751464367\n",
      "Surface training t=7599, loss=0.019878430292010307\n",
      "Surface training t=7600, loss=0.020621594041585922\n",
      "Surface training t=7601, loss=0.017150134779512882\n",
      "Surface training t=7602, loss=0.019061450846493244\n",
      "Surface training t=7603, loss=0.01850698422640562\n",
      "Surface training t=7604, loss=0.018885775469243526\n",
      "Surface training t=7605, loss=0.01711746584624052\n",
      "Surface training t=7606, loss=0.017295855563133955\n",
      "Surface training t=7607, loss=0.019809582270681858\n",
      "Surface training t=7608, loss=0.018340996466577053\n",
      "Surface training t=7609, loss=0.018045049160718918\n",
      "Surface training t=7610, loss=0.016386666800826788\n",
      "Surface training t=7611, loss=0.018442682921886444\n",
      "Surface training t=7612, loss=0.020509074442088604\n",
      "Surface training t=7613, loss=0.017768082208931446\n",
      "Surface training t=7614, loss=0.017487685196101665\n",
      "Surface training t=7615, loss=0.016794852446764708\n",
      "Surface training t=7616, loss=0.019543218426406384\n",
      "Surface training t=7617, loss=0.018374389968812466\n",
      "Surface training t=7618, loss=0.01828981377184391\n",
      "Surface training t=7619, loss=0.01746363192796707\n",
      "Surface training t=7620, loss=0.018241137266159058\n",
      "Surface training t=7621, loss=0.017490861006081104\n",
      "Surface training t=7622, loss=0.01838821731507778\n",
      "Surface training t=7623, loss=0.01896193716675043\n",
      "Surface training t=7624, loss=0.018310120329260826\n",
      "Surface training t=7625, loss=0.01679627224802971\n",
      "Surface training t=7626, loss=0.01753609161823988\n",
      "Surface training t=7627, loss=0.018551096320152283\n",
      "Surface training t=7628, loss=0.02021531295031309\n",
      "Surface training t=7629, loss=0.016414178535342216\n",
      "Surface training t=7630, loss=0.016608314588665962\n",
      "Surface training t=7631, loss=0.017057853285223246\n",
      "Surface training t=7632, loss=0.019623173400759697\n",
      "Surface training t=7633, loss=0.01752213668078184\n",
      "Surface training t=7634, loss=0.01749008148908615\n",
      "Surface training t=7635, loss=0.018104580231010914\n",
      "Surface training t=7636, loss=0.01737262960523367\n",
      "Surface training t=7637, loss=0.017149039544165134\n",
      "Surface training t=7638, loss=0.020030594430863857\n",
      "Surface training t=7639, loss=0.01632033661007881\n",
      "Surface training t=7640, loss=0.017216497100889683\n",
      "Surface training t=7641, loss=0.016189515125006437\n",
      "Surface training t=7642, loss=0.016417952720075846\n",
      "Surface training t=7643, loss=0.020254090428352356\n",
      "Surface training t=7644, loss=0.016396530903875828\n",
      "Surface training t=7645, loss=0.019998875446617603\n",
      "Surface training t=7646, loss=0.014861654490232468\n",
      "Surface training t=7647, loss=0.017259483225643635\n",
      "Surface training t=7648, loss=0.016977699007838964\n",
      "Surface training t=7649, loss=0.01961371023207903\n",
      "Surface training t=7650, loss=0.01832319237291813\n",
      "Surface training t=7651, loss=0.016322416253387928\n",
      "Surface training t=7652, loss=0.017653288319706917\n",
      "Surface training t=7653, loss=0.01859225519001484\n",
      "Surface training t=7654, loss=0.01726906280964613\n",
      "Surface training t=7655, loss=0.01627905247732997\n",
      "Surface training t=7656, loss=0.018317722715437412\n",
      "Surface training t=7657, loss=0.01802169159054756\n",
      "Surface training t=7658, loss=0.017873789183795452\n",
      "Surface training t=7659, loss=0.016836208291351795\n",
      "Surface training t=7660, loss=0.01688543241471052\n",
      "Surface training t=7661, loss=0.01948434952646494\n",
      "Surface training t=7662, loss=0.017805459909141064\n",
      "Surface training t=7663, loss=0.01600581780076027\n",
      "Surface training t=7664, loss=0.014617865905165672\n",
      "Surface training t=7665, loss=0.01753481337800622\n",
      "Surface training t=7666, loss=0.018622901290655136\n",
      "Surface training t=7667, loss=0.018704845570027828\n",
      "Surface training t=7668, loss=0.016339251771569252\n",
      "Surface training t=7669, loss=0.01767773926258087\n",
      "Surface training t=7670, loss=0.01819815393537283\n",
      "Surface training t=7671, loss=0.017101196572184563\n",
      "Surface training t=7672, loss=0.01644089911133051\n",
      "Surface training t=7673, loss=0.017979699186980724\n",
      "Surface training t=7674, loss=0.018529257737100124\n",
      "Surface training t=7675, loss=0.016078801825642586\n",
      "Surface training t=7676, loss=0.01599061395972967\n",
      "Surface training t=7677, loss=0.017247986048460007\n",
      "Surface training t=7678, loss=0.019064691849052906\n",
      "Surface training t=7679, loss=0.01771185826510191\n",
      "Surface training t=7680, loss=0.01606137491762638\n",
      "Surface training t=7681, loss=0.016530287452042103\n",
      "Surface training t=7682, loss=0.016267450992017984\n",
      "Surface training t=7683, loss=0.019553441554307938\n",
      "Surface training t=7684, loss=0.016769757494330406\n",
      "Surface training t=7685, loss=0.016779903322458267\n",
      "Surface training t=7686, loss=0.01580193219706416\n",
      "Surface training t=7687, loss=0.017978391610085964\n",
      "Surface training t=7688, loss=0.017215121537446976\n",
      "Surface training t=7689, loss=0.017575718462467194\n",
      "Surface training t=7690, loss=0.017184450291097164\n",
      "Surface training t=7691, loss=0.016715383157134056\n",
      "Surface training t=7692, loss=0.01628284342586994\n",
      "Surface training t=7693, loss=0.018036791123449802\n",
      "Surface training t=7694, loss=0.01757704559713602\n",
      "Surface training t=7695, loss=0.0176305016502738\n",
      "Surface training t=7696, loss=0.01619275752454996\n",
      "Surface training t=7697, loss=0.017317162826657295\n",
      "Surface training t=7698, loss=0.016546722035855055\n",
      "Surface training t=7699, loss=0.0183533588424325\n",
      "Surface training t=7700, loss=0.016488938592374325\n",
      "Surface training t=7701, loss=0.016796107403934002\n",
      "Surface training t=7702, loss=0.017692881636321545\n",
      "Surface training t=7703, loss=0.01687890011817217\n",
      "Surface training t=7704, loss=0.01574478531256318\n",
      "Surface training t=7705, loss=0.016186813358217478\n",
      "Surface training t=7706, loss=0.019008605740964413\n",
      "Surface training t=7707, loss=0.016152219846844673\n",
      "Surface training t=7708, loss=0.016594392247498035\n",
      "Surface training t=7709, loss=0.016748243011534214\n",
      "Surface training t=7710, loss=0.017429176717996597\n",
      "Surface training t=7711, loss=0.01727071963250637\n",
      "Surface training t=7712, loss=0.016284326557070017\n",
      "Surface training t=7713, loss=0.016719084233045578\n",
      "Surface training t=7714, loss=0.016330227721482515\n",
      "Surface training t=7715, loss=0.017174105159938335\n",
      "Surface training t=7716, loss=0.016365086659789085\n",
      "Surface training t=7717, loss=0.016539042815566063\n",
      "Surface training t=7718, loss=0.01606868300586939\n",
      "Surface training t=7719, loss=0.017176720313727856\n",
      "Surface training t=7720, loss=0.018514104187488556\n",
      "Surface training t=7721, loss=0.016136434860527515\n",
      "Surface training t=7722, loss=0.016280457377433777\n",
      "Surface training t=7723, loss=0.016316150315105915\n",
      "Surface training t=7724, loss=0.01748615875840187\n",
      "Surface training t=7725, loss=0.016351137310266495\n",
      "Surface training t=7726, loss=0.014472065027803183\n",
      "Surface training t=7727, loss=0.01773891504853964\n",
      "Surface training t=7728, loss=0.016672187950462103\n",
      "Surface training t=7729, loss=0.018593239597976208\n",
      "Surface training t=7730, loss=0.01512504555284977\n",
      "Surface training t=7731, loss=0.01661339681595564\n",
      "Surface training t=7732, loss=0.015701809898018837\n",
      "Surface training t=7733, loss=0.016537451650947332\n",
      "Surface training t=7734, loss=0.017923242412507534\n",
      "Surface training t=7735, loss=0.015563905704766512\n",
      "Surface training t=7736, loss=0.014980883803218603\n",
      "Surface training t=7737, loss=0.017396915704011917\n",
      "Surface training t=7738, loss=0.01941437181085348\n",
      "Surface training t=7739, loss=0.015176910907030106\n",
      "Surface training t=7740, loss=0.014490146655589342\n",
      "Surface training t=7741, loss=0.016065142583101988\n",
      "Surface training t=7742, loss=0.018863520585000515\n",
      "Surface training t=7743, loss=0.017360515892505646\n",
      "Surface training t=7744, loss=0.016290065832436085\n",
      "Surface training t=7745, loss=0.015837408136576414\n",
      "Surface training t=7746, loss=0.015270845964550972\n",
      "Surface training t=7747, loss=0.01806547027081251\n",
      "Surface training t=7748, loss=0.015624936670064926\n",
      "Surface training t=7749, loss=0.017832075245678425\n",
      "Surface training t=7750, loss=0.014948833733797073\n",
      "Surface training t=7751, loss=0.016713051591068506\n",
      "Surface training t=7752, loss=0.014983847737312317\n",
      "Surface training t=7753, loss=0.017244494520127773\n",
      "Surface training t=7754, loss=0.017413410358130932\n",
      "Surface training t=7755, loss=0.016578241251409054\n",
      "Surface training t=7756, loss=0.0168041680008173\n",
      "Surface training t=7757, loss=0.015422152820974588\n",
      "Surface training t=7758, loss=0.015277558472007513\n",
      "Surface training t=7759, loss=0.017007751390337944\n",
      "Surface training t=7760, loss=0.017050025053322315\n",
      "Surface training t=7761, loss=0.01639491133391857\n",
      "Surface training t=7762, loss=0.01493614399805665\n",
      "Surface training t=7763, loss=0.017221168614923954\n",
      "Surface training t=7764, loss=0.017274520359933376\n",
      "Surface training t=7765, loss=0.01643257588148117\n",
      "Surface training t=7766, loss=0.016221163794398308\n",
      "Surface training t=7767, loss=0.016078984830528498\n",
      "Surface training t=7768, loss=0.016194733791053295\n",
      "Surface training t=7769, loss=0.016528199892491102\n",
      "Surface training t=7770, loss=0.01611467730253935\n",
      "Surface training t=7771, loss=0.01781357452273369\n",
      "Surface training t=7772, loss=0.014765885192900896\n",
      "Surface training t=7773, loss=0.01642999891191721\n",
      "Surface training t=7774, loss=0.015931046567857265\n",
      "Surface training t=7775, loss=0.0183163033798337\n",
      "Surface training t=7776, loss=0.014966852962970734\n",
      "Surface training t=7777, loss=0.015983643010258675\n",
      "Surface training t=7778, loss=0.015704569406807423\n",
      "Surface training t=7779, loss=0.018121459521353245\n",
      "Surface training t=7780, loss=0.016444029286503792\n",
      "Surface training t=7781, loss=0.016078236512839794\n",
      "Surface training t=7782, loss=0.014924666378647089\n",
      "Surface training t=7783, loss=0.016874469816684723\n",
      "Surface training t=7784, loss=0.016160229220986366\n",
      "Surface training t=7785, loss=0.01608971878886223\n",
      "Surface training t=7786, loss=0.014819953590631485\n",
      "Surface training t=7787, loss=0.017861152067780495\n",
      "Surface training t=7788, loss=0.01643672212958336\n",
      "Surface training t=7789, loss=0.016704974696040154\n",
      "Surface training t=7790, loss=0.014708077069371939\n",
      "Surface training t=7791, loss=0.015860979445278645\n",
      "Surface training t=7792, loss=0.017715640366077423\n",
      "Surface training t=7793, loss=0.016909806057810783\n",
      "Surface training t=7794, loss=0.014834743458777666\n",
      "Surface training t=7795, loss=0.015596253331750631\n",
      "Surface training t=7796, loss=0.014423239044845104\n",
      "Surface training t=7797, loss=0.017361341044306755\n",
      "Surface training t=7798, loss=0.016813979484140873\n",
      "Surface training t=7799, loss=0.015870780684053898\n",
      "Surface training t=7800, loss=0.014228991698473692\n",
      "Surface training t=7801, loss=0.01653029303997755\n",
      "Surface training t=7802, loss=0.017702365294098854\n",
      "Surface training t=7803, loss=0.01527799479663372\n",
      "Surface training t=7804, loss=0.014786635059863329\n",
      "Surface training t=7805, loss=0.016227327287197113\n",
      "Surface training t=7806, loss=0.01823416445404291\n",
      "Surface training t=7807, loss=0.014878959394991398\n",
      "Surface training t=7808, loss=0.014507303945720196\n",
      "Surface training t=7809, loss=0.015895165503025055\n",
      "Surface training t=7810, loss=0.018348180688917637\n",
      "Surface training t=7811, loss=0.015303275547921658\n",
      "Surface training t=7812, loss=0.01387077011168003\n",
      "Surface training t=7813, loss=0.015359412413090467\n",
      "Surface training t=7814, loss=0.018271380104124546\n",
      "Surface training t=7815, loss=0.015872225165367126\n",
      "Surface training t=7816, loss=0.014022581744939089\n",
      "Surface training t=7817, loss=0.01578916097059846\n",
      "Surface training t=7818, loss=0.01598646165803075\n",
      "Surface training t=7819, loss=0.018872058019042015\n",
      "Surface training t=7820, loss=0.01494567096233368\n",
      "Surface training t=7821, loss=0.015658108051866293\n",
      "Surface training t=7822, loss=0.014705192763358355\n",
      "Surface training t=7823, loss=0.015882613603025675\n",
      "Surface training t=7824, loss=0.015849375631660223\n",
      "Surface training t=7825, loss=0.017115273512899876\n",
      "Surface training t=7826, loss=0.016474898904561996\n",
      "Surface training t=7827, loss=0.01571357762441039\n",
      "Surface training t=7828, loss=0.014511846005916595\n",
      "Surface training t=7829, loss=0.01568909687921405\n",
      "Surface training t=7830, loss=0.016042492352426052\n",
      "Surface training t=7831, loss=0.01615135185420513\n",
      "Surface training t=7832, loss=0.01435495587065816\n",
      "Surface training t=7833, loss=0.016543162520974874\n",
      "Surface training t=7834, loss=0.016267042607069016\n",
      "Surface training t=7835, loss=0.016837895847857\n",
      "Surface training t=7836, loss=0.015807785093784332\n",
      "Surface training t=7837, loss=0.01471675094217062\n",
      "Surface training t=7838, loss=0.01561663905158639\n",
      "Surface training t=7839, loss=0.015700020827353\n",
      "Surface training t=7840, loss=0.01724295038729906\n",
      "Surface training t=7841, loss=0.014162058476358652\n",
      "Surface training t=7842, loss=0.014160091988742352\n",
      "Surface training t=7843, loss=0.01563386106863618\n",
      "Surface training t=7844, loss=0.017783072777092457\n",
      "Surface training t=7845, loss=0.0170104019343853\n",
      "Surface training t=7846, loss=0.01526612276211381\n",
      "Surface training t=7847, loss=0.01487776217982173\n",
      "Surface training t=7848, loss=0.015654340386390686\n",
      "Surface training t=7849, loss=0.017341368831694126\n",
      "Surface training t=7850, loss=0.015123080462217331\n",
      "Surface training t=7851, loss=0.01630980893969536\n",
      "Surface training t=7852, loss=0.015066949184983969\n",
      "Surface training t=7853, loss=0.015326394699513912\n",
      "Surface training t=7854, loss=0.014678622595965862\n",
      "Surface training t=7855, loss=0.015547120943665504\n",
      "Surface training t=7856, loss=0.016752416267991066\n",
      "Surface training t=7857, loss=0.01567831262946129\n",
      "Surface training t=7858, loss=0.01479006977751851\n",
      "Surface training t=7859, loss=0.01594909466803074\n",
      "Surface training t=7860, loss=0.015616933815181255\n",
      "Surface training t=7861, loss=0.01665440294891596\n",
      "Surface training t=7862, loss=0.01344403438270092\n",
      "Surface training t=7863, loss=0.014704606030136347\n",
      "Surface training t=7864, loss=0.014009551145136356\n",
      "Surface training t=7865, loss=0.016728668473660946\n",
      "Surface training t=7866, loss=0.017809291370213032\n",
      "Surface training t=7867, loss=0.014774186536669731\n",
      "Surface training t=7868, loss=0.014216911513358355\n",
      "Surface training t=7869, loss=0.014525758102536201\n",
      "Surface training t=7870, loss=0.017793496139347553\n",
      "Surface training t=7871, loss=0.015179789625108242\n",
      "Surface training t=7872, loss=0.014630389399826527\n",
      "Surface training t=7873, loss=0.01493852911517024\n",
      "Surface training t=7874, loss=0.015650690998882055\n",
      "Surface training t=7875, loss=0.016839854419231415\n",
      "Surface training t=7876, loss=0.01450352231040597\n",
      "Surface training t=7877, loss=0.01551949791610241\n",
      "Surface training t=7878, loss=0.014985604677349329\n",
      "Surface training t=7879, loss=0.016498812939971685\n",
      "Surface training t=7880, loss=0.016096655279397964\n",
      "Surface training t=7881, loss=0.015704993158578873\n",
      "Surface training t=7882, loss=0.01497289864346385\n",
      "Surface training t=7883, loss=0.015414187218993902\n",
      "Surface training t=7884, loss=0.014810800086706877\n",
      "Surface training t=7885, loss=0.016518237069249153\n",
      "Surface training t=7886, loss=0.01371294166892767\n",
      "Surface training t=7887, loss=0.014625755604356527\n",
      "Surface training t=7888, loss=0.013719968497753143\n",
      "Surface training t=7889, loss=0.015143896918743849\n",
      "Surface training t=7890, loss=0.016412630677223206\n",
      "Surface training t=7891, loss=0.016301306895911694\n",
      "Surface training t=7892, loss=0.015705375466495752\n",
      "Surface training t=7893, loss=0.014936467632651329\n",
      "Surface training t=7894, loss=0.014839635696262121\n",
      "Surface training t=7895, loss=0.016481016762554646\n",
      "Surface training t=7896, loss=0.01596651878207922\n",
      "Surface training t=7897, loss=0.015263322740793228\n",
      "Surface training t=7898, loss=0.013939485885202885\n",
      "Surface training t=7899, loss=0.014607490971684456\n",
      "Surface training t=7900, loss=0.015718191396445036\n",
      "Surface training t=7901, loss=0.015644889790564775\n",
      "Surface training t=7902, loss=0.01561905536800623\n",
      "Surface training t=7903, loss=0.015049672219902277\n",
      "Surface training t=7904, loss=0.014655502047389746\n",
      "Surface training t=7905, loss=0.015786800999194384\n",
      "Surface training t=7906, loss=0.01495796674862504\n",
      "Surface training t=7907, loss=0.016081247944384813\n",
      "Surface training t=7908, loss=0.015105886850506067\n",
      "Surface training t=7909, loss=0.014851832762360573\n",
      "Surface training t=7910, loss=0.01584323961287737\n",
      "Surface training t=7911, loss=0.015705252531915903\n",
      "Surface training t=7912, loss=0.015898984856903553\n",
      "Surface training t=7913, loss=0.01435502665117383\n",
      "Surface training t=7914, loss=0.013887430541217327\n",
      "Surface training t=7915, loss=0.015121875796467066\n",
      "Surface training t=7916, loss=0.01574948662891984\n",
      "Surface training t=7917, loss=0.01585326809436083\n",
      "Surface training t=7918, loss=0.01472538523375988\n",
      "Surface training t=7919, loss=0.01569021213799715\n",
      "Surface training t=7920, loss=0.015063165221363306\n",
      "Surface training t=7921, loss=0.015589411370456219\n",
      "Surface training t=7922, loss=0.016080706380307674\n",
      "Surface training t=7923, loss=0.013946541585028172\n",
      "Surface training t=7924, loss=0.01322123222053051\n",
      "Surface training t=7925, loss=0.015034227631986141\n",
      "Surface training t=7926, loss=0.015946433879435062\n",
      "Surface training t=7927, loss=0.016030725091695786\n",
      "Surface training t=7928, loss=0.01376850763335824\n",
      "Surface training t=7929, loss=0.013958454132080078\n",
      "Surface training t=7930, loss=0.013672732282429934\n",
      "Surface training t=7931, loss=0.016817377880215645\n",
      "Surface training t=7932, loss=0.01562744239345193\n",
      "Surface training t=7933, loss=0.014353279955685139\n",
      "Surface training t=7934, loss=0.012751625385135412\n",
      "Surface training t=7935, loss=0.013513803016394377\n",
      "Surface training t=7936, loss=0.016395380720496178\n",
      "Surface training t=7937, loss=0.016319801565259695\n",
      "Surface training t=7938, loss=0.016538445837795734\n",
      "Surface training t=7939, loss=0.01394711621105671\n",
      "Surface training t=7940, loss=0.013537560123950243\n",
      "Surface training t=7941, loss=0.014475886709988117\n",
      "Surface training t=7942, loss=0.017519029788672924\n",
      "Surface training t=7943, loss=0.014905866701155901\n",
      "Surface training t=7944, loss=0.014847864862531424\n",
      "Surface training t=7945, loss=0.014188389293849468\n",
      "Surface training t=7946, loss=0.01463255425915122\n",
      "Surface training t=7947, loss=0.015902049839496613\n",
      "Surface training t=7948, loss=0.016811968758702278\n",
      "Surface training t=7949, loss=0.014101308770477772\n",
      "Surface training t=7950, loss=0.013331125024706125\n",
      "Surface training t=7951, loss=0.014374278020113707\n",
      "Surface training t=7952, loss=0.015664651058614254\n",
      "Surface training t=7953, loss=0.014762342907488346\n",
      "Surface training t=7954, loss=0.01456093369051814\n",
      "Surface training t=7955, loss=0.014612296596169472\n",
      "Surface training t=7956, loss=0.015337123535573483\n",
      "Surface training t=7957, loss=0.015462486073374748\n",
      "Surface training t=7958, loss=0.015321610029786825\n",
      "Surface training t=7959, loss=0.015412603504955769\n",
      "Surface training t=7960, loss=0.014378495514392853\n",
      "Surface training t=7961, loss=0.01482531288638711\n",
      "Surface training t=7962, loss=0.014195901341736317\n",
      "Surface training t=7963, loss=0.015065887477248907\n",
      "Surface training t=7964, loss=0.016202131286263466\n",
      "Surface training t=7965, loss=0.013756001833826303\n",
      "Surface training t=7966, loss=0.014684483408927917\n",
      "Surface training t=7967, loss=0.015817913692444563\n",
      "Surface training t=7968, loss=0.016716710291802883\n",
      "Surface training t=7969, loss=0.013811743818223476\n",
      "Surface training t=7970, loss=0.013267226051539183\n",
      "Surface training t=7971, loss=0.01358141377568245\n",
      "Surface training t=7972, loss=0.015275507234036922\n",
      "Surface training t=7973, loss=0.016185203567147255\n",
      "Surface training t=7974, loss=0.01513769431039691\n",
      "Surface training t=7975, loss=0.014500935096293688\n",
      "Surface training t=7976, loss=0.014229798223823309\n",
      "Surface training t=7977, loss=0.014438645914196968\n",
      "Surface training t=7978, loss=0.01756907533854246\n",
      "Surface training t=7979, loss=0.014158683829009533\n",
      "Surface training t=7980, loss=0.014857292175292969\n",
      "Surface training t=7981, loss=0.013862007297575474\n",
      "Surface training t=7982, loss=0.01587773533537984\n",
      "Surface training t=7983, loss=0.016135712154209614\n",
      "Surface training t=7984, loss=0.014352572150528431\n",
      "Surface training t=7985, loss=0.013709526974707842\n",
      "Surface training t=7986, loss=0.01375999255105853\n",
      "Surface training t=7987, loss=0.014749305322766304\n",
      "Surface training t=7988, loss=0.01637679524719715\n",
      "Surface training t=7989, loss=0.014655655715614557\n",
      "Surface training t=7990, loss=0.014431367628276348\n",
      "Surface training t=7991, loss=0.014004604425281286\n",
      "Surface training t=7992, loss=0.015160684008151293\n",
      "Surface training t=7993, loss=0.01428527431562543\n",
      "Surface training t=7994, loss=0.015511677134782076\n",
      "Surface training t=7995, loss=0.014987481757998466\n",
      "Surface training t=7996, loss=0.014361337292939425\n",
      "Surface training t=7997, loss=0.014550289139151573\n",
      "Surface training t=7998, loss=0.013838480226695538\n",
      "Surface training t=7999, loss=0.014329483732581139\n",
      "Surface training t=8000, loss=0.014447001740336418\n",
      "Surface training t=8001, loss=0.014528775587677956\n",
      "Surface training t=8002, loss=0.013883295003324747\n",
      "Surface training t=8003, loss=0.014846826437860727\n",
      "Surface training t=8004, loss=0.014542063232511282\n",
      "Surface training t=8005, loss=0.014471729751676321\n",
      "Surface training t=8006, loss=0.015066503547132015\n",
      "Surface training t=8007, loss=0.014657715801149607\n",
      "Surface training t=8008, loss=0.014865597244352102\n",
      "Surface training t=8009, loss=0.015203255694359541\n",
      "Surface training t=8010, loss=0.014652548357844353\n",
      "Surface training t=8011, loss=0.01402724627405405\n",
      "Surface training t=8012, loss=0.015360361896455288\n",
      "Surface training t=8013, loss=0.014296286273747683\n",
      "Surface training t=8014, loss=0.015125587582588196\n",
      "Surface training t=8015, loss=0.012917201034724712\n",
      "Surface training t=8016, loss=0.013907878194004297\n",
      "Surface training t=8017, loss=0.015016624238342047\n",
      "Surface training t=8018, loss=0.016907192766666412\n",
      "Surface training t=8019, loss=0.014319146517664194\n",
      "Surface training t=8020, loss=0.014121316839009523\n",
      "Surface training t=8021, loss=0.013410268351435661\n",
      "Surface training t=8022, loss=0.015111132524907589\n",
      "Surface training t=8023, loss=0.015446361154317856\n",
      "Surface training t=8024, loss=0.015054973773658276\n",
      "Surface training t=8025, loss=0.01501817861571908\n",
      "Surface training t=8026, loss=0.0138945821672678\n",
      "Surface training t=8027, loss=0.012772152666002512\n",
      "Surface training t=8028, loss=0.013059830758720636\n",
      "Surface training t=8029, loss=0.01431808527559042\n",
      "Surface training t=8030, loss=0.016652196645736694\n",
      "Surface training t=8031, loss=0.013718157541006804\n",
      "Surface training t=8032, loss=0.014103259425610304\n",
      "Surface training t=8033, loss=0.012967097107321024\n",
      "Surface training t=8034, loss=0.013628249522298574\n",
      "Surface training t=8035, loss=0.015259583946317434\n",
      "Surface training t=8036, loss=0.017491833306849003\n",
      "Surface training t=8037, loss=0.013485393021255732\n",
      "Surface training t=8038, loss=0.013693701941519976\n",
      "Surface training t=8039, loss=0.013225441332906485\n",
      "Surface training t=8040, loss=0.014409285504370928\n",
      "Surface training t=8041, loss=0.01613608794286847\n",
      "Surface training t=8042, loss=0.015412730164825916\n",
      "Surface training t=8043, loss=0.014211631380021572\n",
      "Surface training t=8044, loss=0.013309812638908625\n",
      "Surface training t=8045, loss=0.01431560656055808\n",
      "Surface training t=8046, loss=0.017036931589245796\n",
      "Surface training t=8047, loss=0.0137268234975636\n",
      "Surface training t=8048, loss=0.0128421769477427\n",
      "Surface training t=8049, loss=0.011609155684709549\n",
      "Surface training t=8050, loss=0.011905591003596783\n",
      "Surface training t=8051, loss=0.012970678973942995\n",
      "Surface training t=8052, loss=0.01485668309032917\n",
      "Surface training t=8053, loss=0.016086763702332973\n",
      "Surface training t=8054, loss=0.015184355899691582\n",
      "Surface training t=8055, loss=0.01253663981333375\n",
      "Surface training t=8056, loss=0.011984299402683973\n",
      "Surface training t=8057, loss=0.013491745106875896\n",
      "Surface training t=8058, loss=0.015098667703568935\n",
      "Surface training t=8059, loss=0.015199401881545782\n",
      "Surface training t=8060, loss=0.014763990417122841\n",
      "Surface training t=8061, loss=0.014485332649201155\n",
      "Surface training t=8062, loss=0.013988028280436993\n",
      "Surface training t=8063, loss=0.014521949924528599\n",
      "Surface training t=8064, loss=0.016723121516406536\n",
      "Surface training t=8065, loss=0.014295632485300303\n",
      "Surface training t=8066, loss=0.014633240643888712\n",
      "Surface training t=8067, loss=0.013310332782566547\n",
      "Surface training t=8068, loss=0.014697331469506025\n",
      "Surface training t=8069, loss=0.014181563630700111\n",
      "Surface training t=8070, loss=0.016457244753837585\n",
      "Surface training t=8071, loss=0.013996425084769726\n",
      "Surface training t=8072, loss=0.013164850417524576\n",
      "Surface training t=8073, loss=0.012998029123991728\n",
      "Surface training t=8074, loss=0.014090391341596842\n",
      "Surface training t=8075, loss=0.015116303227841854\n",
      "Surface training t=8076, loss=0.016777160577476025\n",
      "Surface training t=8077, loss=0.012998983263969421\n",
      "Surface training t=8078, loss=0.013842840678989887\n",
      "Surface training t=8079, loss=0.013741925824433565\n",
      "Surface training t=8080, loss=0.016204181127250195\n",
      "Surface training t=8081, loss=0.014846526551991701\n",
      "Surface training t=8082, loss=0.015123017132282257\n",
      "Surface training t=8083, loss=0.013839120510965586\n",
      "Surface training t=8084, loss=0.014228261541575193\n",
      "Surface training t=8085, loss=0.013816013932228088\n",
      "Surface training t=8086, loss=0.013896818272769451\n",
      "Surface training t=8087, loss=0.014083503279834986\n",
      "Surface training t=8088, loss=0.014026450458914042\n",
      "Surface training t=8089, loss=0.01492115156725049\n",
      "Surface training t=8090, loss=0.014727160334587097\n",
      "Surface training t=8091, loss=0.013196539133787155\n",
      "Surface training t=8092, loss=0.013998291920870543\n",
      "Surface training t=8093, loss=0.01356192259117961\n",
      "Surface training t=8094, loss=0.015184567775577307\n",
      "Surface training t=8095, loss=0.015140058938413858\n",
      "Surface training t=8096, loss=0.016014041379094124\n",
      "Surface training t=8097, loss=0.013903625309467316\n",
      "Surface training t=8098, loss=0.013728479389101267\n",
      "Surface training t=8099, loss=0.013639444019645452\n",
      "Surface training t=8100, loss=0.015907939057797194\n",
      "Surface training t=8101, loss=0.013833573553711176\n",
      "Surface training t=8102, loss=0.013659174088388681\n",
      "Surface training t=8103, loss=0.013578168116509914\n",
      "Surface training t=8104, loss=0.01454198220744729\n",
      "Surface training t=8105, loss=0.01469888724386692\n",
      "Surface training t=8106, loss=0.015679897274821997\n",
      "Surface training t=8107, loss=0.013186793308705091\n",
      "Surface training t=8108, loss=0.01433219900354743\n",
      "Surface training t=8109, loss=0.013247924856841564\n",
      "Surface training t=8110, loss=0.015140966512262821\n",
      "Surface training t=8111, loss=0.014029807411134243\n",
      "Surface training t=8112, loss=0.014991265255957842\n",
      "Surface training t=8113, loss=0.01398138701915741\n",
      "Surface training t=8114, loss=0.014360812492668629\n",
      "Surface training t=8115, loss=0.013694776222109795\n",
      "Surface training t=8116, loss=0.014747981447726488\n",
      "Surface training t=8117, loss=0.014415580779314041\n",
      "Surface training t=8118, loss=0.01452892366796732\n",
      "Surface training t=8119, loss=0.013739926274865866\n",
      "Surface training t=8120, loss=0.01498609408736229\n",
      "Surface training t=8121, loss=0.013753439765423536\n",
      "Surface training t=8122, loss=0.014845119323581457\n",
      "Surface training t=8123, loss=0.013411412946879864\n",
      "Surface training t=8124, loss=0.014121501706540585\n",
      "Surface training t=8125, loss=0.01385036576539278\n",
      "Surface training t=8126, loss=0.014896364882588387\n",
      "Surface training t=8127, loss=0.014401997905224562\n",
      "Surface training t=8128, loss=0.01493166433647275\n",
      "Surface training t=8129, loss=0.014659411739557981\n",
      "Surface training t=8130, loss=0.014853834174573421\n",
      "Surface training t=8131, loss=0.013123178388923407\n",
      "Surface training t=8132, loss=0.01364269107580185\n",
      "Surface training t=8133, loss=0.013498861342668533\n",
      "Surface training t=8134, loss=0.014573178719729185\n",
      "Surface training t=8135, loss=0.014345772098749876\n",
      "Surface training t=8136, loss=0.01520114066079259\n",
      "Surface training t=8137, loss=0.013254048768430948\n",
      "Surface training t=8138, loss=0.014425923116505146\n",
      "Surface training t=8139, loss=0.013956907670944929\n",
      "Surface training t=8140, loss=0.015955781564116478\n",
      "Surface training t=8141, loss=0.013728820253163576\n",
      "Surface training t=8142, loss=0.013828158844262362\n",
      "Surface training t=8143, loss=0.012721916660666466\n",
      "Surface training t=8144, loss=0.01534544862806797\n",
      "Surface training t=8145, loss=0.014523640275001526\n",
      "Surface training t=8146, loss=0.014341961592435837\n",
      "Surface training t=8147, loss=0.012850969098508358\n",
      "Surface training t=8148, loss=0.01293529849499464\n",
      "Surface training t=8149, loss=0.01294516259804368\n",
      "Surface training t=8150, loss=0.01299882447347045\n",
      "Surface training t=8151, loss=0.013385361526161432\n",
      "Surface training t=8152, loss=0.016126261092722416\n",
      "Surface training t=8153, loss=0.01280790800228715\n",
      "Surface training t=8154, loss=0.013039120938628912\n",
      "Surface training t=8155, loss=0.012573979794979095\n",
      "Surface training t=8156, loss=0.013606035150587559\n",
      "Surface training t=8157, loss=0.013937779702246189\n",
      "Surface training t=8158, loss=0.014502785168588161\n",
      "Surface training t=8159, loss=0.013655895367264748\n",
      "Surface training t=8160, loss=0.013776675332337618\n",
      "Surface training t=8161, loss=0.013143835589289665\n",
      "Surface training t=8162, loss=0.014811027329415083\n",
      "Surface training t=8163, loss=0.01374478405341506\n",
      "Surface training t=8164, loss=0.014917175751179457\n",
      "Surface training t=8165, loss=0.013262531720101833\n",
      "Surface training t=8166, loss=0.01538025913760066\n",
      "Surface training t=8167, loss=0.014331172220408916\n",
      "Surface training t=8168, loss=0.014449601992964745\n",
      "Surface training t=8169, loss=0.013514689169824123\n",
      "Surface training t=8170, loss=0.01313754916191101\n",
      "Surface training t=8171, loss=0.013048755005002022\n",
      "Surface training t=8172, loss=0.01453997753560543\n",
      "Surface training t=8173, loss=0.013873700518161058\n",
      "Surface training t=8174, loss=0.016474921256303787\n",
      "Surface training t=8175, loss=0.014067062176764011\n",
      "Surface training t=8176, loss=0.013616474345326424\n",
      "Surface training t=8177, loss=0.012033766135573387\n",
      "Surface training t=8178, loss=0.012872627470642328\n",
      "Surface training t=8179, loss=0.013977352529764175\n",
      "Surface training t=8180, loss=0.016385586000978947\n",
      "Surface training t=8181, loss=0.01233851769939065\n",
      "Surface training t=8182, loss=0.012324138078838587\n",
      "Surface training t=8183, loss=0.012044199742376804\n",
      "Surface training t=8184, loss=0.01357273617759347\n",
      "Surface training t=8185, loss=0.015102567616850138\n",
      "Surface training t=8186, loss=0.015393675304949284\n",
      "Surface training t=8187, loss=0.013302972540259361\n",
      "Surface training t=8188, loss=0.012360125314444304\n",
      "Surface training t=8189, loss=0.012010395526885986\n",
      "Surface training t=8190, loss=0.01323383953422308\n",
      "Surface training t=8191, loss=0.014996541664004326\n",
      "Surface training t=8192, loss=0.01668248325586319\n",
      "Surface training t=8193, loss=0.01283891499042511\n",
      "Surface training t=8194, loss=0.013557835016399622\n",
      "Surface training t=8195, loss=0.012080110143870115\n",
      "Surface training t=8196, loss=0.012920853216201067\n",
      "Surface training t=8197, loss=0.013748952187597752\n",
      "Surface training t=8198, loss=0.015752115286886692\n",
      "Surface training t=8199, loss=0.015501707326620817\n",
      "Surface training t=8200, loss=0.01273120753467083\n",
      "Surface training t=8201, loss=0.011543767526745796\n",
      "Surface training t=8202, loss=0.012662854976952076\n",
      "Surface training t=8203, loss=0.013031074311584234\n",
      "Surface training t=8204, loss=0.015972432680428028\n",
      "Surface training t=8205, loss=0.013767581433057785\n",
      "Surface training t=8206, loss=0.014181829057633877\n",
      "Surface training t=8207, loss=0.013262734282761812\n",
      "Surface training t=8208, loss=0.014321846887469292\n",
      "Surface training t=8209, loss=0.013018968980759382\n",
      "Surface training t=8210, loss=0.014083877671509981\n",
      "Surface training t=8211, loss=0.014718545600771904\n",
      "Surface training t=8212, loss=0.014472258742898703\n",
      "Surface training t=8213, loss=0.013272651936858892\n",
      "Surface training t=8214, loss=0.013110867235809565\n",
      "Surface training t=8215, loss=0.012880886904895306\n",
      "Surface training t=8216, loss=0.014520973898470402\n",
      "Surface training t=8217, loss=0.01397387171164155\n",
      "Surface training t=8218, loss=0.013959595933556557\n",
      "Surface training t=8219, loss=0.012694695498794317\n",
      "Surface training t=8220, loss=0.013721053954213858\n",
      "Surface training t=8221, loss=0.01333595672622323\n",
      "Surface training t=8222, loss=0.013640901073813438\n",
      "Surface training t=8223, loss=0.013359772972762585\n",
      "Surface training t=8224, loss=0.014143723994493484\n",
      "Surface training t=8225, loss=0.012998573016375303\n",
      "Surface training t=8226, loss=0.014121582731604576\n",
      "Surface training t=8227, loss=0.012382290326058865\n",
      "Surface training t=8228, loss=0.013982600532472134\n",
      "Surface training t=8229, loss=0.013035951647907495\n",
      "Surface training t=8230, loss=0.014731096103787422\n",
      "Surface training t=8231, loss=0.014054638333618641\n",
      "Surface training t=8232, loss=0.015463186893612146\n",
      "Surface training t=8233, loss=0.014057329390197992\n",
      "Surface training t=8234, loss=0.014594651758670807\n",
      "Surface training t=8235, loss=0.012927965726703405\n",
      "Surface training t=8236, loss=0.01365212257951498\n",
      "Surface training t=8237, loss=0.012955927290022373\n",
      "Surface training t=8238, loss=0.013479987159371376\n",
      "Surface training t=8239, loss=0.013261185958981514\n",
      "Surface training t=8240, loss=0.014015146065503359\n",
      "Surface training t=8241, loss=0.012940568383783102\n",
      "Surface training t=8242, loss=0.014250713866204023\n",
      "Surface training t=8243, loss=0.013726865872740746\n",
      "Surface training t=8244, loss=0.014289391692727804\n",
      "Surface training t=8245, loss=0.013764404226094484\n",
      "Surface training t=8246, loss=0.01435431744903326\n",
      "Surface training t=8247, loss=0.013219914399087429\n",
      "Surface training t=8248, loss=0.012980574741959572\n",
      "Surface training t=8249, loss=0.01229246286675334\n",
      "Surface training t=8250, loss=0.011566739529371262\n",
      "Surface training t=8251, loss=0.011059687472879887\n",
      "Surface training t=8252, loss=0.010899366345256567\n",
      "Surface training t=8253, loss=0.010562826879322529\n",
      "Surface training t=8254, loss=0.01020263647660613\n",
      "Surface training t=8255, loss=0.010198027361184359\n",
      "Surface training t=8256, loss=0.009804687462747097\n",
      "Surface training t=8257, loss=0.009941630065441132\n",
      "Surface training t=8258, loss=0.009488335344940424\n",
      "Surface training t=8259, loss=0.009441491216421127\n",
      "Surface training t=8260, loss=0.009231348987668753\n",
      "Surface training t=8261, loss=0.009643067140132189\n",
      "Surface training t=8262, loss=0.00923191662877798\n",
      "Surface training t=8263, loss=0.008920803666114807\n",
      "Surface training t=8264, loss=0.009452326223254204\n",
      "Surface training t=8265, loss=0.008916770108044147\n",
      "Surface training t=8266, loss=0.009213381446897984\n",
      "Surface training t=8267, loss=0.009044228587299585\n",
      "Surface training t=8268, loss=0.00909703690558672\n",
      "Surface training t=8269, loss=0.009056471288204193\n",
      "Surface training t=8270, loss=0.008616249077022076\n",
      "Surface training t=8271, loss=0.008821039460599422\n",
      "Surface training t=8272, loss=0.008829767815768719\n",
      "Surface training t=8273, loss=0.00870345439761877\n",
      "Surface training t=8274, loss=0.0092131313867867\n",
      "Surface training t=8275, loss=0.008482661563903093\n",
      "Surface training t=8276, loss=0.008763348683714867\n",
      "Surface training t=8277, loss=0.00910339830443263\n",
      "Surface training t=8278, loss=0.008660553023219109\n",
      "Surface training t=8279, loss=0.009126912336796522\n",
      "Surface training t=8280, loss=0.008864097762852907\n",
      "Surface training t=8281, loss=0.008458174765110016\n",
      "Surface training t=8282, loss=0.008874312043190002\n",
      "Surface training t=8283, loss=0.009272366762161255\n",
      "Surface training t=8284, loss=0.008428132627159357\n",
      "Surface training t=8285, loss=0.008821654133498669\n",
      "Surface training t=8286, loss=0.009047378320246935\n",
      "Surface training t=8287, loss=0.008428131695836782\n",
      "Surface training t=8288, loss=0.009390841238200665\n",
      "Surface training t=8289, loss=0.008922679349780083\n",
      "Surface training t=8290, loss=0.008562891278415918\n",
      "Surface training t=8291, loss=0.009016959927976131\n",
      "Surface training t=8292, loss=0.008945533074438572\n",
      "Surface training t=8293, loss=0.008482424542307854\n",
      "Surface training t=8294, loss=0.009856791235506535\n",
      "Surface training t=8295, loss=0.008900756016373634\n",
      "Surface training t=8296, loss=0.010079640429466963\n",
      "Surface training t=8297, loss=0.008902573492377996\n",
      "Surface training t=8298, loss=0.009259090758860111\n",
      "Surface training t=8299, loss=0.009022731333971024\n",
      "Surface training t=8300, loss=0.008641354274004698\n",
      "Surface training t=8301, loss=0.008415797725319862\n",
      "Surface training t=8302, loss=0.008845095057040453\n",
      "Surface training t=8303, loss=0.009217693470418453\n",
      "Surface training t=8304, loss=0.008850038517266512\n",
      "Surface training t=8305, loss=0.009521367959678173\n",
      "Surface training t=8306, loss=0.008313056081533432\n",
      "Surface training t=8307, loss=0.009452454280108213\n",
      "Surface training t=8308, loss=0.009242819622159004\n",
      "Surface training t=8309, loss=0.00853542285040021\n",
      "Surface training t=8310, loss=0.008243656251579523\n",
      "Surface training t=8311, loss=0.009282259736210108\n",
      "Surface training t=8312, loss=0.008769586682319641\n",
      "Surface training t=8313, loss=0.0083991470746696\n",
      "Surface training t=8314, loss=0.00868268497288227\n",
      "Surface training t=8315, loss=0.009560408536344767\n",
      "Surface training t=8316, loss=0.00831213966012001\n",
      "Surface training t=8317, loss=0.008141202386468649\n",
      "Surface training t=8318, loss=0.009283761493861675\n",
      "Surface training t=8319, loss=0.0092050745151937\n",
      "Surface training t=8320, loss=0.008572021499276161\n",
      "Surface training t=8321, loss=0.009457791689783335\n",
      "Surface training t=8322, loss=0.009194604121148586\n",
      "Surface training t=8323, loss=0.008051634300500154\n",
      "Surface training t=8324, loss=0.009431413374841213\n",
      "Surface training t=8325, loss=0.009447747375816107\n",
      "Surface training t=8326, loss=0.009057545103132725\n",
      "Surface training t=8327, loss=0.010454097762703896\n",
      "Surface training t=8328, loss=0.00962620647624135\n",
      "Surface training t=8329, loss=0.009459518361836672\n",
      "Surface training t=8330, loss=0.009867355227470398\n",
      "Surface training t=8331, loss=0.008946736808866262\n",
      "Surface training t=8332, loss=0.008641102816909552\n",
      "Surface training t=8333, loss=0.010016064159572124\n",
      "Surface training t=8334, loss=0.008589620236307383\n",
      "Surface training t=8335, loss=0.010195339564234018\n",
      "Surface training t=8336, loss=0.008513248525559902\n",
      "Surface training t=8337, loss=0.009294490329921246\n",
      "Surface training t=8338, loss=0.009362902492284775\n",
      "Surface training t=8339, loss=0.008852885104715824\n",
      "Surface training t=8340, loss=0.00877551268786192\n",
      "Surface training t=8341, loss=0.009870974812656641\n",
      "Surface training t=8342, loss=0.008521854411810637\n",
      "Surface training t=8343, loss=0.008783809375017881\n",
      "Surface training t=8344, loss=0.009362112265080214\n",
      "Surface training t=8345, loss=0.00856055784970522\n",
      "Surface training t=8346, loss=0.00913786981254816\n",
      "Surface training t=8347, loss=0.009539423510432243\n",
      "Surface training t=8348, loss=0.009222101420164108\n",
      "Surface training t=8349, loss=0.009346285834908485\n",
      "Surface training t=8350, loss=0.009359047282487154\n",
      "Surface training t=8351, loss=0.008243241347372532\n",
      "Surface training t=8352, loss=0.009053618181496859\n",
      "Surface training t=8353, loss=0.009688011836260557\n",
      "Surface training t=8354, loss=0.008776380680501461\n",
      "Surface training t=8355, loss=0.009846670553088188\n",
      "Surface training t=8356, loss=0.008232176303863525\n",
      "Surface training t=8357, loss=0.009501435328274965\n",
      "Surface training t=8358, loss=0.009362480137497187\n",
      "Surface training t=8359, loss=0.00918744457885623\n",
      "Surface training t=8360, loss=0.00923611968755722\n",
      "Surface training t=8361, loss=0.008152496069669724\n",
      "Surface training t=8362, loss=0.00978422025218606\n",
      "Surface training t=8363, loss=0.009115467546507716\n",
      "Surface training t=8364, loss=0.009417724329978228\n",
      "Surface training t=8365, loss=0.009768705815076828\n",
      "Surface training t=8366, loss=0.008365271147340536\n",
      "Surface training t=8367, loss=0.010235741268843412\n",
      "Surface training t=8368, loss=0.009254815522581339\n",
      "Surface training t=8369, loss=0.010303668212145567\n",
      "Surface training t=8370, loss=0.009559768717736006\n",
      "Surface training t=8371, loss=0.009408187586814165\n",
      "Surface training t=8372, loss=0.008317484520375729\n",
      "Surface training t=8373, loss=0.010067472234368324\n",
      "Surface training t=8374, loss=0.008781448006629944\n",
      "Surface training t=8375, loss=0.009199887048453093\n",
      "Surface training t=8376, loss=0.00946106482297182\n",
      "Surface training t=8377, loss=0.008293436141684651\n",
      "Surface training t=8378, loss=0.009669015649706125\n",
      "Surface training t=8379, loss=0.009563576895743608\n",
      "Surface training t=8380, loss=0.010011952836066484\n",
      "Surface training t=8381, loss=0.008824634831398726\n",
      "Surface training t=8382, loss=0.009002221748232841\n",
      "Surface training t=8383, loss=0.008413314819335938\n",
      "Surface training t=8384, loss=0.01004323223605752\n",
      "Surface training t=8385, loss=0.008727912325412035\n",
      "Surface training t=8386, loss=0.008669870905578136\n",
      "Surface training t=8387, loss=0.009850929956883192\n",
      "Surface training t=8388, loss=0.008977940771728754\n",
      "Surface training t=8389, loss=0.009712779894471169\n",
      "Surface training t=8390, loss=0.008791024796664715\n",
      "Surface training t=8391, loss=0.00939241610467434\n",
      "Surface training t=8392, loss=0.008744371589273214\n",
      "Surface training t=8393, loss=0.009454240091145039\n",
      "Surface training t=8394, loss=0.009291683323681355\n",
      "Surface training t=8395, loss=0.009110289625823498\n",
      "Surface training t=8396, loss=0.009796508122235537\n",
      "Surface training t=8397, loss=0.009272333234548569\n",
      "Surface training t=8398, loss=0.00832377839833498\n",
      "Surface training t=8399, loss=0.008175515569746494\n",
      "Surface training t=8400, loss=0.009703242219984531\n",
      "Surface training t=8401, loss=0.009185513947159052\n",
      "Surface training t=8402, loss=0.009632785804569721\n",
      "Surface training t=8403, loss=0.008756290189921856\n",
      "Surface training t=8404, loss=0.010236709844321012\n",
      "Surface training t=8405, loss=0.00831880234181881\n",
      "Surface training t=8406, loss=0.00942371366545558\n",
      "Surface training t=8407, loss=0.009005435276776552\n",
      "Surface training t=8408, loss=0.008428290020674467\n",
      "Surface training t=8409, loss=0.010134832002222538\n",
      "Surface training t=8410, loss=0.008007993455976248\n",
      "Surface training t=8411, loss=0.010428240057080984\n",
      "Surface training t=8412, loss=0.008372071199119091\n",
      "Surface training t=8413, loss=0.009252785239368677\n",
      "Surface training t=8414, loss=0.009524528868496418\n",
      "Surface training t=8415, loss=0.009435868822038174\n",
      "Surface training t=8416, loss=0.00907919043675065\n",
      "Surface training t=8417, loss=0.009412531740963459\n",
      "Surface training t=8418, loss=0.009436615277081728\n",
      "Surface training t=8419, loss=0.009591924492269754\n",
      "Surface training t=8420, loss=0.007896835915744305\n",
      "Surface training t=8421, loss=0.008432737085968256\n",
      "Surface training t=8422, loss=0.00985665898770094\n",
      "Surface training t=8423, loss=0.008489922853186727\n",
      "Surface training t=8424, loss=0.01022133743390441\n",
      "Surface training t=8425, loss=0.007919503143057227\n",
      "Surface training t=8426, loss=0.009301895275712013\n",
      "Surface training t=8427, loss=0.009490840137004852\n",
      "Surface training t=8428, loss=0.007864195154979825\n",
      "Surface training t=8429, loss=0.009469145443290472\n",
      "Surface training t=8430, loss=0.00888118939474225\n",
      "Surface training t=8431, loss=0.009592467918992043\n",
      "Surface training t=8432, loss=0.00857473537325859\n",
      "Surface training t=8433, loss=0.008525131037458777\n",
      "Surface training t=8434, loss=0.010176114272326231\n",
      "Surface training t=8435, loss=0.008300067391246557\n",
      "Surface training t=8436, loss=0.010259906761348248\n",
      "Surface training t=8437, loss=0.009013187605887651\n",
      "Surface training t=8438, loss=0.009986443910747766\n",
      "Surface training t=8439, loss=0.008402078878134489\n",
      "Surface training t=8440, loss=0.009195690508931875\n",
      "Surface training t=8441, loss=0.009833276737481356\n",
      "Surface training t=8442, loss=0.009294227696955204\n",
      "Surface training t=8443, loss=0.009051098488271236\n",
      "Surface training t=8444, loss=0.009724600240588188\n",
      "Surface training t=8445, loss=0.007929466664791107\n",
      "Surface training t=8446, loss=0.010061155073344707\n",
      "Surface training t=8447, loss=0.008726227097213268\n",
      "Surface training t=8448, loss=0.007794389966875315\n",
      "Surface training t=8449, loss=0.009893940761685371\n",
      "Surface training t=8450, loss=0.00783664477057755\n",
      "Surface training t=8451, loss=0.009211996104568243\n",
      "Surface training t=8452, loss=0.008798910304903984\n",
      "Surface training t=8453, loss=0.009475153870880604\n",
      "Surface training t=8454, loss=0.008907013107091188\n",
      "Surface training t=8455, loss=0.009566465392708778\n",
      "Surface training t=8456, loss=0.00966575276106596\n",
      "Surface training t=8457, loss=0.00870165741071105\n",
      "Surface training t=8458, loss=0.008183043915778399\n",
      "Surface training t=8459, loss=0.010419825557619333\n",
      "Surface training t=8460, loss=0.008113496005535126\n",
      "Surface training t=8461, loss=0.010384096764028072\n",
      "Surface training t=8462, loss=0.011296642012894154\n",
      "Surface training t=8463, loss=0.010579366702586412\n",
      "Surface training t=8464, loss=0.010065366979688406\n",
      "Surface training t=8465, loss=0.01272316137328744\n",
      "Surface training t=8466, loss=0.018768240697681904\n",
      "Surface training t=8467, loss=0.018710085190832615\n",
      "Surface training t=8468, loss=0.016565101221203804\n",
      "Surface training t=8469, loss=0.015525450464338064\n",
      "Surface training t=8470, loss=0.014142623636871576\n",
      "Surface training t=8471, loss=0.014325884636491537\n",
      "Surface training t=8472, loss=0.016405325382947922\n",
      "Surface training t=8473, loss=0.015636011492460966\n",
      "Surface training t=8474, loss=0.013264772016555071\n",
      "Surface training t=8475, loss=0.0161324730142951\n",
      "Surface training t=8476, loss=0.015598421450704336\n",
      "Surface training t=8477, loss=0.018857975490391254\n",
      "Surface training t=8478, loss=0.011735344771295786\n",
      "Surface training t=8479, loss=0.017028722912073135\n",
      "Surface training t=8480, loss=0.013380981516093016\n",
      "Surface training t=8481, loss=0.016869456972926855\n",
      "Surface training t=8482, loss=0.01778174377977848\n",
      "Surface training t=8483, loss=0.013864226639270782\n",
      "Surface training t=8484, loss=0.014468284323811531\n",
      "Surface training t=8485, loss=0.014442099258303642\n",
      "Surface training t=8486, loss=0.017663635313510895\n",
      "Surface training t=8487, loss=0.015305024106055498\n",
      "Surface training t=8488, loss=0.01318026753142476\n",
      "Surface training t=8489, loss=0.0176599882543087\n",
      "Surface training t=8490, loss=0.014295748434960842\n",
      "Surface training t=8491, loss=0.01631881669163704\n",
      "Surface training t=8492, loss=0.0148656009696424\n",
      "Surface training t=8493, loss=0.014690048061311245\n",
      "Surface training t=8494, loss=0.014145221561193466\n",
      "Surface training t=8495, loss=0.016508689150214195\n",
      "Surface training t=8496, loss=0.016084195114672184\n",
      "Surface training t=8497, loss=0.01482116524130106\n",
      "Surface training t=8498, loss=0.013977224007248878\n",
      "Surface training t=8499, loss=0.01684839278459549\n",
      "Surface training t=8500, loss=0.015148906968533993\n",
      "Surface training t=8501, loss=0.014982085209339857\n",
      "Surface training t=8502, loss=0.013968696352094412\n",
      "Surface training t=8503, loss=0.015414053108543158\n",
      "Surface training t=8504, loss=0.01676227245479822\n",
      "Surface training t=8505, loss=0.013500634580850601\n",
      "Surface training t=8506, loss=0.013819372747093439\n",
      "Surface training t=8507, loss=0.016038070432841778\n",
      "Surface training t=8508, loss=0.01588371954858303\n",
      "Surface training t=8509, loss=0.0172443687915802\n",
      "Surface training t=8510, loss=0.011435726191848516\n",
      "Surface training t=8511, loss=0.014411227311939001\n",
      "Surface training t=8512, loss=0.015519499313086271\n",
      "Surface training t=8513, loss=0.018092983402311802\n",
      "Surface training t=8514, loss=0.01510919351130724\n",
      "Surface training t=8515, loss=0.013203865848481655\n",
      "Surface training t=8516, loss=0.013300087302923203\n",
      "Surface training t=8517, loss=0.016564104706048965\n",
      "Surface training t=8518, loss=0.015929066110402346\n",
      "Surface training t=8519, loss=0.014825165271759033\n",
      "Surface training t=8520, loss=0.013092640321701765\n",
      "Surface training t=8521, loss=0.016289798077195883\n",
      "Surface training t=8522, loss=0.013694129884243011\n",
      "Surface training t=8523, loss=0.01739155501127243\n",
      "Surface training t=8524, loss=0.015003083273768425\n",
      "Surface training t=8525, loss=0.012988334987312555\n",
      "Surface training t=8526, loss=0.013295921962708235\n",
      "Surface training t=8527, loss=0.016402876935899258\n",
      "Surface training t=8528, loss=0.014959563035517931\n",
      "Surface training t=8529, loss=0.015630630310624838\n",
      "Surface training t=8530, loss=0.011610675137490034\n",
      "Surface training t=8531, loss=0.015709485858678818\n",
      "Surface training t=8532, loss=0.016701553016901016\n",
      "Surface training t=8533, loss=0.01330885011702776\n",
      "Surface training t=8534, loss=0.015397690702229738\n",
      "Surface training t=8535, loss=0.01566887367516756\n",
      "Surface training t=8536, loss=0.015434198081493378\n",
      "Surface training t=8537, loss=0.012000983115285635\n",
      "Surface training t=8538, loss=0.013223320245742798\n",
      "Surface training t=8539, loss=0.01764496974647045\n",
      "Surface training t=8540, loss=0.014198866672813892\n",
      "Surface training t=8541, loss=0.01553251314908266\n",
      "Surface training t=8542, loss=0.011782566551119089\n",
      "Surface training t=8543, loss=0.016925296746194363\n",
      "Surface training t=8544, loss=0.014418024104088545\n",
      "Surface training t=8545, loss=0.017363947816193104\n",
      "Surface training t=8546, loss=0.011520254891365767\n",
      "Surface training t=8547, loss=0.015411569736897945\n",
      "Surface training t=8548, loss=0.013246613088995218\n",
      "Surface training t=8549, loss=0.01607132190838456\n",
      "Surface training t=8550, loss=0.014150169212371111\n",
      "Surface training t=8551, loss=0.014045139774680138\n",
      "Surface training t=8552, loss=0.014148753602057695\n",
      "Surface training t=8553, loss=0.01592617016285658\n",
      "Surface training t=8554, loss=0.014367821626365185\n",
      "Surface training t=8555, loss=0.015965917147696018\n",
      "Surface training t=8556, loss=0.01195773808285594\n",
      "Surface training t=8557, loss=0.016651117242872715\n",
      "Surface training t=8558, loss=0.012393911369144917\n",
      "Surface training t=8559, loss=0.016767079010605812\n",
      "Surface training t=8560, loss=0.012854539789259434\n",
      "Surface training t=8561, loss=0.01515594543889165\n",
      "Surface training t=8562, loss=0.013519632164388895\n",
      "Surface training t=8563, loss=0.01667715795338154\n",
      "Surface training t=8564, loss=0.013199443463236094\n",
      "Surface training t=8565, loss=0.014211331028491259\n",
      "Surface training t=8566, loss=0.012869182508438826\n",
      "Surface training t=8567, loss=0.017685786820948124\n",
      "Surface training t=8568, loss=0.013263505883514881\n",
      "Surface training t=8569, loss=0.01478063315153122\n",
      "Surface training t=8570, loss=0.012361834291368723\n",
      "Surface training t=8571, loss=0.017162330448627472\n",
      "Surface training t=8572, loss=0.012887762393802404\n",
      "Surface training t=8573, loss=0.01537213521078229\n",
      "Surface training t=8574, loss=0.012705062981694937\n",
      "Surface training t=8575, loss=0.016236452385783195\n",
      "Surface training t=8576, loss=0.013783696107566357\n",
      "Surface training t=8577, loss=0.014186757151037455\n",
      "Surface training t=8578, loss=0.012892159633338451\n",
      "Surface training t=8579, loss=0.015786128118634224\n",
      "Surface training t=8580, loss=0.014043467119336128\n",
      "Surface training t=8581, loss=0.016193771734833717\n",
      "Surface training t=8582, loss=0.011311286594718695\n",
      "Surface training t=8583, loss=0.014783980790525675\n",
      "Surface training t=8584, loss=0.014303470961749554\n",
      "Surface training t=8585, loss=0.016031411476433277\n",
      "Surface training t=8586, loss=0.012629211880266666\n",
      "Surface training t=8587, loss=0.013155144173651934\n",
      "Surface training t=8588, loss=0.013349994085729122\n",
      "Surface training t=8589, loss=0.017411336302757263\n",
      "Surface training t=8590, loss=0.012239903211593628\n",
      "Surface training t=8591, loss=0.015184073243290186\n",
      "Surface training t=8592, loss=0.01248185196891427\n",
      "Surface training t=8593, loss=0.016319820657372475\n",
      "Surface training t=8594, loss=0.012368726078420877\n",
      "Surface training t=8595, loss=0.01639620680361986\n",
      "Surface training t=8596, loss=0.012141335755586624\n",
      "Surface training t=8597, loss=0.016059560235589743\n",
      "Surface training t=8598, loss=0.011915931012481451\n",
      "Surface training t=8599, loss=0.015222783666104078\n",
      "Surface training t=8600, loss=0.01449799258261919\n",
      "Surface training t=8601, loss=0.014339921064674854\n",
      "Surface training t=8602, loss=0.014714792370796204\n",
      "Surface training t=8603, loss=0.012572098057717085\n",
      "Surface training t=8604, loss=0.013043257407844067\n",
      "Surface training t=8605, loss=0.01641221810132265\n",
      "Surface training t=8606, loss=0.013595348224043846\n",
      "Surface training t=8607, loss=0.014115686528384686\n",
      "Surface training t=8608, loss=0.011431835126131773\n",
      "Surface training t=8609, loss=0.016572366002947092\n",
      "Surface training t=8610, loss=0.01393167208880186\n",
      "Surface training t=8611, loss=0.014467539731413126\n",
      "Surface training t=8612, loss=0.013904927764087915\n",
      "Surface training t=8613, loss=0.013303859159350395\n",
      "Surface training t=8614, loss=0.01424111844971776\n",
      "Surface training t=8615, loss=0.014350841287523508\n",
      "Surface training t=8616, loss=0.014491249807178974\n",
      "Surface training t=8617, loss=0.013177950866520405\n",
      "Surface training t=8618, loss=0.012933670077472925\n",
      "Surface training t=8619, loss=0.015547503717243671\n",
      "Surface training t=8620, loss=0.013443364761769772\n",
      "Surface training t=8621, loss=0.015639742370694876\n",
      "Surface training t=8622, loss=0.011454558465629816\n",
      "Surface training t=8623, loss=0.015137739013880491\n",
      "Surface training t=8624, loss=0.013266864698380232\n",
      "Surface training t=8625, loss=0.01566970394924283\n",
      "Surface training t=8626, loss=0.012813872192054987\n",
      "Surface training t=8627, loss=0.013862238731235266\n",
      "Surface training t=8628, loss=0.013421045150607824\n",
      "Surface training t=8629, loss=0.01576313329860568\n",
      "Surface training t=8630, loss=0.013648789841681719\n",
      "Surface training t=8631, loss=0.01360034616664052\n",
      "Surface training t=8632, loss=0.01341561833396554\n",
      "Surface training t=8633, loss=0.015111241489648819\n",
      "Surface training t=8634, loss=0.01311413198709488\n",
      "Surface training t=8635, loss=0.014647928066551685\n",
      "Surface training t=8636, loss=0.01145660039037466\n",
      "Surface training t=8637, loss=0.014788038562983274\n",
      "Surface training t=8638, loss=0.013142838142812252\n",
      "Surface training t=8639, loss=0.014987532515078783\n",
      "Surface training t=8640, loss=0.013563569635152817\n",
      "Surface training t=8641, loss=0.013816647697240114\n",
      "Surface training t=8642, loss=0.012781024444848299\n",
      "Surface training t=8643, loss=0.014605335891246796\n",
      "Surface training t=8644, loss=0.013689459301531315\n",
      "Surface training t=8645, loss=0.013652982655912638\n",
      "Surface training t=8646, loss=0.013531523291021585\n",
      "Surface training t=8647, loss=0.01568687241524458\n",
      "Surface training t=8648, loss=0.011904715094715357\n",
      "Surface training t=8649, loss=0.014190063811838627\n",
      "Surface training t=8650, loss=0.012521823402494192\n",
      "Surface training t=8651, loss=0.015320997219532728\n",
      "Surface training t=8652, loss=0.01293405657634139\n",
      "Surface training t=8653, loss=0.014457778073847294\n",
      "Surface training t=8654, loss=0.012458557728677988\n",
      "Surface training t=8655, loss=0.014699412044137716\n",
      "Surface training t=8656, loss=0.012910290621221066\n",
      "Surface training t=8657, loss=0.015253408346325159\n",
      "Surface training t=8658, loss=0.01282207714393735\n",
      "Surface training t=8659, loss=0.014516191091388464\n",
      "Surface training t=8660, loss=0.012782803736627102\n",
      "Surface training t=8661, loss=0.014076308347284794\n",
      "Surface training t=8662, loss=0.013311092741787434\n",
      "Surface training t=8663, loss=0.015017024707049131\n",
      "Surface training t=8664, loss=0.011763171292841434\n",
      "Surface training t=8665, loss=0.014599986840039492\n",
      "Surface training t=8666, loss=0.012928267940878868\n",
      "Surface training t=8667, loss=0.01516756508499384\n",
      "Surface training t=8668, loss=0.011205949354916811\n",
      "Surface training t=8669, loss=0.01445162296295166\n",
      "Surface training t=8670, loss=0.012723573949187994\n",
      "Surface training t=8671, loss=0.01574624376371503\n",
      "Surface training t=8672, loss=0.012429492548108101\n",
      "Surface training t=8673, loss=0.014349272474646568\n",
      "Surface training t=8674, loss=0.013331030495464802\n",
      "Surface training t=8675, loss=0.01369637856259942\n",
      "Surface training t=8676, loss=0.013377523981034756\n",
      "Surface training t=8677, loss=0.014867526013404131\n",
      "Surface training t=8678, loss=0.01281311921775341\n",
      "Surface training t=8679, loss=0.014846259262412786\n",
      "Surface training t=8680, loss=0.012311313766986132\n",
      "Surface training t=8681, loss=0.014287393540143967\n",
      "Surface training t=8682, loss=0.01270976522937417\n",
      "Surface training t=8683, loss=0.013416089117527008\n",
      "Surface training t=8684, loss=0.013173332903534174\n",
      "Surface training t=8685, loss=0.015211386140435934\n",
      "Surface training t=8686, loss=0.01213643979281187\n",
      "Surface training t=8687, loss=0.013381130062043667\n",
      "Surface training t=8688, loss=0.011420492548495531\n",
      "Surface training t=8689, loss=0.015020300168544054\n",
      "Surface training t=8690, loss=0.01215133536607027\n",
      "Surface training t=8691, loss=0.01527914684265852\n",
      "Surface training t=8692, loss=0.011307959444820881\n",
      "Surface training t=8693, loss=0.014245040714740753\n",
      "Surface training t=8694, loss=0.011464379262179136\n",
      "Surface training t=8695, loss=0.015074000228196383\n",
      "Surface training t=8696, loss=0.013540478888899088\n",
      "Surface training t=8697, loss=0.014394118916243315\n",
      "Surface training t=8698, loss=0.01208500238135457\n",
      "Surface training t=8699, loss=0.012556179892271757\n",
      "Surface training t=8700, loss=0.015102578327059746\n",
      "Surface training t=8701, loss=0.0153533685952425\n",
      "Surface training t=8702, loss=0.012596262153238058\n",
      "Surface training t=8703, loss=0.012975635938346386\n",
      "Surface training t=8704, loss=0.012189753353595734\n",
      "Surface training t=8705, loss=0.015104533173143864\n",
      "Surface training t=8706, loss=0.012372971046715975\n",
      "Surface training t=8707, loss=0.014124843757599592\n",
      "Surface training t=8708, loss=0.009996110573410988\n",
      "Surface training t=8709, loss=0.011438641231507063\n",
      "Surface training t=8710, loss=0.01165316253900528\n",
      "Surface training t=8711, loss=0.015583915635943413\n",
      "Surface training t=8712, loss=0.01365254633128643\n",
      "Surface training t=8713, loss=0.011918125208467245\n",
      "Surface training t=8714, loss=0.011238089296966791\n",
      "Surface training t=8715, loss=0.009943183045834303\n",
      "Surface training t=8716, loss=0.011952236760407686\n",
      "Surface training t=8717, loss=0.014349394477903843\n",
      "Surface training t=8718, loss=0.015659918542951345\n",
      "Surface training t=8719, loss=0.012685881927609444\n",
      "Surface training t=8720, loss=0.011425147764384747\n",
      "Surface training t=8721, loss=0.01199393905699253\n",
      "Surface training t=8722, loss=0.01242268830537796\n",
      "Surface training t=8723, loss=0.015039307996630669\n",
      "Surface training t=8724, loss=0.011668459046632051\n",
      "Surface training t=8725, loss=0.01145584275946021\n",
      "Surface training t=8726, loss=0.012845400255173445\n",
      "Surface training t=8727, loss=0.01369538251310587\n",
      "Surface training t=8728, loss=0.014224682468920946\n",
      "Surface training t=8729, loss=0.012891639955341816\n",
      "Surface training t=8730, loss=0.012070666998624802\n",
      "Surface training t=8731, loss=0.011109250131994486\n",
      "Surface training t=8732, loss=0.013122365344315767\n",
      "Surface training t=8733, loss=0.012669709976762533\n",
      "Surface training t=8734, loss=0.014561711344867945\n",
      "Surface training t=8735, loss=0.013775634113699198\n",
      "Surface training t=8736, loss=0.014421486295759678\n",
      "Surface training t=8737, loss=0.01403905637562275\n",
      "Surface training t=8738, loss=0.01411960506811738\n",
      "Surface training t=8739, loss=0.01410949369892478\n",
      "Surface training t=8740, loss=0.01239950954914093\n",
      "Surface training t=8741, loss=0.014725978020578623\n",
      "Surface training t=8742, loss=0.01326749473810196\n",
      "Surface training t=8743, loss=0.0156893664970994\n",
      "Surface training t=8744, loss=0.011518279556185007\n",
      "Surface training t=8745, loss=0.011844621039927006\n",
      "Surface training t=8746, loss=0.013574196491390467\n",
      "Surface training t=8747, loss=0.015286197420209646\n",
      "Surface training t=8748, loss=0.01493335422128439\n",
      "Surface training t=8749, loss=0.01034115580841899\n",
      "Surface training t=8750, loss=0.011975912842899561\n",
      "Surface training t=8751, loss=0.013730293605476618\n",
      "Surface training t=8752, loss=0.01581117231398821\n",
      "Surface training t=8753, loss=0.012365327216684818\n",
      "Surface training t=8754, loss=0.012140252627432346\n",
      "Surface training t=8755, loss=0.012334547936916351\n",
      "Surface training t=8756, loss=0.015808233991265297\n",
      "Surface training t=8757, loss=0.014297117479145527\n",
      "Surface training t=8758, loss=0.012487676925957203\n",
      "Surface training t=8759, loss=0.013055748771876097\n",
      "Surface training t=8760, loss=0.012701311148703098\n",
      "Surface training t=8761, loss=0.01321840239688754\n",
      "Surface training t=8762, loss=0.01355489157140255\n",
      "Surface training t=8763, loss=0.013853243552148342\n",
      "Surface training t=8764, loss=0.013226345181465149\n",
      "Surface training t=8765, loss=0.013073074165731668\n",
      "Surface training t=8766, loss=0.013349518645554781\n",
      "Surface training t=8767, loss=0.014382602646946907\n",
      "Surface training t=8768, loss=0.012860113754868507\n",
      "Surface training t=8769, loss=0.012435723096132278\n",
      "Surface training t=8770, loss=0.013498580083251\n",
      "Surface training t=8771, loss=0.01401813980191946\n",
      "Surface training t=8772, loss=0.013536154292523861\n",
      "Surface training t=8773, loss=0.013111828360706568\n",
      "Surface training t=8774, loss=0.012217006646096706\n",
      "Surface training t=8775, loss=0.014033414423465729\n",
      "Surface training t=8776, loss=0.012658350169658661\n",
      "Surface training t=8777, loss=0.014294369146227837\n",
      "Surface training t=8778, loss=0.01180169079452753\n",
      "Surface training t=8779, loss=0.014059638604521751\n",
      "Surface training t=8780, loss=0.013864850159734488\n",
      "Surface training t=8781, loss=0.013057177886366844\n",
      "Surface training t=8782, loss=0.013354409951716661\n",
      "Surface training t=8783, loss=0.012765804305672646\n",
      "Surface training t=8784, loss=0.013367237988859415\n",
      "Surface training t=8785, loss=0.01225488306954503\n",
      "Surface training t=8786, loss=0.014211301226168871\n",
      "Surface training t=8787, loss=0.012878380250185728\n",
      "Surface training t=8788, loss=0.013088544365018606\n",
      "Surface training t=8789, loss=0.013312228955328465\n",
      "Surface training t=8790, loss=0.0132128712721169\n",
      "Surface training t=8791, loss=0.013457948341965675\n",
      "Surface training t=8792, loss=0.013054308481514454\n",
      "Surface training t=8793, loss=0.01369827426970005\n",
      "Surface training t=8794, loss=0.012358650099486113\n",
      "Surface training t=8795, loss=0.013523646630346775\n",
      "Surface training t=8796, loss=0.012609587050974369\n",
      "Surface training t=8797, loss=0.014630934223532677\n",
      "Surface training t=8798, loss=0.012000378221273422\n",
      "Surface training t=8799, loss=0.013993505854159594\n",
      "Surface training t=8800, loss=0.011863208841532469\n",
      "Surface training t=8801, loss=0.01336947875097394\n",
      "Surface training t=8802, loss=0.01465048873797059\n",
      "Surface training t=8803, loss=0.012643887661397457\n",
      "Surface training t=8804, loss=0.013761097099632025\n",
      "Surface training t=8805, loss=0.011303219944238663\n",
      "Surface training t=8806, loss=0.014714451972395182\n",
      "Surface training t=8807, loss=0.012086286675184965\n",
      "Surface training t=8808, loss=0.013385569676756859\n",
      "Surface training t=8809, loss=0.012675042264163494\n",
      "Surface training t=8810, loss=0.013854168355464935\n",
      "Surface training t=8811, loss=0.013090142980217934\n",
      "Surface training t=8812, loss=0.012414855882525444\n",
      "Surface training t=8813, loss=0.013290864881128073\n",
      "Surface training t=8814, loss=0.012260650750249624\n",
      "Surface training t=8815, loss=0.014027653262019157\n",
      "Surface training t=8816, loss=0.012227130588144064\n",
      "Surface training t=8817, loss=0.01443668594583869\n",
      "Surface training t=8818, loss=0.012239922303706408\n",
      "Surface training t=8819, loss=0.013258884195238352\n",
      "Surface training t=8820, loss=0.012897839304059744\n",
      "Surface training t=8821, loss=0.013838099781423807\n",
      "Surface training t=8822, loss=0.012718460522592068\n",
      "Surface training t=8823, loss=0.012849211692810059\n",
      "Surface training t=8824, loss=0.01317941676825285\n",
      "Surface training t=8825, loss=0.013592016883194447\n",
      "Surface training t=8826, loss=0.012420465238392353\n",
      "Surface training t=8827, loss=0.012662512715905905\n",
      "Surface training t=8828, loss=0.012639401480555534\n",
      "Surface training t=8829, loss=0.01418972248211503\n",
      "Surface training t=8830, loss=0.012741779908537865\n",
      "Surface training t=8831, loss=0.012239165604114532\n",
      "Surface training t=8832, loss=0.011783221736550331\n",
      "Surface training t=8833, loss=0.014561849180608988\n",
      "Surface training t=8834, loss=0.012801718432456255\n",
      "Surface training t=8835, loss=0.012900007888674736\n",
      "Surface training t=8836, loss=0.011671054642647505\n",
      "Surface training t=8837, loss=0.013531836215406656\n",
      "Surface training t=8838, loss=0.013298147823661566\n",
      "Surface training t=8839, loss=0.013284886255860329\n",
      "Surface training t=8840, loss=0.011033238377422094\n",
      "Surface training t=8841, loss=0.013050815090537071\n",
      "Surface training t=8842, loss=0.013458447996526957\n",
      "Surface training t=8843, loss=0.012868395075201988\n",
      "Surface training t=8844, loss=0.013495538383722305\n",
      "Surface training t=8845, loss=0.012404171749949455\n",
      "Surface training t=8846, loss=0.014127631671726704\n",
      "Surface training t=8847, loss=0.01335659995675087\n",
      "Surface training t=8848, loss=0.012545784004032612\n",
      "Surface training t=8849, loss=0.012261868920177221\n",
      "Surface training t=8850, loss=0.012553081382066011\n",
      "Surface training t=8851, loss=0.01412666728720069\n",
      "Surface training t=8852, loss=0.012056566774845123\n",
      "Surface training t=8853, loss=0.013578843791037798\n",
      "Surface training t=8854, loss=0.011523745022714138\n",
      "Surface training t=8855, loss=0.014161224942654371\n",
      "Surface training t=8856, loss=0.01210983470082283\n",
      "Surface training t=8857, loss=0.013409349601715803\n",
      "Surface training t=8858, loss=0.01437901146709919\n",
      "Surface training t=8859, loss=0.01192700257524848\n",
      "Surface training t=8860, loss=0.012375832069665194\n",
      "Surface training t=8861, loss=0.012206824030727148\n",
      "Surface training t=8862, loss=0.013680530246347189\n",
      "Surface training t=8863, loss=0.012993499636650085\n",
      "Surface training t=8864, loss=0.012010044418275356\n",
      "Surface training t=8865, loss=0.013323185965418816\n",
      "Surface training t=8866, loss=0.012909764889627695\n",
      "Surface training t=8867, loss=0.013442729134112597\n",
      "Surface training t=8868, loss=0.012097489088773727\n",
      "Surface training t=8869, loss=0.012535650748759508\n",
      "Surface training t=8870, loss=0.013863604050129652\n",
      "Surface training t=8871, loss=0.0134517434053123\n",
      "Surface training t=8872, loss=0.013590270653367043\n",
      "Surface training t=8873, loss=0.010826088022440672\n",
      "Surface training t=8874, loss=0.011926444713026285\n",
      "Surface training t=8875, loss=0.014567643404006958\n",
      "Surface training t=8876, loss=0.013964212033897638\n",
      "Surface training t=8877, loss=0.012486605439335108\n",
      "Surface training t=8878, loss=0.011079506017267704\n",
      "Surface training t=8879, loss=0.013347767759114504\n",
      "Surface training t=8880, loss=0.013802662957459688\n",
      "Surface training t=8881, loss=0.013269033282995224\n",
      "Surface training t=8882, loss=0.013271212112158537\n",
      "Surface training t=8883, loss=0.011226363480091095\n",
      "Surface training t=8884, loss=0.013429549522697926\n",
      "Surface training t=8885, loss=0.013280726969242096\n",
      "Surface training t=8886, loss=0.013453581370413303\n",
      "Surface training t=8887, loss=0.012207346502691507\n",
      "Surface training t=8888, loss=0.012734462972730398\n",
      "Surface training t=8889, loss=0.012798464857041836\n",
      "Surface training t=8890, loss=0.01223317114636302\n",
      "Surface training t=8891, loss=0.013159389141947031\n",
      "Surface training t=8892, loss=0.012040109373629093\n",
      "Surface training t=8893, loss=0.013538583181798458\n",
      "Surface training t=8894, loss=0.01237880950793624\n",
      "Surface training t=8895, loss=0.012637716252356768\n",
      "Surface training t=8896, loss=0.011802580673247576\n",
      "Surface training t=8897, loss=0.013433292508125305\n",
      "Surface training t=8898, loss=0.013282619882375002\n",
      "Surface training t=8899, loss=0.012937199790030718\n",
      "Surface training t=8900, loss=0.012827679049223661\n",
      "Surface training t=8901, loss=0.012258314527571201\n",
      "Surface training t=8902, loss=0.012384500354528427\n",
      "Surface training t=8903, loss=0.013112093321979046\n",
      "Surface training t=8904, loss=0.01182723930105567\n",
      "Surface training t=8905, loss=0.013474113307893276\n",
      "Surface training t=8906, loss=0.011180879548192024\n",
      "Surface training t=8907, loss=0.012756424956023693\n",
      "Surface training t=8908, loss=0.012322722002863884\n",
      "Surface training t=8909, loss=0.013932277448475361\n",
      "Surface training t=8910, loss=0.012493315152823925\n",
      "Surface training t=8911, loss=0.01234905794262886\n",
      "Surface training t=8912, loss=0.011185183189809322\n",
      "Surface training t=8913, loss=0.012827452272176743\n",
      "Surface training t=8914, loss=0.01239753793925047\n",
      "Surface training t=8915, loss=0.013900279998779297\n",
      "Surface training t=8916, loss=0.011628606356680393\n",
      "Surface training t=8917, loss=0.013385567348450422\n",
      "Surface training t=8918, loss=0.012128124013543129\n",
      "Surface training t=8919, loss=0.01362088555470109\n",
      "Surface training t=8920, loss=0.011244489345699549\n",
      "Surface training t=8921, loss=0.012269904837012291\n",
      "Surface training t=8922, loss=0.011024896521121264\n",
      "Surface training t=8923, loss=0.014353432226926088\n",
      "Surface training t=8924, loss=0.01303651463240385\n",
      "Surface training t=8925, loss=0.0128693338483572\n",
      "Surface training t=8926, loss=0.011910622473806143\n",
      "Surface training t=8927, loss=0.011898567900061607\n",
      "Surface training t=8928, loss=0.014932713471353054\n",
      "Surface training t=8929, loss=0.011874336749315262\n",
      "Surface training t=8930, loss=0.013476241379976273\n",
      "Surface training t=8931, loss=0.011663175653666258\n",
      "Surface training t=8932, loss=0.011650283820927143\n",
      "Surface training t=8933, loss=0.013002792373299599\n",
      "Surface training t=8934, loss=0.01259561674669385\n",
      "Surface training t=8935, loss=0.013303162530064583\n",
      "Surface training t=8936, loss=0.0112674655392766\n",
      "Surface training t=8937, loss=0.012782549019902945\n",
      "Surface training t=8938, loss=0.01069092471152544\n",
      "Surface training t=8939, loss=0.01314564747735858\n",
      "Surface training t=8940, loss=0.013103900477290154\n",
      "Surface training t=8941, loss=0.013081017415970564\n",
      "Surface training t=8942, loss=0.012437212280929089\n",
      "Surface training t=8943, loss=0.012065610848367214\n",
      "Surface training t=8944, loss=0.011796372011303902\n",
      "Surface training t=8945, loss=0.013384635094553232\n",
      "Surface training t=8946, loss=0.012959740590304136\n",
      "Surface training t=8947, loss=0.012733896728605032\n",
      "Surface training t=8948, loss=0.011451838072389364\n",
      "Surface training t=8949, loss=0.0129506248049438\n",
      "Surface training t=8950, loss=0.01184356864541769\n",
      "Surface training t=8951, loss=0.013135957531630993\n",
      "Surface training t=8952, loss=0.012204277329146862\n",
      "Surface training t=8953, loss=0.012809865642338991\n",
      "Surface training t=8954, loss=0.012411302421241999\n",
      "Surface training t=8955, loss=0.013424188829958439\n",
      "Surface training t=8956, loss=0.011256552767008543\n",
      "Surface training t=8957, loss=0.012209970504045486\n",
      "Surface training t=8958, loss=0.011367855127900839\n",
      "Surface training t=8959, loss=0.014124874956905842\n",
      "Surface training t=8960, loss=0.011916776653379202\n",
      "Surface training t=8961, loss=0.013589791487902403\n",
      "Surface training t=8962, loss=0.011224835645407438\n",
      "Surface training t=8963, loss=0.01188467163592577\n",
      "Surface training t=8964, loss=0.012935009319335222\n",
      "Surface training t=8965, loss=0.01350933127105236\n",
      "Surface training t=8966, loss=0.013387414161115885\n",
      "Surface training t=8967, loss=0.01249903067946434\n",
      "Surface training t=8968, loss=0.012478784658014774\n",
      "Surface training t=8969, loss=0.012718903832137585\n",
      "Surface training t=8970, loss=0.01255320617929101\n",
      "Surface training t=8971, loss=0.012142103165388107\n",
      "Surface training t=8972, loss=0.012019217945635319\n",
      "Surface training t=8973, loss=0.01366417109966278\n",
      "Surface training t=8974, loss=0.011337873991578817\n",
      "Surface training t=8975, loss=0.012621067464351654\n",
      "Surface training t=8976, loss=0.01186133548617363\n",
      "Surface training t=8977, loss=0.013610759750008583\n",
      "Surface training t=8978, loss=0.012700000777840614\n",
      "Surface training t=8979, loss=0.013171771075576544\n",
      "Surface training t=8980, loss=0.011594579089432955\n",
      "Surface training t=8981, loss=0.012253462336957455\n",
      "Surface training t=8982, loss=0.011926708277314901\n",
      "Surface training t=8983, loss=0.013368817511945963\n",
      "Surface training t=8984, loss=0.011291332077234983\n",
      "Surface training t=8985, loss=0.012792433146387339\n",
      "Surface training t=8986, loss=0.011247205547988415\n",
      "Surface training t=8987, loss=0.01358101051300764\n",
      "Surface training t=8988, loss=0.012118258513510227\n",
      "Surface training t=8989, loss=0.012507520150393248\n",
      "Surface training t=8990, loss=0.012397340033203363\n",
      "Surface training t=8991, loss=0.012441498227417469\n",
      "Surface training t=8992, loss=0.013745707459747791\n",
      "Surface training t=8993, loss=0.01215912913903594\n",
      "Surface training t=8994, loss=0.012104277964681387\n",
      "Surface training t=8995, loss=0.01280278991907835\n",
      "Surface training t=8996, loss=0.011590181849896908\n",
      "Surface training t=8997, loss=0.013627996202558279\n",
      "Surface training t=8998, loss=0.012832794804126024\n",
      "Surface training t=8999, loss=0.012697113212198019\n",
      "Surface training t=9000, loss=0.013331547379493713\n",
      "Surface training t=9001, loss=0.012026120442897081\n",
      "Surface training t=9002, loss=0.011778478976339102\n",
      "Surface training t=9003, loss=0.012403437402099371\n",
      "Surface training t=9004, loss=0.012143774423748255\n",
      "Surface training t=9005, loss=0.012467783410102129\n",
      "Surface training t=9006, loss=0.010924696456640959\n",
      "Surface training t=9007, loss=0.01207169471308589\n",
      "Surface training t=9008, loss=0.010730222798883915\n",
      "Surface training t=9009, loss=0.011619499884545803\n",
      "Surface training t=9010, loss=0.012192613445222378\n",
      "Surface training t=9011, loss=0.013587377965450287\n",
      "Surface training t=9012, loss=0.012374925892800093\n",
      "Surface training t=9013, loss=0.011773025151342154\n",
      "Surface training t=9014, loss=0.010739267338067293\n",
      "Surface training t=9015, loss=0.012582932133227587\n",
      "Surface training t=9016, loss=0.013674266636371613\n",
      "Surface training t=9017, loss=0.01484614098444581\n",
      "Surface training t=9018, loss=0.010985936038196087\n",
      "Surface training t=9019, loss=0.010606480296701193\n",
      "Surface training t=9020, loss=0.011497820727527142\n",
      "Surface training t=9021, loss=0.013225123286247253\n",
      "Surface training t=9022, loss=0.014293049927800894\n",
      "Surface training t=9023, loss=0.01135868951678276\n",
      "Surface training t=9024, loss=0.011555102653801441\n",
      "Surface training t=9025, loss=0.011196089442819357\n",
      "Surface training t=9026, loss=0.013261738233268261\n",
      "Surface training t=9027, loss=0.013199624605476856\n",
      "Surface training t=9028, loss=0.011749189347028732\n",
      "Surface training t=9029, loss=0.012837780173867941\n",
      "Surface training t=9030, loss=0.011481006164103746\n",
      "Surface training t=9031, loss=0.012446021661162376\n",
      "Surface training t=9032, loss=0.01074826717376709\n",
      "Surface training t=9033, loss=0.013205494731664658\n",
      "Surface training t=9034, loss=0.013379084412008524\n",
      "Surface training t=9035, loss=0.012270824983716011\n",
      "Surface training t=9036, loss=0.01324785454198718\n",
      "Surface training t=9037, loss=0.01052840705960989\n",
      "Surface training t=9038, loss=0.011147720739245415\n",
      "Surface training t=9039, loss=0.013028858229517937\n",
      "Surface training t=9040, loss=0.013798877596855164\n",
      "Surface training t=9041, loss=0.013273939024657011\n",
      "Surface training t=9042, loss=0.010447002947330475\n",
      "Surface training t=9043, loss=0.011968238279223442\n",
      "Surface training t=9044, loss=0.012174071744084358\n",
      "Surface training t=9045, loss=0.013859303668141365\n",
      "Surface training t=9046, loss=0.012312466744333506\n",
      "Surface training t=9047, loss=0.011654397007077932\n",
      "Surface training t=9048, loss=0.012190711684525013\n",
      "Surface training t=9049, loss=0.013321886770427227\n",
      "Surface training t=9050, loss=0.013036258053034544\n",
      "Surface training t=9051, loss=0.012465355452150106\n",
      "Surface training t=9052, loss=0.011619660072028637\n",
      "Surface training t=9053, loss=0.01241076597943902\n",
      "Surface training t=9054, loss=0.013100056443363428\n",
      "Surface training t=9055, loss=0.013017802964895964\n",
      "Surface training t=9056, loss=0.011268991511315107\n",
      "Surface training t=9057, loss=0.011187233030796051\n",
      "Surface training t=9058, loss=0.01294188853353262\n",
      "Surface training t=9059, loss=0.014250225853174925\n",
      "Surface training t=9060, loss=0.012608480639755726\n",
      "Surface training t=9061, loss=0.011555051896721125\n",
      "Surface training t=9062, loss=0.011423146352171898\n",
      "Surface training t=9063, loss=0.013207285664975643\n",
      "Surface training t=9064, loss=0.012309200130403042\n",
      "Surface training t=9065, loss=0.013381874654442072\n",
      "Surface training t=9066, loss=0.011355168651789427\n",
      "Surface training t=9067, loss=0.011455742176622152\n",
      "Surface training t=9068, loss=0.010751496069133282\n",
      "Surface training t=9069, loss=0.012959662824869156\n",
      "Surface training t=9070, loss=0.0132742072455585\n",
      "Surface training t=9071, loss=0.011201397515833378\n",
      "Surface training t=9072, loss=0.011781023815274239\n",
      "Surface training t=9073, loss=0.011985987424850464\n",
      "Surface training t=9074, loss=0.01314191846176982\n",
      "Surface training t=9075, loss=0.013103495351970196\n",
      "Surface training t=9076, loss=0.011205180082470179\n",
      "Surface training t=9077, loss=0.011207259260118008\n",
      "Surface training t=9078, loss=0.011736771557480097\n",
      "Surface training t=9079, loss=0.012677485588937998\n",
      "Surface training t=9080, loss=0.013776545412838459\n",
      "Surface training t=9081, loss=0.011167687363922596\n",
      "Surface training t=9082, loss=0.011381963267922401\n",
      "Surface training t=9083, loss=0.011951973661780357\n",
      "Surface training t=9084, loss=0.012851324398070574\n",
      "Surface training t=9085, loss=0.012686601839959621\n",
      "Surface training t=9086, loss=0.010783747304230928\n",
      "Surface training t=9087, loss=0.01165554765611887\n",
      "Surface training t=9088, loss=0.010912901721894741\n",
      "Surface training t=9089, loss=0.012947708833962679\n",
      "Surface training t=9090, loss=0.012063303496688604\n",
      "Surface training t=9091, loss=0.012224632315337658\n",
      "Surface training t=9092, loss=0.010776614304631948\n",
      "Surface training t=9093, loss=0.011210544034838676\n",
      "Surface training t=9094, loss=0.012285575270652771\n",
      "Surface training t=9095, loss=0.012251876760274172\n",
      "Surface training t=9096, loss=0.011769953183829784\n",
      "Surface training t=9097, loss=0.010881087742745876\n",
      "Surface training t=9098, loss=0.011372239794582129\n",
      "Surface training t=9099, loss=0.010971440933644772\n",
      "Surface training t=9100, loss=0.012064559385180473\n",
      "Surface training t=9101, loss=0.012672929093241692\n",
      "Surface training t=9102, loss=0.012065129354596138\n",
      "Surface training t=9103, loss=0.01100602000951767\n",
      "Surface training t=9104, loss=0.011308658868074417\n",
      "Surface training t=9105, loss=0.010563360061496496\n",
      "Surface training t=9106, loss=0.012185384053736925\n",
      "Surface training t=9107, loss=0.012290074490010738\n",
      "Surface training t=9108, loss=0.012448111549019814\n",
      "Surface training t=9109, loss=0.010486125946044922\n",
      "Surface training t=9110, loss=0.010669652838259935\n",
      "Surface training t=9111, loss=0.0103385541588068\n",
      "Surface training t=9112, loss=0.010989377740770578\n",
      "Surface training t=9113, loss=0.011621595360338688\n",
      "Surface training t=9114, loss=0.012413105927407742\n",
      "Surface training t=9115, loss=0.011919810902327299\n",
      "Surface training t=9116, loss=0.011464402545243502\n",
      "Surface training t=9117, loss=0.010556768160313368\n",
      "Surface training t=9118, loss=0.01127774128690362\n",
      "Surface training t=9119, loss=0.011099610477685928\n",
      "Surface training t=9120, loss=0.012210489716380835\n",
      "Surface training t=9121, loss=0.01193728344514966\n",
      "Surface training t=9122, loss=0.012259600218385458\n",
      "Surface training t=9123, loss=0.01080601243302226\n",
      "Surface training t=9124, loss=0.011738422326743603\n",
      "Surface training t=9125, loss=0.011520199943333864\n",
      "Surface training t=9126, loss=0.0118266805075109\n",
      "Surface training t=9127, loss=0.010350258089601994\n",
      "Surface training t=9128, loss=0.011071459390223026\n",
      "Surface training t=9129, loss=0.00968416640534997\n",
      "Surface training t=9130, loss=0.009017659816890955\n",
      "Surface training t=9131, loss=0.009088678285479546\n",
      "Surface training t=9132, loss=0.009062978671863675\n",
      "Surface training t=9133, loss=0.008635603589937091\n",
      "Surface training t=9134, loss=0.008559830253943801\n",
      "Surface training t=9135, loss=0.008364065317437053\n",
      "Surface training t=9136, loss=0.00819510081782937\n",
      "Surface training t=9137, loss=0.00819594832137227\n",
      "Surface training t=9138, loss=0.007722895592451096\n",
      "Surface training t=9139, loss=0.00768876145593822\n",
      "Surface training t=9140, loss=0.007636314956471324\n",
      "Surface training t=9141, loss=0.007554702460765839\n",
      "Surface training t=9142, loss=0.007433245424181223\n",
      "Surface training t=9143, loss=0.00755032105371356\n",
      "Surface training t=9144, loss=0.007519647711887956\n",
      "Surface training t=9145, loss=0.007743200287222862\n",
      "Surface training t=9146, loss=0.007787550799548626\n",
      "Surface training t=9147, loss=0.007775892270728946\n",
      "Surface training t=9148, loss=0.007008631946519017\n",
      "Surface training t=9149, loss=0.007265563355758786\n",
      "Surface training t=9150, loss=0.006846402073279023\n",
      "Surface training t=9151, loss=0.007263821084052324\n",
      "Surface training t=9152, loss=0.007446337956935167\n",
      "Surface training t=9153, loss=0.007123786024749279\n",
      "Surface training t=9154, loss=0.007010747212916613\n",
      "Surface training t=9155, loss=0.006897236220538616\n",
      "Surface training t=9156, loss=0.006979831261560321\n",
      "Surface training t=9157, loss=0.007765311514958739\n",
      "Surface training t=9158, loss=0.007711646147072315\n",
      "Surface training t=9159, loss=0.007180477725341916\n",
      "Surface training t=9160, loss=0.0077708743046969175\n",
      "Surface training t=9161, loss=0.008216463262215257\n",
      "Surface training t=9162, loss=0.007487491704523563\n",
      "Surface training t=9163, loss=0.008318013977259398\n",
      "Surface training t=9164, loss=0.009469639975577593\n",
      "Surface training t=9165, loss=0.008385537890717387\n",
      "Surface training t=9166, loss=0.007744802162051201\n",
      "Surface training t=9167, loss=0.008710696827620268\n",
      "Surface training t=9168, loss=0.008268540259450674\n",
      "Surface training t=9169, loss=0.007204812252894044\n",
      "Surface training t=9170, loss=0.008164230268448591\n",
      "Surface training t=9171, loss=0.007006666157394648\n",
      "Surface training t=9172, loss=0.00766524625942111\n",
      "Surface training t=9173, loss=0.007272204617038369\n",
      "Surface training t=9174, loss=0.00685218651778996\n",
      "Surface training t=9175, loss=0.006962428335100412\n",
      "Surface training t=9176, loss=0.007116790395230055\n",
      "Surface training t=9177, loss=0.008034845348447561\n",
      "Surface training t=9178, loss=0.006943742744624615\n",
      "Surface training t=9179, loss=0.007033963222056627\n",
      "Surface training t=9180, loss=0.008107483154162765\n",
      "Surface training t=9181, loss=0.00732794962823391\n",
      "Surface training t=9182, loss=0.008502495475113392\n",
      "Surface training t=9183, loss=0.008415980264544487\n",
      "Surface training t=9184, loss=0.006996504031121731\n",
      "Surface training t=9185, loss=0.0076199674513190985\n",
      "Surface training t=9186, loss=0.00765188317745924\n",
      "Surface training t=9187, loss=0.006857467815279961\n",
      "Surface training t=9188, loss=0.006873018806800246\n",
      "Surface training t=9189, loss=0.007003248203545809\n",
      "Surface training t=9190, loss=0.008185513317584991\n",
      "Surface training t=9191, loss=0.007848074892535806\n",
      "Surface training t=9192, loss=0.008274723775684834\n",
      "Surface training t=9193, loss=0.0077573147136718035\n",
      "Surface training t=9194, loss=0.007307008840143681\n",
      "Surface training t=9195, loss=0.008557003922760487\n",
      "Surface training t=9196, loss=0.0070808278396725655\n",
      "Surface training t=9197, loss=0.008057211292907596\n",
      "Surface training t=9198, loss=0.007487934082746506\n",
      "Surface training t=9199, loss=0.007020846474915743\n",
      "Surface training t=9200, loss=0.007696904940530658\n",
      "Surface training t=9201, loss=0.007368849124759436\n",
      "Surface training t=9202, loss=0.00794348050840199\n",
      "Surface training t=9203, loss=0.007419105852022767\n",
      "Surface training t=9204, loss=0.00720978295430541\n",
      "Surface training t=9205, loss=0.008014766033738852\n",
      "Surface training t=9206, loss=0.007801145780831575\n",
      "Surface training t=9207, loss=0.007149366196244955\n",
      "Surface training t=9208, loss=0.00782288913615048\n",
      "Surface training t=9209, loss=0.008124245097860694\n",
      "Surface training t=9210, loss=0.0071733330842107534\n",
      "Surface training t=9211, loss=0.00825443770736456\n",
      "Surface training t=9212, loss=0.00865328498184681\n",
      "Surface training t=9213, loss=0.0076606092043221\n",
      "Surface training t=9214, loss=0.008709956659004092\n",
      "Surface training t=9215, loss=0.009157423861324787\n",
      "Surface training t=9216, loss=0.008087139343842864\n",
      "Surface training t=9217, loss=0.00818002549931407\n",
      "Surface training t=9218, loss=0.008704377803951502\n",
      "Surface training t=9219, loss=0.0068412586115300655\n",
      "Surface training t=9220, loss=0.008510912768542767\n",
      "Surface training t=9221, loss=0.0091614646371454\n",
      "Surface training t=9222, loss=0.0098629891872406\n",
      "Surface training t=9223, loss=0.007986633107066154\n",
      "Surface training t=9224, loss=0.00855179550126195\n",
      "Surface training t=9225, loss=0.009707861114293337\n",
      "Surface training t=9226, loss=0.00928641832433641\n",
      "Surface training t=9227, loss=0.010315509513020515\n",
      "Surface training t=9228, loss=0.0084472238086164\n",
      "Surface training t=9229, loss=0.00885151675902307\n",
      "Surface training t=9230, loss=0.009723226306959987\n",
      "Surface training t=9231, loss=0.009363872930407524\n",
      "Surface training t=9232, loss=0.009369720704853535\n",
      "Surface training t=9233, loss=0.007583995582535863\n",
      "Surface training t=9234, loss=0.008946973364800215\n",
      "Surface training t=9235, loss=0.009481071028858423\n",
      "Surface training t=9236, loss=0.009580770041793585\n",
      "Surface training t=9237, loss=0.009379522874951363\n",
      "Surface training t=9238, loss=0.00765337492339313\n",
      "Surface training t=9239, loss=0.008772378787398338\n",
      "Surface training t=9240, loss=0.008458361262455583\n",
      "Surface training t=9241, loss=0.006938010919839144\n",
      "Surface training t=9242, loss=0.008223161567002535\n",
      "Surface training t=9243, loss=0.006707345601171255\n",
      "Surface training t=9244, loss=0.00788179668597877\n",
      "Surface training t=9245, loss=0.007558732060715556\n",
      "Surface training t=9246, loss=0.006699599791318178\n",
      "Surface training t=9247, loss=0.006909526884555817\n",
      "Surface training t=9248, loss=0.007880851160734892\n",
      "Surface training t=9249, loss=0.0071449968963861465\n",
      "Surface training t=9250, loss=0.007978288689628243\n",
      "Surface training t=9251, loss=0.006665041437372565\n",
      "Surface training t=9252, loss=0.00678234058432281\n",
      "Surface training t=9253, loss=0.007459013955667615\n",
      "Surface training t=9254, loss=0.007979022804647684\n",
      "Surface training t=9255, loss=0.007031125016510487\n",
      "Surface training t=9256, loss=0.007951550418511033\n",
      "Surface training t=9257, loss=0.007194863399490714\n",
      "Surface training t=9258, loss=0.006695583695545793\n",
      "Surface training t=9259, loss=0.007067092461511493\n",
      "Surface training t=9260, loss=0.008989423513412476\n",
      "Surface training t=9261, loss=0.007038715062662959\n",
      "Surface training t=9262, loss=0.007057890761643648\n",
      "Surface training t=9263, loss=0.00835309224203229\n",
      "Surface training t=9264, loss=0.006799604510888457\n",
      "Surface training t=9265, loss=0.0074421747121959925\n",
      "Surface training t=9266, loss=0.0083428043872118\n",
      "Surface training t=9267, loss=0.006660429993644357\n",
      "Surface training t=9268, loss=0.008064255118370056\n",
      "Surface training t=9269, loss=0.007526832399889827\n",
      "Surface training t=9270, loss=0.006914598168805242\n",
      "Surface training t=9271, loss=0.007446703035384417\n",
      "Surface training t=9272, loss=0.008059090003371239\n",
      "Surface training t=9273, loss=0.009214871097356081\n",
      "Surface training t=9274, loss=0.009073164546862245\n",
      "Surface training t=9275, loss=0.007425004616379738\n",
      "Surface training t=9276, loss=0.008970639202743769\n",
      "Surface training t=9277, loss=0.0075443515088409185\n",
      "Surface training t=9278, loss=0.007869626162573695\n",
      "Surface training t=9279, loss=0.0072680991142988205\n",
      "Surface training t=9280, loss=0.007115381769835949\n",
      "Surface training t=9281, loss=0.008886775933206081\n",
      "Surface training t=9282, loss=0.006612912751734257\n",
      "Surface training t=9283, loss=0.006504997378215194\n",
      "Surface training t=9284, loss=0.007401458453387022\n",
      "Surface training t=9285, loss=0.007831796538084745\n",
      "Surface training t=9286, loss=0.006816676817834377\n",
      "Surface training t=9287, loss=0.008164682192727923\n",
      "Surface training t=9288, loss=0.0071842914912849665\n",
      "Surface training t=9289, loss=0.007480270694941282\n",
      "Surface training t=9290, loss=0.007502367720007896\n",
      "Surface training t=9291, loss=0.007777252933010459\n",
      "Surface training t=9292, loss=0.007218305021524429\n",
      "Surface training t=9293, loss=0.008213984780013561\n",
      "Surface training t=9294, loss=0.007348115323111415\n",
      "Surface training t=9295, loss=0.0073867065366357565\n",
      "Surface training t=9296, loss=0.008104793727397919\n",
      "Surface training t=9297, loss=0.007223877822980285\n",
      "Surface training t=9298, loss=0.007965078810229897\n",
      "Surface training t=9299, loss=0.00773010915145278\n",
      "Surface training t=9300, loss=0.006941416068002582\n",
      "Surface training t=9301, loss=0.008594542741775513\n",
      "Surface training t=9302, loss=0.006604877999052405\n",
      "Surface training t=9303, loss=0.006352471420541406\n",
      "Surface training t=9304, loss=0.00790181802585721\n",
      "Surface training t=9305, loss=0.007590022869408131\n",
      "Surface training t=9306, loss=0.0069350257981568575\n",
      "Surface training t=9307, loss=0.008085689274594188\n",
      "Surface training t=9308, loss=0.007801974890753627\n",
      "Surface training t=9309, loss=0.0073177621234208345\n",
      "Surface training t=9310, loss=0.00807109009474516\n",
      "Surface training t=9311, loss=0.007874404778704047\n",
      "Surface training t=9312, loss=0.006423376267775893\n",
      "Surface training t=9313, loss=0.00782854319550097\n",
      "Surface training t=9314, loss=0.007160382578149438\n",
      "Surface training t=9315, loss=0.00793209858238697\n",
      "Surface training t=9316, loss=0.007587898755446076\n",
      "Surface training t=9317, loss=0.007115853019058704\n",
      "Surface training t=9318, loss=0.008148365188390017\n",
      "Surface training t=9319, loss=0.007926852209493518\n",
      "Surface training t=9320, loss=0.007175222039222717\n",
      "Surface training t=9321, loss=0.008509402396157384\n",
      "Surface training t=9322, loss=0.0072203443851321936\n",
      "Surface training t=9323, loss=0.008237973088398576\n",
      "Surface training t=9324, loss=0.007108273915946484\n",
      "Surface training t=9325, loss=0.006550197955220938\n",
      "Surface training t=9326, loss=0.00905839679762721\n",
      "Surface training t=9327, loss=0.0074245373252779245\n",
      "Surface training t=9328, loss=0.0070858539547771215\n",
      "Surface training t=9329, loss=0.00811511417850852\n",
      "Surface training t=9330, loss=0.00914130499586463\n",
      "Surface training t=9331, loss=0.00844704220071435\n",
      "Surface training t=9332, loss=0.009130299557000399\n",
      "Surface training t=9333, loss=0.00726092210970819\n",
      "Surface training t=9334, loss=0.007551969960331917\n",
      "Surface training t=9335, loss=0.008227797923609614\n",
      "Surface training t=9336, loss=0.007612917572259903\n",
      "Surface training t=9337, loss=0.007578713586553931\n",
      "Surface training t=9338, loss=0.007030310109257698\n",
      "Surface training t=9339, loss=0.007563455030322075\n",
      "Surface training t=9340, loss=0.008160760160535574\n",
      "Surface training t=9341, loss=0.007721404079347849\n",
      "Surface training t=9342, loss=0.00802044803276658\n",
      "Surface training t=9343, loss=0.0070132482796907425\n",
      "Surface training t=9344, loss=0.006887039402499795\n",
      "Surface training t=9345, loss=0.008435674011707306\n",
      "Surface training t=9346, loss=0.007030748529359698\n",
      "Surface training t=9347, loss=0.0075130395125597715\n",
      "Surface training t=9348, loss=0.00845305947586894\n",
      "Surface training t=9349, loss=0.006460837088525295\n",
      "Surface training t=9350, loss=0.008813187247142196\n",
      "Surface training t=9351, loss=0.006902862340211868\n",
      "Surface training t=9352, loss=0.007921995362266898\n",
      "Surface training t=9353, loss=0.008222065633162856\n",
      "Surface training t=9354, loss=0.006734209135174751\n",
      "Surface training t=9355, loss=0.006369845941662788\n",
      "Surface training t=9356, loss=0.006416129879653454\n",
      "Surface training t=9357, loss=0.009080676594749093\n",
      "Surface training t=9358, loss=0.00685004866681993\n",
      "Surface training t=9359, loss=0.009028997039422393\n",
      "Surface training t=9360, loss=0.010387304704636335\n",
      "Surface training t=9361, loss=0.01239040493965149\n",
      "Surface training t=9362, loss=0.01608623843640089\n",
      "Surface training t=9363, loss=0.01594025455415249\n",
      "Surface training t=9364, loss=0.012586541473865509\n",
      "Surface training t=9365, loss=0.012210193555802107\n",
      "Surface training t=9366, loss=0.014024278614670038\n",
      "Surface training t=9367, loss=0.01532780472189188\n",
      "Surface training t=9368, loss=0.014995623379945755\n",
      "Surface training t=9369, loss=0.013712929096072912\n",
      "Surface training t=9370, loss=0.013147805817425251\n",
      "Surface training t=9371, loss=0.014119809959083796\n",
      "Surface training t=9372, loss=0.014790271408855915\n",
      "Surface training t=9373, loss=0.014398896601051092\n",
      "Surface training t=9374, loss=0.0136661846190691\n",
      "Surface training t=9375, loss=0.014258967246860266\n",
      "Surface training t=9376, loss=0.014515138231217861\n",
      "Surface training t=9377, loss=0.01462020818144083\n",
      "Surface training t=9378, loss=0.014326540287584066\n",
      "Surface training t=9379, loss=0.013319998979568481\n",
      "Surface training t=9380, loss=0.013244043570011854\n",
      "Surface training t=9381, loss=0.013399291783571243\n",
      "Surface training t=9382, loss=0.01411910867318511\n",
      "Surface training t=9383, loss=0.014445694163441658\n",
      "Surface training t=9384, loss=0.01352269435301423\n",
      "Surface training t=9385, loss=0.013286082074046135\n",
      "Surface training t=9386, loss=0.013841346837580204\n",
      "Surface training t=9387, loss=0.014179897960275412\n",
      "Surface training t=9388, loss=0.01426111115142703\n",
      "Surface training t=9389, loss=0.013424312230199575\n",
      "Surface training t=9390, loss=0.01357997814193368\n",
      "Surface training t=9391, loss=0.014129670802503824\n",
      "Surface training t=9392, loss=0.014032735023647547\n",
      "Surface training t=9393, loss=0.013449296355247498\n",
      "Surface training t=9394, loss=0.013645272236317396\n",
      "Surface training t=9395, loss=0.013539780862629414\n",
      "Surface training t=9396, loss=0.013954118359833956\n",
      "Surface training t=9397, loss=0.014146796893328428\n",
      "Surface training t=9398, loss=0.01387967262417078\n",
      "Surface training t=9399, loss=0.013735740911215544\n",
      "Surface training t=9400, loss=0.013618588913232088\n",
      "Surface training t=9401, loss=0.013695717323571444\n",
      "Surface training t=9402, loss=0.013499528635293245\n",
      "Surface training t=9403, loss=0.013916818890720606\n",
      "Surface training t=9404, loss=0.01393697177991271\n",
      "Surface training t=9405, loss=0.014001705218106508\n",
      "Surface training t=9406, loss=0.013560797553509474\n",
      "Surface training t=9407, loss=0.013703908305615187\n",
      "Surface training t=9408, loss=0.013872005976736546\n",
      "Surface training t=9409, loss=0.014040070585906506\n",
      "Surface training t=9410, loss=0.013711267616599798\n",
      "Surface training t=9411, loss=0.013290921691805124\n",
      "Surface training t=9412, loss=0.013430577237159014\n",
      "Surface training t=9413, loss=0.013548661023378372\n",
      "Surface training t=9414, loss=0.013823708519339561\n",
      "Surface training t=9415, loss=0.0142202815040946\n",
      "Surface training t=9416, loss=0.01370343379676342\n",
      "Surface training t=9417, loss=0.013150416780263186\n",
      "Surface training t=9418, loss=0.01302553666755557\n",
      "Surface training t=9419, loss=0.013835661578923464\n",
      "Surface training t=9420, loss=0.013940193690359592\n",
      "Surface training t=9421, loss=0.013712117448449135\n",
      "Surface training t=9422, loss=0.01322125643491745\n",
      "Surface training t=9423, loss=0.014030028134584427\n",
      "Surface training t=9424, loss=0.01349590951576829\n",
      "Surface training t=9425, loss=0.013644651044160128\n",
      "Surface training t=9426, loss=0.013309486210346222\n",
      "Surface training t=9427, loss=0.012898818589746952\n",
      "Surface training t=9428, loss=0.013656564988195896\n",
      "Surface training t=9429, loss=0.013595030643045902\n",
      "Surface training t=9430, loss=0.013671399559825659\n",
      "Surface training t=9431, loss=0.013350720051676035\n",
      "Surface training t=9432, loss=0.01332455687224865\n",
      "Surface training t=9433, loss=0.013607541564852\n",
      "Surface training t=9434, loss=0.013910819310694933\n",
      "Surface training t=9435, loss=0.013656912371516228\n",
      "Surface training t=9436, loss=0.013010979630053043\n",
      "Surface training t=9437, loss=0.013285097200423479\n",
      "Surface training t=9438, loss=0.013680115807801485\n",
      "Surface training t=9439, loss=0.013469518162310123\n",
      "Surface training t=9440, loss=0.01342533715069294\n",
      "Surface training t=9441, loss=0.013250711839646101\n",
      "Surface training t=9442, loss=0.013663523364812136\n",
      "Surface training t=9443, loss=0.013775596860796213\n",
      "Surface training t=9444, loss=0.013048821594566107\n",
      "Surface training t=9445, loss=0.012714977376163006\n",
      "Surface training t=9446, loss=0.013070530258119106\n",
      "Surface training t=9447, loss=0.013508806470781565\n",
      "Surface training t=9448, loss=0.013039875775575638\n",
      "Surface training t=9449, loss=0.013403799384832382\n",
      "Surface training t=9450, loss=0.013580935541540384\n",
      "Surface training t=9451, loss=0.01334516704082489\n",
      "Surface training t=9452, loss=0.012991731520742178\n",
      "Surface training t=9453, loss=0.01306076254695654\n",
      "Surface training t=9454, loss=0.013238301035016775\n",
      "Surface training t=9455, loss=0.013664904050529003\n",
      "Surface training t=9456, loss=0.013065096456557512\n",
      "Surface training t=9457, loss=0.013035887852311134\n",
      "Surface training t=9458, loss=0.013014787342399359\n",
      "Surface training t=9459, loss=0.013198315631598234\n",
      "Surface training t=9460, loss=0.012844670563936234\n",
      "Surface training t=9461, loss=0.013001224491745234\n",
      "Surface training t=9462, loss=0.012539958115667105\n",
      "Surface training t=9463, loss=0.012641336768865585\n",
      "Surface training t=9464, loss=0.012524241115897894\n",
      "Surface training t=9465, loss=0.013091923203319311\n",
      "Surface training t=9466, loss=0.014057281892746687\n",
      "Surface training t=9467, loss=0.013549827504903078\n",
      "Surface training t=9468, loss=0.012691883370280266\n",
      "Surface training t=9469, loss=0.012199304532259703\n",
      "Surface training t=9470, loss=0.013678922317922115\n",
      "Surface training t=9471, loss=0.01382277486845851\n",
      "Surface training t=9472, loss=0.013479003682732582\n",
      "Surface training t=9473, loss=0.012926602270454168\n",
      "Surface training t=9474, loss=0.012498862110078335\n",
      "Surface training t=9475, loss=0.013042136561125517\n",
      "Surface training t=9476, loss=0.013203451409935951\n",
      "Surface training t=9477, loss=0.013336541131138802\n",
      "Surface training t=9478, loss=0.013420319184660912\n",
      "Surface training t=9479, loss=0.013261016458272934\n",
      "Surface training t=9480, loss=0.013100544456392527\n",
      "Surface training t=9481, loss=0.013455255422741175\n",
      "Surface training t=9482, loss=0.012945717666298151\n",
      "Surface training t=9483, loss=0.01309350784868002\n",
      "Surface training t=9484, loss=0.013102696277201176\n",
      "Surface training t=9485, loss=0.012994157150387764\n",
      "Surface training t=9486, loss=0.013069058302789927\n",
      "Surface training t=9487, loss=0.01281166635453701\n",
      "Surface training t=9488, loss=0.013238298706710339\n",
      "Surface training t=9489, loss=0.012814103160053492\n",
      "Surface training t=9490, loss=0.013164026662707329\n",
      "Surface training t=9491, loss=0.013093849178403616\n",
      "Surface training t=9492, loss=0.012693795375525951\n",
      "Surface training t=9493, loss=0.012990586459636688\n",
      "Surface training t=9494, loss=0.013126952573657036\n",
      "Surface training t=9495, loss=0.013381954748183489\n",
      "Surface training t=9496, loss=0.012950330041348934\n",
      "Surface training t=9497, loss=0.013409219682216644\n",
      "Surface training t=9498, loss=0.01301799388602376\n",
      "Surface training t=9499, loss=0.013178708031773567\n",
      "Surface training t=9500, loss=0.013137447647750378\n",
      "Surface training t=9501, loss=0.013113630004227161\n",
      "Surface training t=9502, loss=0.013187041506171227\n",
      "Surface training t=9503, loss=0.013002607505768538\n",
      "Surface training t=9504, loss=0.012727993540465832\n",
      "Surface training t=9505, loss=0.012383218854665756\n",
      "Surface training t=9506, loss=0.012591941747814417\n",
      "Surface training t=9507, loss=0.012771987821906805\n",
      "Surface training t=9508, loss=0.01354167377576232\n",
      "Surface training t=9509, loss=0.012729247100651264\n",
      "Surface training t=9510, loss=0.012618561275303364\n",
      "Surface training t=9511, loss=0.012398367282003164\n",
      "Surface training t=9512, loss=0.013160129077732563\n",
      "Surface training t=9513, loss=0.013152383733540773\n",
      "Surface training t=9514, loss=0.012797997333109379\n",
      "Surface training t=9515, loss=0.012636355590075254\n",
      "Surface training t=9516, loss=0.012829448096454144\n",
      "Surface training t=9517, loss=0.0129206208512187\n",
      "Surface training t=9518, loss=0.012496340554207563\n",
      "Surface training t=9519, loss=0.01314585842192173\n",
      "Surface training t=9520, loss=0.012621276080608368\n",
      "Surface training t=9521, loss=0.0124981296248734\n",
      "Surface training t=9522, loss=0.012709970586001873\n",
      "Surface training t=9523, loss=0.012477993499487638\n",
      "Surface training t=9524, loss=0.012347955256700516\n",
      "Surface training t=9525, loss=0.012367104180157185\n",
      "Surface training t=9526, loss=0.013042614329606295\n",
      "Surface training t=9527, loss=0.01319031324237585\n",
      "Surface training t=9528, loss=0.01308018621057272\n",
      "Surface training t=9529, loss=0.012192478403449059\n",
      "Surface training t=9530, loss=0.012229871936142445\n",
      "Surface training t=9531, loss=0.013003906235098839\n",
      "Surface training t=9532, loss=0.013233291916549206\n",
      "Surface training t=9533, loss=0.013410045299679041\n",
      "Surface training t=9534, loss=0.01244067121297121\n",
      "Surface training t=9535, loss=0.01251545362174511\n",
      "Surface training t=9536, loss=0.012347572483122349\n",
      "Surface training t=9537, loss=0.01218534866347909\n",
      "Surface training t=9538, loss=0.012421716004610062\n",
      "Surface training t=9539, loss=0.012835065834224224\n",
      "Surface training t=9540, loss=0.012972199358046055\n",
      "Surface training t=9541, loss=0.012743011582642794\n",
      "Surface training t=9542, loss=0.012461849488317966\n",
      "Surface training t=9543, loss=0.012564413715153933\n",
      "Surface training t=9544, loss=0.012774393428117037\n",
      "Surface training t=9545, loss=0.012787785846740007\n",
      "Surface training t=9546, loss=0.012341884896159172\n",
      "Surface training t=9547, loss=0.011874758172780275\n",
      "Surface training t=9548, loss=0.01246384670957923\n",
      "Surface training t=9549, loss=0.012455268297344446\n",
      "Surface training t=9550, loss=0.013189457356929779\n",
      "Surface training t=9551, loss=0.01252339594066143\n",
      "Surface training t=9552, loss=0.013160704635083675\n",
      "Surface training t=9553, loss=0.012627757620066404\n",
      "Surface training t=9554, loss=0.012711969204246998\n",
      "Surface training t=9555, loss=0.012768890708684921\n",
      "Surface training t=9556, loss=0.012388871517032385\n",
      "Surface training t=9557, loss=0.012287055607885122\n",
      "Surface training t=9558, loss=0.012313864193856716\n",
      "Surface training t=9559, loss=0.012996294535696507\n",
      "Surface training t=9560, loss=0.012672487180680037\n",
      "Surface training t=9561, loss=0.012484279461205006\n",
      "Surface training t=9562, loss=0.011827319860458374\n",
      "Surface training t=9563, loss=0.01186015224084258\n",
      "Surface training t=9564, loss=0.013207674026489258\n",
      "Surface training t=9565, loss=0.013205843977630138\n",
      "Surface training t=9566, loss=0.012611088808625937\n",
      "Surface training t=9567, loss=0.011608260218054056\n",
      "Surface training t=9568, loss=0.011911517474800348\n",
      "Surface training t=9569, loss=0.013275702949613333\n",
      "Surface training t=9570, loss=0.012922955211251974\n",
      "Surface training t=9571, loss=0.012554869055747986\n",
      "Surface training t=9572, loss=0.012074293103069067\n",
      "Surface training t=9573, loss=0.012376595754176378\n",
      "Surface training t=9574, loss=0.012584158685058355\n",
      "Surface training t=9575, loss=0.012679741717875004\n",
      "Surface training t=9576, loss=0.012258026283234358\n",
      "Surface training t=9577, loss=0.012087611015886068\n",
      "Surface training t=9578, loss=0.012301516719162464\n",
      "Surface training t=9579, loss=0.012840413488447666\n",
      "Surface training t=9580, loss=0.01269081560894847\n",
      "Surface training t=9581, loss=0.012421198654919863\n",
      "Surface training t=9582, loss=0.01165742613375187\n",
      "Surface training t=9583, loss=0.012109111063182354\n",
      "Surface training t=9584, loss=0.012416754383593798\n",
      "Surface training t=9585, loss=0.012600054033100605\n",
      "Surface training t=9586, loss=0.012661177664995193\n",
      "Surface training t=9587, loss=0.011890298686921597\n",
      "Surface training t=9588, loss=0.01202531112357974\n",
      "Surface training t=9589, loss=0.012563079595565796\n",
      "Surface training t=9590, loss=0.013055946677923203\n",
      "Surface training t=9591, loss=0.012735218740999699\n",
      "Surface training t=9592, loss=0.012442220468074083\n",
      "Surface training t=9593, loss=0.012252766638994217\n",
      "Surface training t=9594, loss=0.012674553785473108\n",
      "Surface training t=9595, loss=0.012398786377161741\n",
      "Surface training t=9596, loss=0.012253556866198778\n",
      "Surface training t=9597, loss=0.012182501144707203\n",
      "Surface training t=9598, loss=0.01253225514665246\n",
      "Surface training t=9599, loss=0.012414302676916122\n",
      "Surface training t=9600, loss=0.012161326128989458\n",
      "Surface training t=9601, loss=0.012826736085116863\n",
      "Surface training t=9602, loss=0.012535722460597754\n",
      "Surface training t=9603, loss=0.012484617065638304\n",
      "Surface training t=9604, loss=0.012293829582631588\n",
      "Surface training t=9605, loss=0.01150562334805727\n",
      "Surface training t=9606, loss=0.012515492737293243\n",
      "Surface training t=9607, loss=0.012885764706879854\n",
      "Surface training t=9608, loss=0.012200558558106422\n",
      "Surface training t=9609, loss=0.011346177197992802\n",
      "Surface training t=9610, loss=0.01188716758042574\n",
      "Surface training t=9611, loss=0.0125687918625772\n",
      "Surface training t=9612, loss=0.012670633848756552\n",
      "Surface training t=9613, loss=0.012318259570747614\n",
      "Surface training t=9614, loss=0.01172498520463705\n",
      "Surface training t=9615, loss=0.012361272238194942\n",
      "Surface training t=9616, loss=0.012850529048591852\n",
      "Surface training t=9617, loss=0.012428865768015385\n",
      "Surface training t=9618, loss=0.012074018362909555\n",
      "Surface training t=9619, loss=0.012016014195978642\n",
      "Surface training t=9620, loss=0.012405684683471918\n",
      "Surface training t=9621, loss=0.012610371690243483\n",
      "Surface training t=9622, loss=0.012073030229657888\n",
      "Surface training t=9623, loss=0.012032679282128811\n",
      "Surface training t=9624, loss=0.012049446813762188\n",
      "Surface training t=9625, loss=0.012240502517670393\n",
      "Surface training t=9626, loss=0.012300774920731783\n",
      "Surface training t=9627, loss=0.012198204174637794\n",
      "Surface training t=9628, loss=0.01267329603433609\n",
      "Surface training t=9629, loss=0.012434340082108974\n",
      "Surface training t=9630, loss=0.011841483879834414\n",
      "Surface training t=9631, loss=0.01241670222952962\n",
      "Surface training t=9632, loss=0.012146332766860723\n",
      "Surface training t=9633, loss=0.011989067774266005\n",
      "Surface training t=9634, loss=0.011874174699187279\n",
      "Surface training t=9635, loss=0.01128727663308382\n",
      "Surface training t=9636, loss=0.011908310931175947\n",
      "Surface training t=9637, loss=0.012252720072865486\n",
      "Surface training t=9638, loss=0.011889110784977674\n",
      "Surface training t=9639, loss=0.01084163784980774\n",
      "Surface training t=9640, loss=0.011764212977141142\n",
      "Surface training t=9641, loss=0.013054209761321545\n",
      "Surface training t=9642, loss=0.012723375577479601\n",
      "Surface training t=9643, loss=0.011963744182139635\n",
      "Surface training t=9644, loss=0.011724622920155525\n",
      "Surface training t=9645, loss=0.012197711504995823\n",
      "Surface training t=9646, loss=0.012359665241092443\n",
      "Surface training t=9647, loss=0.01205820869654417\n",
      "Surface training t=9648, loss=0.011817773804068565\n",
      "Surface training t=9649, loss=0.012233215849846601\n",
      "Surface training t=9650, loss=0.01216395664960146\n",
      "Surface training t=9651, loss=0.012319235596805811\n",
      "Surface training t=9652, loss=0.01205498818308115\n",
      "Surface training t=9653, loss=0.01226888969540596\n",
      "Surface training t=9654, loss=0.012242199387401342\n",
      "Surface training t=9655, loss=0.012115372344851494\n",
      "Surface training t=9656, loss=0.01240893080830574\n",
      "Surface training t=9657, loss=0.012287545949220657\n",
      "Surface training t=9658, loss=0.012275052722543478\n",
      "Surface training t=9659, loss=0.011643068864941597\n",
      "Surface training t=9660, loss=0.011870999354869127\n",
      "Surface training t=9661, loss=0.012280234135687351\n",
      "Surface training t=9662, loss=0.01230359124019742\n",
      "Surface training t=9663, loss=0.01212421664968133\n",
      "Surface training t=9664, loss=0.011821211315691471\n",
      "Surface training t=9665, loss=0.011929124128073454\n",
      "Surface training t=9666, loss=0.011672302149236202\n",
      "Surface training t=9667, loss=0.012018006760627031\n",
      "Surface training t=9668, loss=0.011794311460107565\n",
      "Surface training t=9669, loss=0.011429394129663706\n",
      "Surface training t=9670, loss=0.01136382995173335\n",
      "Surface training t=9671, loss=0.012200682424008846\n",
      "Surface training t=9672, loss=0.012888958677649498\n",
      "Surface training t=9673, loss=0.012060130015015602\n",
      "Surface training t=9674, loss=0.011029454879462719\n",
      "Surface training t=9675, loss=0.011424246244132519\n",
      "Surface training t=9676, loss=0.01246618339791894\n",
      "Surface training t=9677, loss=0.01249455101788044\n",
      "Surface training t=9678, loss=0.011598206125199795\n",
      "Surface training t=9679, loss=0.011452908627688885\n",
      "Surface training t=9680, loss=0.01177951879799366\n",
      "Surface training t=9681, loss=0.012642024084925652\n",
      "Surface training t=9682, loss=0.012442653998732567\n",
      "Surface training t=9683, loss=0.0113389459438622\n",
      "Surface training t=9684, loss=0.010631055571138859\n",
      "Surface training t=9685, loss=0.011606049723923206\n",
      "Surface training t=9686, loss=0.01291141053661704\n",
      "Surface training t=9687, loss=0.012392479460686445\n",
      "Surface training t=9688, loss=0.010467793326824903\n",
      "Surface training t=9689, loss=0.010740978643298149\n",
      "Surface training t=9690, loss=0.012084691319614649\n",
      "Surface training t=9691, loss=0.012540843337774277\n",
      "Surface training t=9692, loss=0.011897720862179995\n",
      "Surface training t=9693, loss=0.011169621255248785\n",
      "Surface training t=9694, loss=0.01189721841365099\n",
      "Surface training t=9695, loss=0.01244601933285594\n",
      "Surface training t=9696, loss=0.011917224153876305\n",
      "Surface training t=9697, loss=0.01165328687056899\n",
      "Surface training t=9698, loss=0.011245953384786844\n",
      "Surface training t=9699, loss=0.011436934117227793\n",
      "Surface training t=9700, loss=0.012106028385460377\n",
      "Surface training t=9701, loss=0.011952421627938747\n",
      "Surface training t=9702, loss=0.011386584490537643\n",
      "Surface training t=9703, loss=0.010737363714724779\n",
      "Surface training t=9704, loss=0.011130490340292454\n",
      "Surface training t=9705, loss=0.011993370484560728\n",
      "Surface training t=9706, loss=0.011364822275936604\n",
      "Surface training t=9707, loss=0.011459984350949526\n",
      "Surface training t=9708, loss=0.011863643303513527\n",
      "Surface training t=9709, loss=0.011683492455631495\n",
      "Surface training t=9710, loss=0.012108026072382927\n",
      "Surface training t=9711, loss=0.011947959661483765\n",
      "Surface training t=9712, loss=0.011484020855277777\n",
      "Surface training t=9713, loss=0.011331758927553892\n",
      "Surface training t=9714, loss=0.011066874023526907\n",
      "Surface training t=9715, loss=0.011133646592497826\n",
      "Surface training t=9716, loss=0.011181752197444439\n",
      "Surface training t=9717, loss=0.01157876243814826\n",
      "Surface training t=9718, loss=0.011556844227015972\n",
      "Surface training t=9719, loss=0.010972840245813131\n",
      "Surface training t=9720, loss=0.010903584770858288\n",
      "Surface training t=9721, loss=0.011239435989409685\n",
      "Surface training t=9722, loss=0.011278430931270123\n",
      "Surface training t=9723, loss=0.010784895159304142\n",
      "Surface training t=9724, loss=0.011593849398195744\n",
      "Surface training t=9725, loss=0.011326839681714773\n",
      "Surface training t=9726, loss=0.010536552872508764\n",
      "Surface training t=9727, loss=0.010650301817804575\n",
      "Surface training t=9728, loss=0.011421477887779474\n",
      "Surface training t=9729, loss=0.011517019011080265\n",
      "Surface training t=9730, loss=0.011685372330248356\n",
      "Surface training t=9731, loss=0.011832815129309893\n",
      "Surface training t=9732, loss=0.011383993085473776\n",
      "Surface training t=9733, loss=0.010815915185958147\n",
      "Surface training t=9734, loss=0.010605507995933294\n",
      "Surface training t=9735, loss=0.01109815202653408\n",
      "Surface training t=9736, loss=0.011082169599831104\n",
      "Surface training t=9737, loss=0.011398302391171455\n",
      "Surface training t=9738, loss=0.010933111421763897\n",
      "Surface training t=9739, loss=0.011255613062530756\n",
      "Surface training t=9740, loss=0.011298999190330505\n",
      "Surface training t=9741, loss=0.010997799690812826\n",
      "Surface training t=9742, loss=0.010979879181832075\n",
      "Surface training t=9743, loss=0.011223803274333477\n",
      "Surface training t=9744, loss=0.011039879638701677\n",
      "Surface training t=9745, loss=0.010972634889185429\n",
      "Surface training t=9746, loss=0.011120876297354698\n",
      "Surface training t=9747, loss=0.011058129370212555\n",
      "Surface training t=9748, loss=0.011110418941825628\n",
      "Surface training t=9749, loss=0.011063545476645231\n",
      "Surface training t=9750, loss=0.011280473321676254\n",
      "Surface training t=9751, loss=0.010776768438518047\n",
      "Surface training t=9752, loss=0.010317991487681866\n",
      "Surface training t=9753, loss=0.010437613353133202\n",
      "Surface training t=9754, loss=0.010381541214883327\n",
      "Surface training t=9755, loss=0.011066907551139593\n",
      "Surface training t=9756, loss=0.011387832462787628\n",
      "Surface training t=9757, loss=0.011224609799683094\n",
      "Surface training t=9758, loss=0.010513267014175653\n",
      "Surface training t=9759, loss=0.010391338262706995\n",
      "Surface training t=9760, loss=0.01025864901021123\n",
      "Surface training t=9761, loss=0.010518873576074839\n",
      "Surface training t=9762, loss=0.010514257941395044\n",
      "Surface training t=9763, loss=0.010854611173272133\n",
      "Surface training t=9764, loss=0.011053189635276794\n",
      "Surface training t=9765, loss=0.01090155215933919\n",
      "Surface training t=9766, loss=0.010831188876181841\n",
      "Surface training t=9767, loss=0.010612116660922766\n",
      "Surface training t=9768, loss=0.010405038483440876\n",
      "Surface training t=9769, loss=0.010090218391269445\n",
      "Surface training t=9770, loss=0.009925138670951128\n",
      "Surface training t=9771, loss=0.010073873680084944\n",
      "Surface training t=9772, loss=0.01023023808375001\n",
      "Surface training t=9773, loss=0.010784472338855267\n",
      "Surface training t=9774, loss=0.011127771344035864\n",
      "Surface training t=9775, loss=0.010773106943815947\n",
      "Surface training t=9776, loss=0.010421824641525745\n",
      "Surface training t=9777, loss=0.010033021681010723\n",
      "Surface training t=9778, loss=0.009712717961519957\n",
      "Surface training t=9779, loss=0.009736505337059498\n",
      "Surface training t=9780, loss=0.009281534468755126\n",
      "Surface training t=9781, loss=0.010037707630544901\n",
      "Surface training t=9782, loss=0.010636430932208896\n",
      "Surface training t=9783, loss=0.011334613896906376\n",
      "Surface training t=9784, loss=0.011347742285579443\n",
      "Surface training t=9785, loss=0.01054252777248621\n",
      "Surface training t=9786, loss=0.009268668247386813\n",
      "Surface training t=9787, loss=0.009383728494867682\n",
      "Surface training t=9788, loss=0.011204750277101994\n",
      "Surface training t=9789, loss=0.013135312125086784\n",
      "Surface training t=9790, loss=0.009280070196837187\n",
      "Surface training t=9791, loss=0.008802614640444517\n",
      "Surface training t=9792, loss=0.008435598341748118\n",
      "Surface training t=9793, loss=0.007118778768926859\n",
      "Surface training t=9794, loss=0.008156841155141592\n",
      "Surface training t=9795, loss=0.008650739211589098\n",
      "Surface training t=9796, loss=0.007514118915423751\n",
      "Surface training t=9797, loss=0.008493403904139996\n",
      "Surface training t=9798, loss=0.010385939851403236\n",
      "Surface training t=9799, loss=0.013252337463200092\n",
      "Surface training t=9800, loss=0.010956785175949335\n",
      "Surface training t=9801, loss=0.010436704847961664\n",
      "Surface training t=9802, loss=0.011648480780422688\n",
      "Surface training t=9803, loss=0.011781123001128435\n",
      "Surface training t=9804, loss=0.010852065868675709\n",
      "Surface training t=9805, loss=0.010185891762375832\n",
      "Surface training t=9806, loss=0.008866419084370136\n",
      "Surface training t=9807, loss=0.0088077443651855\n",
      "Surface training t=9808, loss=0.0098143070936203\n",
      "Surface training t=9809, loss=0.010429413989186287\n",
      "Surface training t=9810, loss=0.011581992264837027\n",
      "Surface training t=9811, loss=0.011173737701028585\n",
      "Surface training t=9812, loss=0.00960400840267539\n",
      "Surface training t=9813, loss=0.008694272488355637\n",
      "Surface training t=9814, loss=0.00953852292150259\n",
      "Surface training t=9815, loss=0.009308477398008108\n",
      "Surface training t=9816, loss=0.01038917526602745\n",
      "Surface training t=9817, loss=0.010493860114365816\n",
      "Surface training t=9818, loss=0.010461931116878986\n",
      "Surface training t=9819, loss=0.008417550474405289\n",
      "Surface training t=9820, loss=0.009497980121523142\n",
      "Surface training t=9821, loss=0.009018959710374475\n",
      "Surface training t=9822, loss=0.009924071840941906\n",
      "Surface training t=9823, loss=0.011429707519710064\n",
      "Surface training t=9824, loss=0.009057123679667711\n",
      "Surface training t=9825, loss=0.01058554369956255\n",
      "Surface training t=9826, loss=0.010339423548430204\n",
      "Surface training t=9827, loss=0.01106336060911417\n",
      "Surface training t=9828, loss=0.012635999359190464\n",
      "Surface training t=9829, loss=0.012303269002586603\n",
      "Surface training t=9830, loss=0.01035493379458785\n",
      "Surface training t=9831, loss=0.009796859230846167\n",
      "Surface training t=9832, loss=0.008890235098078847\n",
      "Surface training t=9833, loss=0.008240068797022104\n",
      "Surface training t=9834, loss=0.009351507993414998\n",
      "Surface training t=9835, loss=0.011263320222496986\n",
      "Surface training t=9836, loss=0.014166306238621473\n",
      "Surface training t=9837, loss=0.01093809213489294\n",
      "Surface training t=9838, loss=0.01077327597886324\n",
      "Surface training t=9839, loss=0.01056953240185976\n",
      "Surface training t=9840, loss=0.010732609778642654\n",
      "Surface training t=9841, loss=0.010984817985445261\n",
      "Surface training t=9842, loss=0.011354513932019472\n",
      "Surface training t=9843, loss=0.011522350367158651\n",
      "Surface training t=9844, loss=0.010502998251467943\n",
      "Surface training t=9845, loss=0.01155139971524477\n",
      "Surface training t=9846, loss=0.010890860110521317\n",
      "Surface training t=9847, loss=0.010559169575572014\n",
      "Surface training t=9848, loss=0.010049918666481972\n",
      "Surface training t=9849, loss=0.011198037769645452\n",
      "Surface training t=9850, loss=0.011687250342220068\n",
      "Surface training t=9851, loss=0.011841978412121534\n",
      "Surface training t=9852, loss=0.010909247677773237\n",
      "Surface training t=9853, loss=0.009654965717345476\n",
      "Surface training t=9854, loss=0.00978720048442483\n",
      "Surface training t=9855, loss=0.01044544018805027\n",
      "Surface training t=9856, loss=0.01213034801185131\n",
      "Surface training t=9857, loss=0.011961412616074085\n",
      "Surface training t=9858, loss=0.01102916244417429\n",
      "Surface training t=9859, loss=0.009471900295466185\n",
      "Surface training t=9860, loss=0.010141468374058604\n",
      "Surface training t=9861, loss=0.00917226541787386\n",
      "Surface training t=9862, loss=0.008855860913172364\n",
      "Surface training t=9863, loss=0.007844449020922184\n",
      "Surface training t=9864, loss=0.007188043324276805\n",
      "Surface training t=9865, loss=0.00812608445994556\n",
      "Surface training t=9866, loss=0.006681529805064201\n",
      "Surface training t=9867, loss=0.007910105399787426\n",
      "Surface training t=9868, loss=0.00684555689804256\n",
      "Surface training t=9869, loss=0.0078002153895795345\n",
      "Surface training t=9870, loss=0.007201716070994735\n",
      "Surface training t=9871, loss=0.007425820222124457\n",
      "Surface training t=9872, loss=0.0062842348124831915\n",
      "Surface training t=9873, loss=0.007051288848742843\n",
      "Surface training t=9874, loss=0.006769213126972318\n",
      "Surface training t=9875, loss=0.00630407128483057\n",
      "Surface training t=9876, loss=0.007229909999296069\n",
      "Surface training t=9877, loss=0.0064074445981532335\n",
      "Surface training t=9878, loss=0.006423186976462603\n",
      "Surface training t=9879, loss=0.00691459234803915\n",
      "Surface training t=9880, loss=0.00671064667403698\n",
      "Surface training t=9881, loss=0.006193757522851229\n",
      "Surface training t=9882, loss=0.006920338375493884\n",
      "Surface training t=9883, loss=0.007195421727374196\n",
      "Surface training t=9884, loss=0.006345926551148295\n",
      "Surface training t=9885, loss=0.006110656540840864\n",
      "Surface training t=9886, loss=0.006177545292302966\n",
      "Surface training t=9887, loss=0.006119890604168177\n",
      "Surface training t=9888, loss=0.006060691084712744\n",
      "Surface training t=9889, loss=0.005806791363283992\n",
      "Surface training t=9890, loss=0.006033686688169837\n",
      "Surface training t=9891, loss=0.00600358797237277\n",
      "Surface training t=9892, loss=0.0062514119781553745\n",
      "Surface training t=9893, loss=0.007062320131808519\n",
      "Surface training t=9894, loss=0.007083123549818993\n",
      "Surface training t=9895, loss=0.00700882263481617\n",
      "Surface training t=9896, loss=0.006675287848338485\n",
      "Surface training t=9897, loss=0.0064612701535224915\n",
      "Surface training t=9898, loss=0.0072629612404853106\n",
      "Surface training t=9899, loss=0.006800571456551552\n",
      "Surface training t=9900, loss=0.006236630724743009\n",
      "Surface training t=9901, loss=0.007376173511147499\n",
      "Surface training t=9902, loss=0.00669124280102551\n",
      "Surface training t=9903, loss=0.00619643903337419\n",
      "Surface training t=9904, loss=0.006083157612010837\n",
      "Surface training t=9905, loss=0.006247129291296005\n",
      "Surface training t=9906, loss=0.00686194677837193\n",
      "Surface training t=9907, loss=0.0071676739025861025\n",
      "Surface training t=9908, loss=0.006272789090871811\n",
      "Surface training t=9909, loss=0.006457613315433264\n",
      "Surface training t=9910, loss=0.006884404690936208\n",
      "Surface training t=9911, loss=0.006525148171931505\n",
      "Surface training t=9912, loss=0.007349870167672634\n",
      "Surface training t=9913, loss=0.006949863163754344\n",
      "Surface training t=9914, loss=0.0064272882882505655\n",
      "Surface training t=9915, loss=0.006284273229539394\n",
      "Surface training t=9916, loss=0.006941911997273564\n",
      "Surface training t=9917, loss=0.007292579393833876\n",
      "Surface training t=9918, loss=0.006118842167779803\n",
      "Surface training t=9919, loss=0.008010477060452104\n",
      "Surface training t=9920, loss=0.006654791068285704\n",
      "Surface training t=9921, loss=0.006008419906720519\n",
      "Surface training t=9922, loss=0.006430288311094046\n",
      "Surface training t=9923, loss=0.008108883630484343\n",
      "Surface training t=9924, loss=0.006221476476639509\n",
      "Surface training t=9925, loss=0.0064636883325874805\n",
      "Surface training t=9926, loss=0.007788644405081868\n",
      "Surface training t=9927, loss=0.006039491388946772\n",
      "Surface training t=9928, loss=0.008082507411018014\n",
      "Surface training t=9929, loss=0.006330503849312663\n",
      "Surface training t=9930, loss=0.007004013983532786\n",
      "Surface training t=9931, loss=0.007242380175739527\n",
      "Surface training t=9932, loss=0.006023086840286851\n",
      "Surface training t=9933, loss=0.005947694880887866\n",
      "Surface training t=9934, loss=0.006924026180058718\n",
      "Surface training t=9935, loss=0.007502258522436023\n",
      "Surface training t=9936, loss=0.006781150354072452\n",
      "Surface training t=9937, loss=0.006716854637488723\n",
      "Surface training t=9938, loss=0.007574266055598855\n",
      "Surface training t=9939, loss=0.006993183167651296\n",
      "Surface training t=9940, loss=0.0071841946337372065\n",
      "Surface training t=9941, loss=0.007450382690876722\n",
      "Surface training t=9942, loss=0.006210253806784749\n",
      "Surface training t=9943, loss=0.006154053146019578\n",
      "Surface training t=9944, loss=0.006500817136839032\n",
      "Surface training t=9945, loss=0.0077317035757005215\n",
      "Surface training t=9946, loss=0.006183328805491328\n",
      "Surface training t=9947, loss=0.0069837921764701605\n",
      "Surface training t=9948, loss=0.0074511554557830095\n",
      "Surface training t=9949, loss=0.006427861517295241\n",
      "Surface training t=9950, loss=0.006449357373639941\n",
      "Surface training t=9951, loss=0.008417854085564613\n",
      "Surface training t=9952, loss=0.006441386183723807\n",
      "Surface training t=9953, loss=0.006157811963930726\n",
      "Surface training t=9954, loss=0.006958674639463425\n",
      "Surface training t=9955, loss=0.007663808995857835\n",
      "Surface training t=9956, loss=0.006857332540675998\n",
      "Surface training t=9957, loss=0.006078735459595919\n",
      "Surface training t=9958, loss=0.007205207599326968\n",
      "Surface training t=9959, loss=0.0075326720252633095\n",
      "Surface training t=9960, loss=0.005985495168715715\n",
      "Surface training t=9961, loss=0.008453877177089453\n",
      "Surface training t=9962, loss=0.006892399163916707\n",
      "Surface training t=9963, loss=0.009087404003366828\n",
      "Surface training t=9964, loss=0.011961410753428936\n",
      "Surface training t=9965, loss=0.01875900849699974\n",
      "Surface training t=9966, loss=0.013621382880955935\n",
      "Surface training t=9967, loss=0.010157144162803888\n",
      "Surface training t=9968, loss=0.013373569119721651\n",
      "Surface training t=9969, loss=0.01417278777807951\n",
      "Surface training t=9970, loss=0.015867894981056452\n",
      "Surface training t=9971, loss=0.01201251894235611\n",
      "Surface training t=9972, loss=0.010052073281258345\n",
      "Surface training t=9973, loss=0.012775220442563295\n",
      "Surface training t=9974, loss=0.016098361928015947\n",
      "Surface training t=9975, loss=0.013803024776279926\n",
      "Surface training t=9976, loss=0.012888525612652302\n",
      "Surface training t=9977, loss=0.010266684927046299\n",
      "Surface training t=9978, loss=0.015197250992059708\n",
      "Surface training t=9979, loss=0.013308261521160603\n",
      "Surface training t=9980, loss=0.014476940501481295\n",
      "Surface training t=9981, loss=0.01146963844075799\n",
      "Surface training t=9982, loss=0.013256567064672709\n",
      "Surface training t=9983, loss=0.01272507756948471\n",
      "Surface training t=9984, loss=0.014882433228194714\n",
      "Surface training t=9985, loss=0.012596397660672665\n",
      "Surface training t=9986, loss=0.013752616941928864\n",
      "Surface training t=9987, loss=0.01154406787827611\n",
      "Surface training t=9988, loss=0.0158363520167768\n",
      "Surface training t=9989, loss=0.010876881424337626\n",
      "Surface training t=9990, loss=0.015024921856820583\n",
      "Surface training t=9991, loss=0.010343536268919706\n",
      "Surface training t=9992, loss=0.015505928080528975\n",
      "Surface training t=9993, loss=0.011390333529561758\n",
      "Surface training t=9994, loss=0.01422900939360261\n",
      "Surface training t=9995, loss=0.012666513212025166\n",
      "Surface training t=9996, loss=0.014126145280897617\n",
      "Surface training t=9997, loss=0.010846242308616638\n",
      "Surface training t=9998, loss=0.014468210749328136\n",
      "Surface training t=9999, loss=0.013662007171660662\n",
      "initial_shape (81, 81) derivs_tensor.shape (6561, 4)\n",
      "self.tokens is ['u', 'du/dx1', 'd^2u/dx1^2', 'du/dx2', 'd^2u/dx2^2']\n",
      "Here, derivs order is {'u': [None], 'du/dx1': [0], 'd^2u/dx1^2': [0, 0], 'du/dx2': [1], 'd^2u/dx2^2': [1, 1]}\n",
      "The cardinality of defined token pool is [5 2 2]\n",
      "Among them, the pool contains [5]\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 1/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 2/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 3/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 4/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 5/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 6/7 solutions.\n",
      "Creating new equation, sparsity value 0.5\n",
      "New solution accepted, confirmed 7/7 solutions.\n",
      "The optimization has been conducted.\n",
      "0.7525779489737819 * du/dx1{power: 1.0} + 0.0 * du/dx2{power: 1.0} + -0.08929666003310022 * d^2u/dx1^2{power: 1.0} + 0.03091468797564032 * d^2u/dx1^2{power: 1.0} * t{power: 1.0} + 0.0 * d^2u/dx2^2{power: 1.0} * cos{power: 1.0, freq: 5.693460370017815, dim: 0.0} + -0.06952879495012283 = du/dx1{power: 1.0} * t{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.8, 0.2]}}, ('sparsity', 'u'): {'optimizable': False, 'value': 0.5}} , with objective function values of [4.10533795] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#sys.path.append('../')\n",
    "\n",
    "#sys.path.pop()\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..')))\n",
    "\n",
    "import numpy as np\n",
    "import epde.interface.interface as epde_alg\n",
    "\n",
    "from epde.interface.prepared_tokens import CustomTokens, CacheStoredTokens, TrigonometricTokens\n",
    "\n",
    "'''\n",
    "Loading data, representing the wave equation solution and moving the time axis into \n",
    "the first position. \n",
    "\n",
    "'''\n",
    "\n",
    "data_file = 'EPDE/examples/data/wave_sln_80.csv'\n",
    "data = np.loadtxt(data_file, delimiter = ',').T\n",
    "\n",
    "'''\n",
    "Defining grids and boundary for the domain. The grids can be (and will be) \n",
    "used as a separate family of tokens and is necessary, when we add functions,\n",
    "dependent on the coordinates, such as trigonometric functions, into the pool.\n",
    "Also, we specify boundary for the domain: the derivatives near the domain \n",
    "boundary tend to have high computational errors.\n",
    "'''\n",
    "\n",
    "t = np.linspace(0, 1, 81); x = np.linspace(0, 1, 81)\n",
    "grids = np.meshgrid(t, x, indexing = 'ij')\n",
    "dimensionality = data.ndim - 1   \n",
    "\n",
    "boundary = 10\n",
    "'''\n",
    "Here, we define the object, dedicated to the equation search: among the \n",
    "initialization arguments, the most important include dimensionality (here we \n",
    "must pass the dimensionality of the input dataset), and number of the equation \n",
    "search iterations. Multiobjective mode flag controls the optimization procedure: if it is False, \n",
    "the algorithm executes a singleobjective optimization, detecting only a single best candidate equation from the \n",
    "point of process representation. Otherwise, in multi-objective optimization mode, a Pareto frontier, \n",
    "containing solutions, \"best\", according to selected metrics (that are complexity and quality) is detected.\n",
    "'''\n",
    "\n",
    "multiobjective_mode = False\n",
    "epde_search_obj = epde_alg.EpdeSearch(multiobjective_mode=multiobjective_mode, use_solver = False, \n",
    "                                      dimensionality = dimensionality, boundary = 10,\n",
    "                                      coordinate_tensors = grids)\n",
    "\n",
    "'''\n",
    "Setting memory usage for cache stored tokens and terms (that are the ones, that \n",
    "are saved after initial calculations during algorithm operations to avoid \n",
    "redundant computations).\n",
    "To prepare for the equation search, data can be denoised, and derivatives have to be computed. \n",
    "Here by .set_preprocessor() we set ANN-based data smoothing, and from that ANN values, the derivatives \n",
    "are calculated, using finite differences. By default, Chebyshev polynomials (default_preprocessor_type='poly') \n",
    "are used to represent the data, and their analytical derivativs are used in algorithm.\n",
    "\n",
    "Next, specifying parameters of the optimization algorithm, such as number of epochs and population size,\n",
    "with .set_singleobjective_params(). \n",
    "'''\n",
    "\n",
    " \n",
    "epde_search_obj.set_memory_properties(example_tensor = data, mem_for_cache_frac = 15)\n",
    "\n",
    "epde_search_obj.set_preprocessor(default_preprocessor_type='ANN', preprocessor_kwargs={'epochs_max' : 10000})\n",
    "\n",
    "popsize = 7\n",
    "# if multiobjective_mode:\n",
    "#     epde_search_obj.set_moeadd_params(population_size = popsize, \n",
    "#                                       training_epochs=40)\n",
    "# else:\n",
    "epde_search_obj.set_singleobjective_params(population_size = popsize, \n",
    "                                           training_epochs=60)\n",
    "'''\n",
    "Defining tokens, containing grid, so our discovered equation can have \n",
    "terms like \"t * du/dt\". Here, we operate on the synthetic data and we do \n",
    "not expect their presence in the desired equation. However, they can be present\n",
    "in an equation, describing some real-world data, thus we provide a tool for their\n",
    "inclusion.\n",
    "\n",
    "To increase the pool size and artificially complicate the equation search problem, we\n",
    "can include trigonometric tokens.\n",
    "\n",
    "'''\n",
    "\n",
    "custom_grid_tokens = CacheStoredTokens(token_type = 'grid',\n",
    "                                       token_labels = ['t', 'x'],\n",
    "                                       token_tensors={'t' : grids[0], 'x' : grids[1]},\n",
    "                                       params_ranges = {'power' : (1, 1)},\n",
    "                                       params_equality_ranges = None)\n",
    "\n",
    "trig_tokens = TrigonometricTokens(dimensionality = dimensionality)\n",
    "\n",
    "'''\n",
    "Method epde_search.fit() is used to initiate the equation search. \n",
    "'''\n",
    "\n",
    "factors_max_number = {'factors_num' : [1, 2], 'probas' : [0.8, 0.2]}\n",
    "\n",
    "opt_val = 5e-1\n",
    "bounds = (1e-8, 1e0) if multiobjective_mode else (opt_val, opt_val)    \n",
    "epde_search_obj.fit(data=data, variable_names=['u',], max_deriv_order=(2, 2),\n",
    "                    equation_terms_max_number=6, data_fun_pow = 1, additional_tokens=[trig_tokens, custom_grid_tokens], \n",
    "                    equation_factors_max_number=factors_max_number,\n",
    "                    eq_sparsity_interval=bounds)\n",
    "\n",
    "'''\n",
    "Here we have conducted a single-objective optimization, ideally detecting equation, similar to \n",
    "0.0 * du/dx1{power: 1.0} + 0.0 * du/dx2{power: 1.0} * cos{power: 1.0, freq: 4.663145421047243, dim: 1.0} \n",
    "+ 0.0 * du/dx2{power: 1.0} + 0.0 * u{power: 1.0} + 0.039843458066873096 * d^2u/dx2^2{power: 1.0} + \n",
    "-0.021396897100174943 = d^2u/dx1^2{power: 1.0}\n",
    "{'terms_number': {'optimizable': False, 'value': 6}, \n",
    " 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], \n",
    "                                                         'probas': [0.8, 0.2]}}, \n",
    "('sparsity', 'u'): {'optimizable': False, 'value': 0.5}} ,\n",
    "with objective function values of [9.13122923]\n",
    "\n",
    "'''\n",
    "\n",
    "epde_search_obj.equation_search_results(only_print=True, num = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76bac8-2478-42af-b34a-73d34516f806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
